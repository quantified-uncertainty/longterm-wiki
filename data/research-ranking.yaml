# Research Importance Ranking
# Pages ordered by how much value deeper investigation would yield.
#
# This list is the source of truth. Scores are derived from position.
# Run `pnpm crux importance sync --apply` to write scores to frontmatter.
#
# Total ranked: 649
ranking:
  - safety-research
  - accident-risks
  - capabilities
  - gap-analysis
  - factors-misalignment-potential-overview
  - factors-civilizational-competence-overview
  - factors-ai-capabilities-overview
  - factors-ai-uses-overview
  - factors-ai-ownership-overview
  - outcomes-overview
  - scientific-research
  - long-horizon
  - agentic-ai
  - factors-misuse-potential-overview
  - factors-transition-turbulence-overview
  - factors-overview
  - epistemics
  - recursive-ai-capabilities
  - biological-threat-exposure
  - cyber-threat-exposure
  - page-creator-pipeline
  - apollo-research
  - ai-timelines
  - scenarios-human-catastrophe-overview
  - racing-intensity
  - lab-safety-practices
  - interpretability-coverage
  - alignment-robustness
  - human-oversight-quality
  - regulatory-capacity
  - quantitative-claims
  - insights
  - self-improvement
  - language-models
  - scenarios-overview
  - factors-overview
  - institutional-quality
  - international-coordination
  - ai-control-concentration
  - compute-forecast-sketch
  - ai-research-workflows
  - causal-diagram-visualization
  - cross-link-automation-proposal
  - fraud
  - why-alignment-hard
  - scenarios-long-term-lockin-overview
  - preference-authenticity
  - reality-coherence
  - political-power
  - economic-power
  - epistemic-risks
  - regulation-debate
  - why-alignment-easy
  - case-against-xrisk
  - winner-take-all
  - instrumental-convergence-framework
  - light-scaffolding
  - agi-timeline
  - economic-labor
  - ai-impacts
  - structural-risks
  - situational-awareness
  - world-models
  - minimal-scaffolding
  - early-warnings
  - table-candidates
  - diagrams
  - samotsvety
  - robot-threat-exposure
  - economic-stability
  - scaling-laws
  - provable-safe
  - fast-takeoff
  - technical-ai-safety
  - ai-governance
  - existential-catastrophe
  - long-term-trajectory
  - human-agency
  - coordination-capacity
  - adoption
  - lab-behavior
  - alignment-deployment-overview
  - genetic-enhancement
  - safety-capability-gap
  - state-actor
  - rogue-actor
  - rapid
  - gradual
  - values
  - suffering-lock-in
  - mesa-optimization-analysis
  - capability-alignment-race
  - irreversibility-threshold
  - feedback-loops
  - fraud-sophistication-curve
  - expertise-atrophy-progression
  - model-organisms-of-misalignment
  - misuse-risks
  - whole-brain-emulation
  - table
  - power-seeking-conditions
  - reward-hacking-taxonomy
  - safety-capability-tradeoff
  - multipolar-trap-dynamics
  - risk-interaction-matrix
  - safety-culture-equilibrium
  - multi-actor-landscape
  - safety-researcher-gap
  - post-incident-recovery
  - media-policy-feedback-loop
  - safety-research-allocation
  - technical-pathways
  - whistleblower-dynamics
  - trust-erosion-dynamics
  - surveillance-chilling-effects
  - winner-take-all-concentration
  - ai-revenue-sources
  - evaluation
  - page-length-research
  - cser
  - coding
  - compute-hardware
  - collective-intelligence
  - biological-organoid
  - biosecurity-orgs-overview
  - miri-era
  - coefficient-giving
  - surprise-threat-exposure
  - coordination
  - adaptability
  - hardware-enabled-governance
  - failed-stalled-proposals
  - steganography
  - learned-helplessness
  - interpretability
  - instrumental-convergence
  - treacherous-turn
  - safety-orgs-epoch-ai
  - coalition-for-epidemic-preparedness-innovations
  - interactive-views
  - scaling-debate
  - persuasion
  - solutions
  - case-for-xrisk
  - gap-analysis-2026-02
  - elon-musk-philanthropy
  - resources
  - tags
  - cause-effect-demo
  - longtermwiki-value-proposition
  - critical-uncertainties
  - corrigibility-failure-pathways
  - flash-dynamics-threshold
  - proliferation-risk-model
  - cyberweapons-attack-automation
  - defense-in-depth-model
  - epistemic-collapse-threshold
  - multipolar-competition
  - slow-takeoff-muddle
  - misaligned-catastrophe
  - parameter-interaction-network
  - risk-activation-timeline
  - trust-cascade-model
  - goal-misgeneralization-research
  - alignment-evals
  - lab-culture
  - expert-opinion
  - deliberation
  - graph
  - compute
  - agi-timeline-debate
  - compute-governance
  - brain-computer-interfaces
  - securebio
  - seldon-lab
  - vara
  - algorithms
  - human-expertise
  - societal-resilience
  - information-authenticity
  - evaluation-awareness
  - tool-use
  - deceptive-alignment-decomposition
  - authoritarian-tools-diffusion
  - expertise-atrophy-cascade
  - interpretability-sufficient
  - common-writing-principles
  - page-types
  - models-style-guide
  - rating-system
  - goal-misgeneralization-probability
  - international-coordination-game
  - risk-cascade-pathways
  - short-timeline-policy-implications
  - disinformation-electoral-impact
  - public-opinion-evolution
  - alignment-interpretability-overview
  - coordination-mechanisms
  - epistemic-health
  - factors-civilizational-competence-epistemics
  - cyberweapons-offense-defense
  - scheming-likelihood-model
  - surveillance-authoritarian-stability
  - agi-development
  - pause-and-redirect
  - is-ai-xrisk-real
  - scenarios-long-term-lockin-epistemics
  - industries
  - governments
  - parameters-strategy
  - dangerous-cap-evals
  - alignment-evaluation-overview
  - capability-unlearning
  - california-sb53
  - corrigibility
  - cooperative-ai
  - collective-epistemics-design-sketches
  - biosecurity-overview
  - anthropic-core-views
  - arb-research
  - capability-elicitation
  - effectiveness-assessment
  - compute-monitoring
  - coordination-tech
  - content-authentication
  - epistemic-infrastructure
  - corporate
  - alignment-policy-overview
  - epistemic-tools-approaches-overview
  - ai-futures-project
  - evals-governance
  - hybrid-systems
  - eu-ai-act
  - reasoning
  - architecture-scenarios-table
  - controlled-vocabulary
  - diagram-naming-research
  - vidur-kapur
  - governance
  - safety-culture-strength
  - racing-dynamics
  - multipolar-trap
  - autonomous-weapons-escalation
  - open-vs-closed
  - labor-transition
  - preference-optimization
  - pause-moratorium
  - natural-abstractions
  - large-language-models
  - societal-trust
  - pause-debate
  - capability-threshold-model
  - ai-risk-portfolio-analysis
  - compounding-risks-analysis
  - carlsmith-six-premises
  - worldview-intervention-mapping
  - heavy-scaffolding
  - sparse-moe
  - ssm-mamba
  - neuro-symbolic
  - power-seeking
  - sleeper-agents
  - irreversibility
  - scientific-corruption
  - proliferation
  - trust-cascade
  - long-timelines
  - knowledge-base
  - cause-effect-diagrams
  - models
  - aligned-agi
  - openclaw-matplotlib-incident-2026
  - novel-unknown
  - neuromorphic
  - ai-acceleration-tradeoff
  - autonomous-weapons-proliferation
  - societal-response
  - alignment-progress
  - lightning-rod-labs
  - ai-transition-model-style-guide
  - bioweapons-attack-chain
  - alignment-robustness-trajectory
  - capabilities-to-safety-pipeline
  - deepfakes-authentication-crisis
  - deployment-architectures-table
  - geopolitics
  - claude-code-espionage-2025
  - dense-transformers
  - deep-learning-era
  - chai
  - mesa-optimization
  - scheming
  - preference-manipulation
  - institutional-capture
  - epistemic-collapse
  - expertise-atrophy
  - economic-disruption
  - deep-learning-era
  - dense-transformers
  - shareholders
  - bioweapons-ai-uplift
  - bioweapons-timeline
  - intervention-timing-windows
  - anthropic-pledge-enforcement
  - automation-bias-cascade
  - authentication-collapse-timeline
  - anthropic-impact
  - public-opinion
  - structural
  - mainstream-era
  - disinformation-detection-race
  - racing-dynamics-impact
  - regulatory-capacity-threshold
  - institutional-adaptation-speed
  - intervention-effectiveness-matrix
  - safety-research-value
  - projecting-compute-spending
  - risk-interaction-network
  - eval-types-table
  - entities
  - warning-signs-model
  - sycophancy-feedback-loop
  - anthropic-valuation
  - anthropic-ipo
  - anthropic-pre-ipo-daf-transfers
  - anthropic-investors
  - transformative-ai
  - anthropic
  - 80000-hours
  - 1day-sooner
  - ea-funding-absorption-capacity
  - ea-shareholder-diversification-anthropic
  - epistemic-orgs-epoch-ai
  - cset
  - conjecture
  - controlai
  - council-on-strategic-risks
  - blueprint-biosecurity
  - epistemic-orgs-overview
  - chan-zuckerberg-initiative
  - frontier-ai-comparison
  - fri
  - frontier-model-forum
  - elicit
  - centre-for-long-term-resilience
  - bridgewater-aia-labs
  - deepmind
  - cea
  - ea-global
  - companies
  - ftx-collapse-ea-funding-lessons
  - goodfire
  - ibbis
  - far-ai
  - govai
  - fli
  - leading-the-future
  - longtermwiki-impact
  - countries
  - directory
  - research-reports
  - risk-style-guide
  - response-style-guide
  - kalshi
  - good-judgment
  - futuresearch
  - fhi
  - gpai
  - hewlett-foundation
  - johns-hopkins-center-for-health-security
  - quri
  - palisade-research
  - metr
  - uk-aisi
  - us-aisi
  - nist-ai
  - long-term-benefit-trust
  - openai-foundation
  - metaculus
  - mats
  - intervention-portfolio
  - monitoring
  - model-registries
  - output-filtering
  - model-spec
  - mech-interp
  - open-source
  - openai-foundation-governance
  - lionheart-ventures
  - founders-fund
  - musk-openai-lawsuit
  - nti-bio
  - meta-ai
  - microsoft
  - openai
  - miri
  - ltff
  - longview-philanthropy
  - funders-overview
  - manifund
  - secure-ai-project
  - sentinel
  - swift-centre
  - securedna
  - pause-ai
  - polymarket
  - situational-awareness-lp
  - manifest
  - manifold
  - lesswrong
  - redwood-research
  - rethink-priorities
  - ssi
  - red-queen-bio
  - sff
  - schmidt-futures
  - venture-capital-overview
  - turion
  - xai
  - open-philanthropy
  - eliezer-yudkowsky-predictions
  - elon-musk-predictions
  - leopold-aschenbrenner
  - gwern
  - evan-hubinger
  - neel-nanda
  - jan-leike
  - connor-leahy
  - chris-olah
  - vitalik-buterin-philanthropy
  - eli-lifland
  - helen-toner
  - holden-karnofsky
  - jaan-tallinn
  - the-sequences
  - peter-thiel-philanthropy
  - center-for-applied-rationality
  - macarthur-foundation
  - ilya-sutskever
  - dario-amodei
  - sam-altman-predictions
  - track-records-overview
  - giving-pledge
  - lighthaven
  - gratified
  - geoffrey-hinton
  - dan-hendrycks
  - demis-hassabis
  - max-tegmark
  - elon-musk
  - ai-control
  - adversarial-robustness
  - ai-safety-institutes
  - ai-assisted
  - adversarial-training
  - ai-forecasting-benchmark
  - agent-foundations
  - ai-forecasting
  - ai-executive-order
  - yann-lecun-predictions
  - ai-assisted-knowledge-management
  - ai-for-human-reasoning-fellowship
  - content-database
  - automation-tools
  - about-this-wiki
  - longterm-strategy
  - longterm-vision
  - enhancement-queue
  - anthropic-pages-refactor-notes
  - paul-christiano
  - circuit-breakers
  - benchmarking
  - cirl
  - alignment-training-overview
  - china-ai-regulations
  - colorado-ai-act
  - coe-ai-convention
  - bletchley-declaration
  - philip-tetlock
  - robin-hanson
  - eliciting-latent-knowledge
  - compute-thresholds
  - deepfake-detection
  - epistemic-security
  - california-sb1047
  - alignment-theoretical-overview
  - canada-aida
  - ai-safety-summit
  - alignment
  - yoshua-bengio
  - eval-saturation
  - corporate-influence
  - ea-biosecurity-scope
  - epistemic-virtue-evals
  - constitutional-ai
  - debate
  - community-notes-for-everything
  - community-notes
  - ai-watch
  - content-moderation
  - formal-verification
  - evals
  - export-controls
  - government-authority-commercial-ai-infrastructure
  - field-building-analysis
  - forecastbench
  - international-summits
  - grokipedia
  - nuno-sempere
  - issa-rice
  - process-supervision
  - model-auditing
  - multi-agent
  - prediction-markets
  - nist-ai-rmf
  - maim
  - international-regimes
  - new-york-raise-act
  - metaforecast
  - governance-policy
  - scalable-eval-approaches
  - scheming-detection
  - probing
  - red-teaming
  - mit-ai-risk-repository
  - provably-safe
  - prosaic-alignment
  - pause
  - yann-lecun
  - donations-list-website
  - sleeper-agent-detection
  - scalable-oversight
  - safety-cases
  - sparse-autoencoders
  - sandboxing
  - responsible-scaling-policies
  - representation-engineering
  - reward-modeling
  - refusal-training
  - rlhf
  - research-agendas
  - rsp
  - reliability-tracking
  - provenance-tracing
  - reward-modeling
  - safety-approaches-table
  - squiggle
  - seoul-declaration
  - rhetoric-highlighting
  - public-education
  - weak-to-strong
  - thresholds
  - structured-access
  - tool-restrictions
  - voluntary-commitments
  - ai-enabled-untraceable-misuse
  - autonomous-weapons
  - value-learning
  - epistemic-tools-tools-overview
  - field-building
  - squiggleai
  - roastmypost
  - whistleblower-protections
  - texas-traiga
  - us-state-legislation
  - standards-bodies
  - training-programs
  - us-executive-order
  - authentication-collapse
  - technical-research
  - authoritarian-takeover
  - authoritarian-tools
  - enfeeblement
  - accident-risks-table
  - ai-welfare
  - xpt
  - ai-takeover
  - timelines-wiki
  - stampy-aisafety-info
  - longterm-wiki
  - bioweapons
  - corrigibility-failure
  - deceptive-alignment
  - distributional-shift
  - concentrated-compute-cybersecurity-risk
  - epistemic-sycophancy
  - compute-concentration
  - autonomous-replication
  - cyber-offense
  - emergent-capabilities
  - cyberweapons
  - bio-risk
  - consensus-manufacturing
  - disinformation
  - deepfakes
  - automation-bias
  - concentration-of-power
  - erosion-of-agency
  - x-com-epistemics
  - wikipedia-and-ai
  - sandbagging
  - goal-misgeneralization
  - flash-dynamics
  - financial-stability-risks-ai-capex
  - reward-hacking
  - rogue-ai-scenarios
  - lock-in
  - reality-fragmentation
  - knowledge-monopoly
  - legal-evidence-crisis
  - sharp-left-turn
  - sycophancy
  - trust-decline
  - historical-revisionism
  - surveillance
  - dual-use
  - cyber-psychosis
  - existential-risk
  - superintelligence
  - governance-focused
  - similar-projects
  - critical-insights
  - strategy-brainstorm
  - org-watch
  - doomer
  - optimistic
  - david-sacks
  - daniela-amodei
  - dustin-moskovitz
  - sam-altman
  - scenarios-ai-takeover-overview
  - safety-generalizability-table
  - arc
  - cais
  - stuart-russell
  - eliezer-yudkowsky
  - nick-bostrom
  - toby-ord
  - vipul-naik
  - marc-andreessen
  - vision
  - changelog
  - architecture
  - project-roadmap
  - importance-ranking
  - documentation-maintenance
  - wikipedia-views
  - stub-style-guide
  - mermaid-diagrams
