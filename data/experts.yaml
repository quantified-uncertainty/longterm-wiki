# Experts Database
# Auto-generated and cleaned - review before using

- id: dario-amodei
  name: Dario Amodei
  affiliation: anthropic
  role: Co-founder & CEO
  website: https://anthropic.com
  knownFor:
    - Constitutional AI
    - Responsible Scaling Policy
    - Claude development
  positions:
    - topic: timelines
      view: Very short
      estimate: 2026-2027
      confidence: medium
    - topic: p-doom
      view: Moderate
      estimate: 10-25%
      confidence: medium
    - topic: current-approaches-scale
      view: Cautiously optimistic
      estimate: 60%
      confidence: medium
      source: Anthropic Core Views (2023)
- id: sam-altman
  name: Sam Altman
  positions:
    - topic: timelines
      view: Short
      estimate: 2027-2030
      confidence: medium
- id: demis-hassabis
  name: Demis Hassabis
  positions:
    - topic: timelines
      view: Short-medium
      estimate: 2028-2035
      confidence: medium
- id: paul-christiano
  name: Paul Christiano
  affiliation: arc
  role: Founder
  website: https://alignment.org
  knownFor:
    - Iterated amplification
    - AI safety via debate
    - scalable oversight
  positions:
    - topic: timelines
      view: Medium
      estimate: 2035-2045
      confidence: low
    - topic: p-doom
      view: Significant
      estimate: 10-20%
      confidence: medium
    - topic: how-hard-is-alignment-
      view: Hard but tractable
      estimate: 50%
      confidence: medium
    - topic: current-approaches-scale
      view: Uncertain
      estimate: 40%
      confidence: medium
      source: ARC Research (2023)
    - topic: inner-alignment-solvability
      view: Hard but tractable
      estimate: Solvable with sufficient investment
      confidence: medium
    - topic: likelihood-of-deceptive-alignment
      view: Significant concern
      estimate: 50%
      confidence: medium
    - topic: p-ai-x-risk-this-century-
      view: Moderate
      estimate: ~20-50%
      confidence: medium
    - topic: would-misalignment-be-catastrophic-
      view: Uncertain, depends on scenario
      estimate: 20-40% → catastrophic
      confidence: low
    - topic: p-ai-catastrophe-
      view: Significant
      estimate: 10-20%
      confidence: medium
      source: Various posts (2022-2023)
    - topic: how-fast-would-takeoff-be-
      view: Slow
      estimate: 5-15 years
      confidence: medium
    - topic: will-advanced-ai-systems-be-deceptive-
      view: Possibly detectable
      estimate: 40%
      confidence: medium
    - topic: will-we-get-adequate-warning-before-catastrophic-ai-
      view: Likely
      estimate: 70%
      confidence: medium
- id: gary-marcus
  name: Gary Marcus
  positions:
    - topic: timelines
      view: Long
      estimate: 2050+
      confidence: medium
- id: eliezer-yudkowsky
  name: Eliezer Yudkowsky
  affiliation: miri
  role: Co-founder & Research Fellow
  website: https://intelligence.org
  knownFor:
    - Early AI safety work
    - decision theory
    - rationalist community
  positions:
    - topic: p-doom
      view: Very high
      estimate: ">90%"
      confidence: high
    - topic: how-hard-is-alignment-
      view: Extremely hard
      estimate: 5%
      confidence: high
    - topic: current-approaches-scale
      view: Very unlikely
      estimate: 5%
      confidence: high
      source: AGI Ruin (2022)
      sourceUrl: https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities
    - topic: inner-alignment-solvability
      view: Very hard
      estimate: Requires major theoretical breakthrough
      confidence: high
    - topic: likelihood-of-deceptive-alignment
      view: Very likely
      estimate: 85%
      confidence: high
    - topic: would-misalignment-be-catastrophic-
      view: Near-certainly catastrophic
      estimate: 95%+ → extinction
      confidence: high
    - topic: p-ai-catastrophe-
      view: Very high
      estimate: ">90%"
      confidence: high
      source: AGI Ruin (2022)
    - topic: how-fast-would-takeoff-be-
      view: Very fast
      estimate: Weeks-months
      confidence: high
    - topic: will-advanced-ai-systems-be-deceptive-
      view: Likely deceptive
      estimate: 85%
      confidence: high
    - topic: will-we-get-adequate-warning-before-catastrophic-ai-
      view: Unlikely
      estimate: 10%
      confidence: high
- id: toby-ord
  name: Toby Ord
  affiliation: fhi
  role: Senior Research Fellow in Philosophy
  website: https://www.tobyord.com
  knownFor:
    - The Precipice
    - existential risk quantification
    - effective altruism
  positions:
    - topic: p-doom
      view: Moderate
      estimate: 10%
      confidence: medium
    - topic: would-misalignment-be-catastrophic-
      view: Significant risk
      estimate: 10% → existential
      confidence: medium
    - topic: p-ai-catastrophe-
      view: Moderate
      estimate: 10%
      confidence: medium
      source: The Precipice (2020)
    - topic: will-we-get-adequate-warning-before-catastrophic-ai-
      view: Uncertain
      estimate: 50%
      confidence: low
- id: yoshua-bengio
  name: Yoshua Bengio
  affiliation: mila
  role: Scientific Director of Mila, Professor
  website: https://yoshuabengio.org
  knownFor:
    - Deep learning pioneer
    - now AI safety advocate
  positions:
    - topic: p-doom
      view: Concerned
      estimate: 10-15%
      confidence: low
    - topic: p-ai-catastrophe-
      view: Concerned
      estimate: 10-15%
      confidence: medium
      source: Public statements (2023)
- id: jan-leike
  name: Jan Leike
  affiliation: anthropic
  role: Head of Alignment
  website: https://anthropic.com
  knownFor:
    - Alignment research
    - scalable oversight
    - RLHF
    - superalignment work
  positions:
    - topic: current-approaches-scale
      view: Uncertain
      estimate: 45%
      confidence: medium
      source: OpenAI Superalignment (2023)
    - topic: will-we-get-adequate-warning-before-catastrophic-ai-
      view: Concerned
      estimate: 40%
      confidence: medium
- id: stuart-russell
  name: Stuart Russell
  affiliation: chai
  role: Professor of Computer Science, CHAI Founder
  website: https://people.eecs.berkeley.edu/~russell/
  knownFor:
    - Human Compatible
    - inverse reinforcement learning
    - AI safety advocacy
  positions:
    - topic: current-approaches-scale
      view: Unlikely without changes
      estimate: 25%
      confidence: medium
      source: Human Compatible (2019)
    - topic: p-ai-catastrophe-
      view: Concerned
      estimate: 15-25%
      confidence: medium
      source: Interviews (2021-2023)
- id: nate-soares
  name: Nate Soares (MIRI)
  positions:
    - topic: inner-alignment-solvability
      view: Extremely difficult
      estimate: Unclear if current paradigm can solve it
      confidence: medium
- id: chris-olah
  name: Chris Olah
  affiliation: anthropic
  role: Co-founder, Head of Interpretability
  website: https://colah.github.io
  knownFor:
    - Mechanistic interpretability
    - neural network visualization
    - clarity of research communication
- id: connor-leahy
  name: Connor Leahy
  affiliation: conjecture
  role: CEO & Co-founder
  website: https://conjecture.dev
  knownFor:
    - Founding Conjecture
    - AI safety advocacy
    - interpretability research
- id: dan-hendrycks
  name: Dan Hendrycks
  affiliation: cais
  role: Director
  website: https://hendrycks.com
  knownFor:
    - AI safety research
    - benchmark creation
    - CAIS leadership
    - catastrophic risk focus
- id: geoffrey-hinton
  name: Geoffrey Hinton
  affiliation: independent
  role: Professor Emeritus, AI Safety Advocate
  website: https://www.cs.toronto.edu/~hinton/
  knownFor:
    - Deep learning pioneer
    - backpropagation
    - now AI risk vocal advocate
- id: holden-karnofsky
  name: Holden Karnofsky
  affiliation: coefficient-giving
  role: Co-CEO
  website: https://www.openphilanthropy.org
  knownFor:
    - Directing billions toward AI safety
    - effective altruism leadership
    - AI timelines work
  positions:
    - topic: will-we-get-adequate-warning-before-catastrophic-ai-
      view: Probably
      estimate: 65%
      confidence: medium
- id: ilya-sutskever
  name: Ilya Sutskever
  affiliation: ssi
  role: Co-founder & Chief Scientist
  website: https://ssi.inc
  knownFor:
    - Deep learning breakthroughs
    - OpenAI leadership
    - now focused on safe superintelligence
- id: neel-nanda
  name: Neel Nanda
  affiliation: deepmind
  role: Alignment Researcher
  website: https://www.neelnanda.io
  knownFor:
    - Mechanistic interpretability
    - TransformerLens library
    - educational content
  positions:
    - topic: likelihood-of-deceptive-alignment
      view: Less likely
      estimate: 15%
      confidence: medium
    - topic: will-advanced-ai-systems-be-deceptive-
      view: Less likely
      estimate: 20%
      confidence: medium
- id: nick-bostrom
  name: Nick Bostrom
  affiliation: fhi
  role: Founding Director (until FHI closure in 2024)
  website: https://nickbostrom.com
  knownFor:
    - Superintelligence
    - existential risk research
    - simulation hypothesis

# Additional experts referenced by organizations
- id: daniela-amodei
  name: Daniela Amodei
  affiliation: anthropic
  role: Co-founder & President
  website: https://anthropic.com
  knownFor:
    - Co-founding Anthropic
    - Operations and business leadership

- id: shane-legg
  name: Shane Legg
  affiliation: deepmind
  role: Co-founder & Chief AGI Scientist
  website: https://deepmind.google
  knownFor:
    - Co-founding DeepMind
    - Early work on AGI
    - Machine super intelligence thesis

- id: buck-shlegeris
  name: Buck Shlegeris
  affiliation: redwood
  role: CEO
  website: https://redwoodresearch.org
  knownFor:
    - AI safety research
    - Redwood Research leadership

- id: ian-hogarth
  name: Ian Hogarth
  affiliation: uk-aisi
  role: Chair
  knownFor:
    - Leading UK AI Safety Institute
    - AI investor and writer

- id: elizabeth-kelly
  name: Elizabeth Kelly
  affiliation: us-aisi
  role: Director
  knownFor:
    - Leading US AI Safety Institute
    - AI policy

- id: evan-hubinger
  name: Evan Hubinger
  positions:
    - topic: likelihood-of-deceptive-alignment
      view: Possible
      estimate: 40%
      confidence: medium
      source: Risks from Learned Optimization
- id: robin-hanson
  name: Robin Hanson
  positions:
    - topic: how-fast-would-takeoff-be-
      view: Gradual
      estimate: Decades to centuries
      confidence: medium
