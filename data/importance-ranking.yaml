# Readership Importance Ranking
# Pages ordered by how important they are for readers navigating AI safety.
#
# This list is the source of truth. Scores are derived from position.
# Run `pnpm crux importance sync --apply` to write scores to frontmatter.
#
# Total ranked: 645
ranking:
  - existential-risk
  - alignment
  - factors-misalignment-potential-overview
  - factors-ai-capabilities-overview
  - superintelligence
  - why-alignment-hard
  - scenarios-ai-takeover-overview
  - factors-civilizational-competence-overview
  - agentic-ai
  - language-models
  - is-ai-xrisk-real
  - accident-risks
  - solutions
  - ai-timelines
  - scaling-laws
  - critical-uncertainties
  - outcomes-overview
  - scenarios-overview
  - factors-ai-ownership-overview
  - factors-transition-turbulence-overview
  - transformative-ai
  - existential-catastrophe
  - alignment-progress
  - reasoning
  - situational-awareness
  - autonomous-replication
  - tool-use
  - agi-timeline-debate
  - preference-manipulation
  - expertise-atrophy
  - deep-learning-era
  - alignment-interpretability-overview
  - capabilities
  - safety-research
  - expert-opinion
  - intervention-timing-windows
  - compute
  - factors-ai-uses-overview
  - case-against-xrisk
  - intervention-effectiveness-matrix
  - large-language-models
  - technical-ai-safety
  - ai-governance
  - provable-safe
  - alignment-robustness
  - ai-impacts
  - cais
  - funders-overview
  - safety-research-allocation
  - disinformation-detection-race
  - microsoft
  - rethink-priorities
  - benchmarking
  - export-controls
  - epistemic-orgs-epoch-ai
  - palisade-research
  - safety-culture-equilibrium
  - openai-foundation
  - epistemic-orgs-overview
  - dan-hendrycks
  - aligned-agi
  - structural-risks
  - alignment-robustness-trajectory
  - goal-misgeneralization-probability
  - fraud
  - miri-era
  - venture-capital-overview
  - seldon-lab
  - goodfire
  - technical-research
  - epistemic-collapse
  - safety-capability-tradeoff
  - deceptive-alignment-decomposition
  - proliferation-risk-model
  - risk-interaction-matrix
  - center-for-applied-rationality
  - far-ai
  - safety-orgs-epoch-ai
  - neel-nanda
  - multipolar-trap
  - natural-abstractions
  - goal-misgeneralization
  - public-opinion
  - media-policy-feedback-loop
  - metr
  - frontier-model-forum
  - elicit
  - nuno-sempere
  - ai-futures-project
  - factors-overview
  - pause-ai
  - cyberweapons
  - standards-bodies
  - optimistic
  - nick-bostrom
  - eliezer-yudkowsky
  - jan-leike
  - max-tegmark
  - secure-ai-project
  - epistemic-risks
  - misuse-risks
  - fast-takeoff
  - cooperative-ai
  - factors-misuse-potential-overview
  - compute-hardware
  - hewlett-foundation
  - biological-organoid
  - epistemics
  - leading-the-future
  - early-warnings
  - geopolitics
  - scheming-likelihood-model
  - dense-transformers
  - collective-intelligence
  - short-timeline-policy-implications
  - racing-dynamics-impact
  - chris-olah
  - minimal-scaffolding
  - cyber-psychosis
  - pause-moratorium
  - evaluation
  - structured-access
  - genetic-enhancement
  - compounding-risks-analysis
  - institutional-adaptation-speed
  - societal-response
  - ea-global
  - timelines-wiki
  - long-term-benefit-trust
  - authoritarian-takeover
  - winner-take-all
  - ai-takeover
  - irreversibility
  - enfeeblement
  - dual-use
  - nist-ai
  - model-auditing
  - fli
  - resources
  - capability-alignment-race
  - evan-hubinger
  - expertise-atrophy-progression
  - sycophancy-feedback-loop
  - trust-cascade-model
  - bioweapons-ai-uplift
  - surveillance-chilling-effects
  - authentication-collapse-timeline
  - manifest
  - ibbis
  - regulation-debate
  - light-scaffolding
  - world-models
  - sparse-moe
  - neuro-symbolic
  - openclaw-matplotlib-incident-2026
  - futuresearch
  - tags
  - cause-effect-demo
  - common-writing-principles
  - corrigibility-failure-pathways
  - power-seeking-conditions
  - flash-dynamics-threshold
  - parameter-interaction-network
  - ai-acceleration-tradeoff
  - model-organisms-of-misalignment
  - expertise-atrophy-cascade
  - autonomous-weapons-proliferation
  - bioweapons-attack-chain
  - cyberweapons-attack-automation
  - alignment-deployment-overview
  - alignment-policy-overview
  - coe-ai-convention
  - california-sb53
  - china-ai-regulations
  - public-opinion-evolution
  - whistleblower-dynamics
  - debate
  - anthropic
  - centre-for-long-term-resilience
  - dangerous-cap-evals
  - scheming
  - steganography
  - coordination-tech
  - corporate
  - epistemic-infrastructure
  - collective-epistemics-design-sketches
  - legal-evidence-crisis
  - canada-aida
  - table
  - alignment-theoretical-overview
  - ai-control
  - compute-monitoring
  - arc
  - chai
  - interpretability-coverage
  - recursive-ai-capabilities
  - ai-control-concentration
  - biological-threat-exposure
  - cyber-threat-exposure
  - alignment-training-overview
  - prosaic-alignment
  - epistemic-sycophancy
  - flash-dynamics
  - epistemic-security
  - governance-focused
  - adversarial-robustness
  - field-building
  - frontier-ai-comparison
  - ai-revenue-sources
  - international-summits
  - ai-executive-order
  - ai-safety-summit
  - capability-unlearning
  - disinformation
  - biosecurity-orgs-overview
  - nti-bio
  - ai-forecasting
  - content-moderation
  - ftx-collapse-ea-funding-lessons
  - governance-policy
  - alignment-evals
  - good-judgment
  - manifold
  - lightning-rod-labs
  - lionheart-ventures
  - sam-altman-predictions
  - ea-shareholder-diversification-anthropic
  - directory
  - deepfakes-authentication-crisis
  - instrumental-convergence
  - ai-safety-institutes
  - compute-governance
  - monitoring
  - international-regimes
  - responsible-scaling-policies
  - hybrid-systems
  - output-filtering
  - lab-safety-practices
  - human-oversight-quality
  - bioweapons
  - trust-decline
  - learned-helplessness
  - representation-engineering
  - public-education
  - ai-welfare
  - reliability-tracking
  - yann-lecun
  - philip-tetlock
  - donations-list-website
  - intervention-portfolio
  - defense-in-depth-model
  - international-coordination
  - coordination-capacity
  - regulatory-capacity
  - coordination
  - algorithms
  - compute-forecast-sketch
  - institutional-quality
  - epistemic-health
  - alignment-evaluation-overview
  - scenarios-human-catastrophe-overview
  - scenarios-long-term-lockin-overview
  - safety-capability-gap
  - risk-cascade-pathways
  - multipolar-trap-dynamics
  - cyberweapons-offense-defense
  - longterm-vision
  - racing-intensity
  - rapid
  - regulatory-capacity-threshold
  - emergent-capabilities
  - compute-concentration
  - sandboxing
  - scheming-detection
  - research-agendas
  - tool-restrictions
  - trust-erosion-dynamics
  - us-executive-order
  - seoul-declaration
  - proliferation
  - sharp-left-turn
  - economic-disruption
  - authentication-collapse
  - training-programs
  - thresholds
  - xpt
  - interactive-views
  - governance
  - economic-power
  - misaligned-catastrophe
  - agi-timeline
  - scaling-debate
  - scientific-research
  - long-horizon
  - political-power
  - values
  - suffering-lock-in
  - state-actor
  - gradual
  - instrumental-convergence-framework
  - irreversibility-threshold
  - risk-activation-timeline
  - mesa-optimization-analysis
  - technical-pathways
  - safety-research-value
  - persuasion
  - coding
  - long-term-trajectory
  - human-agency
  - bletchley-declaration
  - case-for-xrisk
  - anthropic-core-views
  - mit-ai-risk-repository
  - open-vs-closed
  - pause
  - why-alignment-easy
  - worldview-intervention-mapping
  - automation-bias-cascade
  - rogue-actor
  - scalable-oversight
  - safety-cases
  - rsp
  - compute-thresholds
  - sleeper-agent-detection
  - govai
  - fhi
  - 80000-hours
  - anthropic-impact
  - disinformation-electoral-impact
  - agi-development
  - capability-elicitation
  - provably-safe
  - interpretability-sufficient
  - voluntary-commitments
  - provenance-tracing
  - multipolar-competition
  - slow-takeoff-muddle
  - gpai
  - graph
  - preference-optimization
  - process-supervision
  - open-source
  - ai-enabled-untraceable-misuse
  - whistleblower-protections
  - maim
  - metaforecast
  - connor-leahy
  - founders-fund
  - longterm-strategy
  - capability-threshold-model
  - self-improvement
  - mainstream-era
  - ai-risk-portfolio-analysis
  - biosecurity-overview
  - pause-and-redirect
  - whole-brain-emulation
  - claude-code-espionage-2025
  - longview-philanthropy
  - bridgewater-aia-labs
  - pause-debate
  - samotsvety
  - swift-centre
  - schmidt-futures
  - capabilities-to-safety-pipeline
  - peter-thiel-philanthropy
  - models-style-guide
  - preference-authenticity
  - information-authenticity
  - reality-coherence
  - autonomous-weapons-escalation
  - reward-hacking-taxonomy
  - epistemic-collapse-threshold
  - risk-interaction-network
  - safety-researcher-gap
  - projecting-compute-spending
  - bioweapons-timeline
  - lab-behavior
  - novel-unknown
  - architecture-scenarios-table
  - warning-signs-model
  - goal-misgeneralization-research
  - formal-verification
  - circuit-breakers
  - fraud-sophistication-curve
  - post-incident-recovery
  - surveillance-authoritarian-stability
  - cea
  - council-on-strategic-risks
  - controlai
  - evals-governance
  - eu-ai-act
  - evaluation-awareness
  - lab-culture
  - field-building-analysis
  - failed-stalled-proposals
  - apollo-research
  - community-notes-for-everything
  - arb-research
  - ea-funding-absorption-capacity
  - interpretability
  - mech-interp
  - scalable-eval-approaches
  - model-spec
  - nist-ai-rmf
  - epistemic-tools-tools-overview
  - openai-foundation-governance
  - ea-biosecurity-scope
  - parameters-strategy
  - ai-transition-model-style-guide
  - power-seeking
  - rogue-ai-scenarios
  - institutional-capture
  - concentration-of-power
  - sandbagging
  - red-teaming
  - labor-transition
  - concentrated-compute-cybersecurity-risk
  - us-state-legislation
  - new-york-raise-act
  - carlsmith-six-premises
  - authoritarian-tools-diffusion
  - scientific-corruption
  - heavy-scaffolding
  - brain-computer-interfaces
  - neuromorphic
  - quri
  - kalshi
  - giving-pledge
  - coalition-for-epidemic-preparedness-innovations
  - feedback-loops
  - international-coordination-game
  - multi-actor-landscape
  - anthropic-pledge-enforcement
  - cser
  - conjecture
  - coefficient-giving
  - fri
  - red-queen-bio
  - eval-types-table
  - deepmind
  - deployment-architectures-table
  - structural
  - economic-labor
  - winner-take-all-concentration
  - ilya-sutskever
  - anthropic-valuation
  - ssm-mamba
  - longtermwiki-impact
  - adoption
  - cset
  - openai
  - anthropic-investors
  - lesswrong
  - chan-zuckerberg-initiative
  - elon-musk-philanthropy
  - anthropic-ipo
  - blueprint-biosecurity
  - johns-hopkins-center-for-health-security
  - 1day-sooner
  - miri
  - us-aisi
  - uk-aisi
  - redwood-research
  - ssi
  - anthropic-pre-ipo-daf-transfers
  - mats
  - metaculus
  - manifund
  - lighthaven
  - dario-amodei
  - the-sequences
  - open-philanthropy
  - ltff
  - meta-ai
  - marc-andreessen
  - xai
  - situational-awareness-lp
  - vara
  - turion
  - holden-karnofsky
  - demis-hassabis
  - sff
  - vitalik-buterin-philanthropy
  - musk-openai-lawsuit
  - sentinel
  - macarthur-foundation
  - securebio
  - securedna
  - polymarket
  - paul-christiano
  - geoffrey-hinton
  - dustin-moskovitz
  - jaan-tallinn
  - elon-musk
  - daniela-amodei
  - helen-toner
  - leopold-aschenbrenner
  - eli-lifland
  - gwern
  - sam-altman
  - yoshua-bengio
  - stuart-russell
  - toby-ord
  - evals
  - agent-foundations
  - eliezer-yudkowsky-predictions
  - elon-musk-predictions
  - david-sacks
  - issa-rice
  - adversarial-training
  - cirl
  - ai-assisted
  - track-records-overview
  - ai-assisted-knowledge-management
  - robin-hanson
  - ai-for-human-reasoning-fellowship
  - vidur-kapur
  - yann-lecun-predictions
  - vipul-naik
  - eliciting-latent-knowledge
  - corrigibility
  - constitutional-ai
  - coordination-mechanisms
  - effectiveness-assessment
  - corporate-influence
  - colorado-ai-act
  - california-sb1047
  - ai-watch
  - ai-forecasting-benchmark
  - rlhf
  - eval-saturation
  - hardware-enabled-governance
  - government-authority-commercial-ai-infrastructure
  - epistemic-tools-approaches-overview
  - epistemic-virtue-evals
  - deliberation
  - content-authentication
  - community-notes
  - deepfake-detection
  - multi-agent
  - model-registries
  - prediction-markets
  - refusal-training
  - longterm-wiki
  - doomer
  - probing
  - forecastbench
  - grokipedia
  - org-watch
  - reward-modeling
  - racing-dynamics
  - weak-to-strong
  - sparse-autoencoders
  - erosion-of-agency
  - safety-approaches-table
  - safety-generalizability-table
  - stampy-aisafety-info
  - surprise-threat-exposure
  - robot-threat-exposure
  - mesa-optimization
  - deceptive-alignment
  - accident-risks-table
  - authoritarian-tools
  - trust-cascade
  - texas-traiga
  - wikipedia-and-ai
  - x-com-epistemics
  - rhetoric-highlighting
  - roastmypost
  - treacherous-turn
  - corrigibility-failure
  - distributional-shift
  - sleeper-agents
  - autonomous-weapons
  - surveillance
  - bio-risk
  - cyber-offense
  - deepfakes
  - squiggle
  - reward-hacking
  - lock-in
  - automation-bias
  - consensus-manufacturing
  - sycophancy
  - reality-fragmentation
  - knowledge-monopoly
  - financial-stability-risks-ai-capex
  - historical-revisionism
  - long-timelines
  - squiggleai
  - gap-analysis
  - project-roadmap
  - stub-style-guide
  - wikipedia-views
  - gratified
  - scenarios-long-term-lockin-epistemics
  - factors-civilizational-competence-epistemics
  - societal-resilience
  - economic-stability
  - insights
  - controlled-vocabulary
  - gap-analysis-2026-02
  - quantitative-claims
  - research-reports
  - risk-style-guide
  - models
  - mermaid-diagrams
  - causal-diagram-visualization
  - critical-insights
  - longtermwiki-value-proposition
  - about-this-wiki
  - knowledge-base
  - page-types
  - rating-system
  - diagrams
  - ai-research-workflows
  - diagram-naming-research
  - changelog
  - content-database
  - page-creator-pipeline
  - page-length-research
  - cross-link-automation-proposal
  - automation-tools
  - architecture
  - documentation-maintenance
  - cause-effect-diagrams
  - entities
  - enhancement-queue
  - similar-projects
  - vision
  - strategy-brainstorm
  - response-style-guide
  - anthropic-pages-refactor-notes
  - table-candidates
  - societal-trust
  - adaptability
  - safety-culture-strength
  - human-expertise
  - industries
  - governments
  - companies
  - countries
  - shareholders
  - importance-ranking
  - value-learning
  - ai-accountability
  - ai-investigation-risks
  - ai-megaproject-infrastructure
  - ai-powered-investigation
  - ai-talent-market-dynamics
  - astralis-foundation
  - deanonymization
  - frontier-lab-cost-structure
  - planning-for-frontier-lab-scaling
  - pre-tai-capital-deployment
  - safety-spending-at-scale
  - singapore-consensus
  - the-foundation-layer
