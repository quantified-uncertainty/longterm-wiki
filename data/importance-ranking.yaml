# Readership Importance Ranking
# Pages ordered by how important they are for readers navigating AI safety.
#
# This list is the source of truth. Scores are derived from position.
# Run `pnpm crux importance sync --apply` to write scores to frontmatter.
#
# Total ranked: 645
ranking:
  - existential-risk
  - factors-misalignment-potential-overview
  - factors-ai-capabilities-overview
  - factors-civilizational-competence-overview
  - superintelligence
  - why-alignment-hard
  - scenarios-overview
  - outcomes-overview
  - factors-ai-ownership-overview
  - factors-transition-turbulence-overview
  - alignment
  - agentic-ai
  - language-models
  - ai-timelines
  - scaling-laws
  - accident-risks
  - solutions
  - is-ai-xrisk-real
  - scenarios-ai-takeover-overview
  - situational-awareness
  - autonomous-replication
  - preference-manipulation
  - expertise-atrophy
  - alignment-progress
  - capabilities
  - reasoning
  - critical-uncertainties
  - intervention-effectiveness-matrix
  - existential-catastrophe
  - deep-learning-era
  - transformative-ai
  - agi-timeline-debate
  - tool-use
  - factors-ai-uses-overview
  - expert-opinion
  - case-against-xrisk
  - technical-ai-safety
  - compute
  - safety-research
  - cais
  - alignment-interpretability-overview
  - provable-safe
  - intervention-timing-windows
  - safety-research-allocation
  - ai-impacts
  - epistemic-orgs-epoch-ai
  - fraud
  - disinformation-detection-race
  - rethink-priorities
  - dan-hendrycks
  - large-language-models
  - ai-governance
  - funders-overview
  - microsoft
  - export-controls
  - palisade-research
  - alignment-robustness
  - goodfire
  - seldon-lab
  - venture-capital-overview
  - aligned-agi
  - benchmarking
  - safety-culture-equilibrium
  - surveillance-chilling-effects
  - epistemic-orgs-overview
  - openai-foundation
  - safety-orgs-epoch-ai
  - center-for-applied-rationality
  - elicit
  - biological-organoid
  - value-learning
  - structural-risks
  - alignment-robustness-trajectory
  - goal-misgeneralization-probability
  - miri-era
  - bioweapons-ai-uplift
  - ea-global
  - manifest
  - neel-nanda
  - nuno-sempere
  - technical-research
  - risk-interaction-matrix
  - safety-capability-tradeoff
  - deceptive-alignment-decomposition
  - epistemic-collapse
  - far-ai
  - media-policy-feedback-loop
  - public-opinion
  - resources
  - gap-analysis
  - proliferation-risk-model
  - trust-cascade-model
  - multipolar-trap
  - natural-abstractions
  - metr
  - frontier-model-forum
  - pause-ai
  - ai-futures-project
  - cyber-threat-exposure
  - biological-threat-exposure
  - goal-misgeneralization
  - eliezer-yudkowsky
  - nick-bostrom
  - jan-leike
  - optimistic
  - cyberweapons
  - standards-bodies
  - secure-ai-project
  - max-tegmark
  - leading-the-future
  - factors-overview
  - epistemic-risks
  - misuse-risks
  - factors-misuse-potential-overview
  - controlled-vocabulary
  - hewlett-foundation
  - timelines-wiki
  - cyber-psychosis
  - epistemics
  - gap-analysis-2026-02
  - cooperative-ai
  - fast-takeoff
  - compute-hardware
  - early-warnings
  - scheming-likelihood-model
  - collective-intelligence
  - dense-transformers
  - chris-olah
  - minimal-scaffolding
  - genetic-enhancement
  - short-timeline-policy-implications
  - racing-dynamics-impact
  - geopolitics
  - compounding-risks-analysis
  - institutional-adaptation-speed
  - societal-response
  - long-term-benefit-trust
  - expertise-atrophy-progression
  - sycophancy-feedback-loop
  - authentication-collapse-timeline
  - evaluation
  - structured-access
  - pause-moratorium
  - authoritarian-takeover
  - winner-take-all
  - dual-use
  - enfeeblement
  - irreversibility
  - nist-ai
  - causal-diagram-visualization
  - ai-takeover
  - evan-hubinger
  - fli
  - model-auditing
  - ibbis
  - futuresearch
  - tags
  - research-reports
  - models
  - cause-effect-demo
  - capability-alignment-race
  - insights
  - content-database
  - mermaid-diagrams
  - ai-control-concentration
  - economic-power
  - coordination-capacity
  - factors-civilizational-competence-epistemics
  - economic-stability
  - algorithms
  - table
  - regulation-debate
  - light-scaffolding
  - world-models
  - sparse-moe
  - neuro-symbolic
  - openclaw-matplotlib-incident-2026
  - quantitative-claims
  - common-writing-principles
  - recursive-ai-capabilities
  - corrigibility-failure-pathways
  - power-seeking-conditions
  - flash-dynamics-threshold
  - ai-acceleration-tradeoff
  - parameter-interaction-network
  - model-organisms-of-misalignment
  - bioweapons-attack-chain
  - autonomous-weapons-proliferation
  - cyberweapons-attack-automation
  - expertise-atrophy-cascade
  - public-opinion-evolution
  - whistleblower-dynamics
  - alignment-deployment-overview
  - alignment-policy-overview
  - coe-ai-convention
  - california-sb53
  - china-ai-regulations
  - anthropic
  - centre-for-long-term-resilience
  - debate
  - dangerous-cap-evals
  - scheming
  - steganography
  - coordination-tech
  - corporate
  - epistemic-infrastructure
  - legal-evidence-crisis
  - collective-epistemics-design-sketches
  - canada-aida
  - interpretability-coverage
  - compute-monitoring
  - arc
  - chai
  - frontier-ai-comparison
  - ai-revenue-sources
  - risk-style-guide
  - diagrams
  - good-judgment
  - ftx-collapse-ea-funding-lessons
  - ea-shareholder-diversification-anthropic
  - alignment-theoretical-overview
  - ai-control
  - epistemic-security
  - flash-dynamics
  - epistemic-sycophancy
  - adversarial-robustness
  - governance-focused
  - field-building
  - nti-bio
  - biosecurity-orgs-overview
  - alignment-training-overview
  - prosaic-alignment
  - capability-unlearning
  - international-summits
  - ai-safety-summit
  - ai-executive-order
  - ai-forecasting
  - manifold
  - directory
  - human-oversight-quality
  - disinformation
  - content-moderation
  - lightning-rod-labs
  - lionheart-ventures
  - sam-altman-predictions
  - deepfakes-authentication-crisis
  - lab-safety-practices
  - compute-forecast-sketch
  - coordination
  - adaptability
  - governance-policy
  - alignment-evals
  - ai-safety-institutes
  - international-regimes
  - monitoring
  - responsible-scaling-policies
  - hybrid-systems
  - output-filtering
  - international-coordination
  - regulatory-capacity
  - compute-governance
  - instrumental-convergence
  - bioweapons
  - representation-engineering
  - public-education
  - philip-tetlock
  - ai-welfare
  - trust-decline
  - learned-helplessness
  - reliability-tracking
  - critical-insights
  - yann-lecun
  - donations-list-website
  - longterm-vision
  - preference-authenticity
  - information-authenticity
  - reality-coherence
  - institutional-quality
  - epistemic-health
  - human-agency
  - intervention-portfolio
  - defense-in-depth-model
  - safety-capability-gap
  - scenarios-human-catastrophe-overview
  - scenarios-long-term-lockin-overview
  - cyberweapons-offense-defense
  - governance
  - racing-intensity
  - political-power
  - rapid
  - risk-cascade-pathways
  - alignment-evaluation-overview
  - multipolar-trap-dynamics
  - regulatory-capacity-threshold
  - scheming-detection
  - sandboxing
  - research-agendas
  - trust-erosion-dynamics
  - tool-restrictions
  - seoul-declaration
  - us-executive-order
  - compute-concentration
  - emergent-capabilities
  - proliferation
  - thresholds
  - economic-disruption
  - training-programs
  - sharp-left-turn
  - authentication-collapse
  - xpt
  - interactive-views
  - about-this-wiki
  - changelog
  - gradual
  - long-term-trajectory
  - adoption
  - values
  - suffering-lock-in
  - scenarios-long-term-lockin-epistemics
  - state-actor
  - misaligned-catastrophe
  - agi-timeline
  - scaling-debate
  - scientific-research
  - long-horizon
  - coding
  - persuasion
  - instrumental-convergence-framework
  - surprise-threat-exposure
  - rogue-actor
  - irreversibility-threshold
  - risk-activation-timeline
  - mesa-optimization-analysis
  - technical-pathways
  - safety-research-value
  - bletchley-declaration
  - mit-ai-risk-repository
  - anthropic-core-views
  - pause
  - graph
  - case-for-xrisk
  - why-alignment-easy
  - open-vs-closed
  - worldview-intervention-mapping
  - automation-bias-cascade
  - govai
  - 80000-hours
  - anthropic-impact
  - disinformation-electoral-impact
  - longtermwiki-value-proposition
  - fhi
  - scalable-oversight
  - safety-cases
  - compute-thresholds
  - rsp
  - sleeper-agent-detection
  - capability-elicitation
  - provably-safe
  - gpai
  - provenance-tracing
  - voluntary-commitments
  - agi-development
  - interpretability-sufficient
  - multipolar-competition
  - slow-takeoff-muddle
  - ai-research-workflows
  - diagram-naming-research
  - knowledge-base
  - models-style-guide
  - longterm-strategy
  - preference-optimization
  - process-supervision
  - open-source
  - ai-enabled-untraceable-misuse
  - whistleblower-protections
  - maim
  - connor-leahy
  - founders-fund
  - metaforecast
  - bridgewater-aia-labs
  - capability-threshold-model
  - ai-risk-portfolio-analysis
  - mainstream-era
  - pause-and-redirect
  - whole-brain-emulation
  - claude-code-espionage-2025
  - page-types
  - rating-system
  - cause-effect-diagrams
  - longview-philanthropy
  - self-improvement
  - biosecurity-overview
  - samotsvety
  - swift-centre
  - schmidt-futures
  - peter-thiel-philanthropy
  - societal-resilience
  - societal-trust
  - safety-culture-strength
  - human-expertise
  - pause-debate
  - capabilities-to-safety-pipeline
  - autonomous-weapons-escalation
  - reward-hacking-taxonomy
  - lab-behavior
  - bioweapons-timeline
  - epistemic-collapse-threshold
  - projecting-compute-spending
  - novel-unknown
  - architecture-scenarios-table
  - safety-researcher-gap
  - risk-interaction-network
  - warning-signs-model
  - fraud-sophistication-curve
  - post-incident-recovery
  - surveillance-authoritarian-stability
  - cea
  - council-on-strategic-risks
  - controlai
  - enhancement-queue
  - goal-misgeneralization-research
  - circuit-breakers
  - formal-verification
  - evals-governance
  - evaluation-awareness
  - eu-ai-act
  - community-notes-for-everything
  - apollo-research
  - arb-research
  - ea-funding-absorption-capacity
  - lab-culture
  - field-building-analysis
  - failed-stalled-proposals
  - openai-foundation-governance
  - epistemic-tools-tools-overview
  - ea-biosecurity-scope
  - parameters-strategy
  - ai-transition-model-style-guide
  - page-length-research
  - documentation-maintenance
  - interpretability
  - mech-interp
  - model-spec
  - scalable-eval-approaches
  - nist-ai-rmf
  - labor-transition
  - rogue-ai-scenarios
  - institutional-capture
  - us-state-legislation
  - new-york-raise-act
  - red-teaming
  - power-seeking
  - sandbagging
  - concentrated-compute-cybersecurity-risk
  - concentration-of-power
  - scientific-corruption
  - heavy-scaffolding
  - kalshi
  - giving-pledge
  - coalition-for-epidemic-preparedness-innovations
  - carlsmith-six-premises
  - authoritarian-tools-diffusion
  - brain-computer-interfaces
  - neuromorphic
  - quri
  - red-queen-bio
  - anthropic-pledge-enforcement
  - page-creator-pipeline
  - table-candidates
  - automation-tools
  - feedback-loops
  - international-coordination-game
  - multi-actor-landscape
  - eval-types-table
  - cser
  - conjecture
  - coefficient-giving
  - fri
  - ilya-sutskever
  - robot-threat-exposure
  - deployment-architectures-table
  - economic-labor
  - structural
  - winner-take-all-concentration
  - ssm-mamba
  - entities
  - anthropic-valuation
  - cross-link-automation-proposal
  - longtermwiki-impact
  - architecture
  - deepmind
  - cset
  - anthropic-investors
  - lesswrong
  - chan-zuckerberg-initiative
  - elon-musk-philanthropy
  - anthropic-ipo
  - blueprint-biosecurity
  - johns-hopkins-center-for-health-security
  - 1day-sooner
  - openai
  - miri
  - us-aisi
  - uk-aisi
  - redwood-research
  - mats
  - manifund
  - metaculus
  - lighthaven
  - anthropic-pre-ipo-daf-transfers
  - ssi
  - dario-amodei
  - meta-ai
  - open-philanthropy
  - ltff
  - xai
  - marc-andreessen
  - situational-awareness-lp
  - vara
  - turion
  - the-sequences
  - sff
  - musk-openai-lawsuit
  - vitalik-buterin-philanthropy
  - holden-karnofsky
  - macarthur-foundation
  - sentinel
  - securebio
  - securedna
  - polymarket
  - demis-hassabis
  - geoffrey-hinton
  - dustin-moskovitz
  - jaan-tallinn
  - elon-musk
  - daniela-amodei
  - leopold-aschenbrenner
  - helen-toner
  - eli-lifland
  - gwern
  - paul-christiano
  - yoshua-bengio
  - sam-altman
  - stuart-russell
  - toby-ord
  - evals
  - elon-musk-predictions
  - eliezer-yudkowsky-predictions
  - david-sacks
  - issa-rice
  - agent-foundations
  - adversarial-training
  - ai-assisted
  - ai-assisted-knowledge-management
  - robin-hanson
  - track-records-overview
  - ai-for-human-reasoning-fellowship
  - vidur-kapur
  - vipul-naik
  - yann-lecun-predictions
  - cirl
  - corrigibility
  - eliciting-latent-knowledge
  - constitutional-ai
  - coordination-mechanisms
  - effectiveness-assessment
  - colorado-ai-act
  - california-sb1047
  - ai-forecasting-benchmark
  - ai-watch
  - corporate-influence
  - government-authority-commercial-ai-infrastructure
  - hardware-enabled-governance
  - eval-saturation
  - epistemic-tools-approaches-overview
  - community-notes
  - deliberation
  - content-authentication
  - deepfake-detection
  - epistemic-virtue-evals
  - rlhf
  - multi-agent
  - model-registries
  - prediction-markets
  - probing
  - longterm-wiki
  - forecastbench
  - doomer
  - grokipedia
  - org-watch
  - refusal-training
  - reward-modeling
  - sparse-autoencoders
  - racing-dynamics
  - erosion-of-agency
  - safety-approaches-table
  - safety-generalizability-table
  - stampy-aisafety-info
  - rhetoric-highlighting
  - roastmypost
  - weak-to-strong
  - mesa-optimization
  - accident-risks-table
  - authoritarian-tools
  - trust-cascade
  - squiggle
  - squiggleai
  - texas-traiga
  - wikipedia-and-ai
  - x-com-epistemics
  - deceptive-alignment
  - treacherous-turn
  - corrigibility-failure
  - distributional-shift
  - sleeper-agents
  - autonomous-weapons
  - surveillance
  - bio-risk
  - cyber-offense
  - industries
  - deepfakes
  - reward-hacking
  - lock-in
  - automation-bias
  - consensus-manufacturing
  - sycophancy
  - knowledge-monopoly
  - reality-fragmentation
  - financial-stability-risks-ai-capex
  - historical-revisionism
  - long-timelines
  - governments
  - companies
  - countries
  - shareholders
  - similar-projects
  - vision
  - strategy-brainstorm
  - project-roadmap
  - response-style-guide
  - stub-style-guide
  - anthropic-pages-refactor-notes
  - wikipedia-views
  - gratified
  - about
