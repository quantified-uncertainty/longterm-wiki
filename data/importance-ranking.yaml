# Importance Ranking
# Pages ordered by importance to AI safety (most important first).
# This list is the source of truth for importance scores.
# Run `pnpm crux importance sync` to derive 0-100 scores from this ordering.
#
# To rank a new page: read the list, decide where it belongs relative to
# its neighbors, and insert it. The position IS the importance judgment.
#
# Total ranked: 645
ranking:
  - eval-saturation
  - evaluation-awareness
  - alignment
  - scalable-eval-approaches
  - intervention-portfolio
  - intervention-effectiveness-matrix
  - power-seeking
  - case-for-xrisk
  - treacherous-turn
  - ai-acceleration-tradeoff
  - ai-control
  - scheming-detection
  - california-sb53
  - deceptive-alignment
  - leading-the-future
  - openai-foundation
  - sleeper-agents
  - dangerous-cap-evals
  - rlhf
  - situational-awareness
  - sandbagging
  - racing-dynamics
  - evaluation
  - governance-policy
  - misuse-risks
  - solutions
  - ai-assisted
  - alignment-progress
  - bioweapons
  - capability-elicitation
  - capability-threshold-model
  - capability-alignment-race
  - carlsmith-six-premises
  - corrigibility-failure
  - defense-in-depth-model
  - evals
  - instrumental-convergence
  - intervention-timing-windows
  - interpretability
  - mesa-optimization
  - metr
  - multipolar-trap
  - reasoning
  - responsible-scaling-policies
  - risk-activation-timeline
  - scheming
  - self-improvement
  - sleeper-agent-detection
  - technical-research
  - ai-revenue-sources
  - ai-risk-portfolio-analysis
  - safety-cases
  - language-models
  - long-horizon
  - openai-foundation-governance
  - coding
  - capabilities-to-safety-pipeline
  - sharp-left-turn
  - reward-hacking
  - accident-risks
  - research-agendas
  - capabilities
  - lab-culture
  - ai-safety-institutes
  - safety-researcher-gap
  - bioweapons-ai-uplift
  - bioweapons-timeline
  - authoritarian-takeover
  - alignment-evals
  - cyberweapons-attack-automation
  - compute-hardware
  - monitoring
  - thresholds
  - corporate
  - corrigibility
  - emergent-capabilities
  - evals-governance
  - goal-misgeneralization
  - hardware-enabled-governance
  - institutional-adaptation-speed
  - instrumental-convergence-framework
  - mech-interp
  - pause-moratorium
  - pause
  - persuasion
  - effectiveness-assessment
  - power-seeking-conditions
  - proliferation
  - racing-dynamics-impact
  - rsp
  - risk-cascade-pathways
  - risk-interaction-network
  - safety-research-allocation
  - safety-capability-tradeoff
  - sandboxing
  - scheming-likelihood-model
  - scientific-research
  - short-timeline-policy-implications
  - structured-access
  - tool-use
  - tool-restrictions
  - voluntary-commitments
  - warning-signs-model
  - worldview-intervention-mapping
  - anthropic-investors
  - anthropic-valuation
  - eu-ai-act
  - long-term-benefit-trust
  - musk-openai-lawsuit
  - rogue-ai-scenarios
  - agentic-ai
  - coordination-tech
  - autonomous-weapons-escalation
  - eliciting-latent-knowledge
  - scalable-oversight
  - weak-to-strong
  - proliferation-risk-model
  - bioweapons-attack-chain
  - corrigibility-failure-pathways
  - multi-agent
  - multipolar-trap-dynamics
  - risk-interaction-matrix
  - safety-research-value
  - coordination-mechanisms
  - model-registries
  - model-auditing
  - training-programs
  - ai-welfare
  - anthropic-ipo
  - bletchley-declaration
  - compute-concentration
  - compute-governance
  - controlai
  - david-sacks
  - ea-shareholder-diversification-anthropic
  - frontier-model-forum
  - johns-hopkins-center-for-health-security
  - funders-overview
  - marc-andreessen
  - max-tegmark
  - new-york-raise-act
  - nist-ai
  - palisade-research
  - philip-tetlock
  - ssi
  - agi-timeline
  - flash-dynamics
  - critical-uncertainties
  - structural-risks
  - california-sb1047
  - red-teaming
  - regulatory-capacity-threshold
  - circuit-breakers
  - consensus-manufacturing
  - dense-transformers
  - flash-dynamics-threshold
  - export-controls
  - distributional-shift
  - epistemic-risks
  - field-building-analysis
  - debate
  - standards-bodies
  - surveillance-authoritarian-stability
  - lock-in
  - whistleblower-protections
  - concentration-of-power
  - institutional-capture
  - epistemic-security
  - hybrid-systems
  - irreversibility
  - alignment-robustness-trajectory
  - authentication-collapse-timeline
  - authoritarian-tools-diffusion
  - biosecurity-overview
  - capability-unlearning
  - constitutional-ai
  - corporate-influence
  - cyberweapons-offense-defense
  - cyberweapons
  - deceptive-alignment-decomposition
  - failed-stalled-proposals
  - feedback-loops
  - formal-verification
  - goal-misgeneralization-probability
  - goal-misgeneralization-research
  - governance-focused
  - heavy-scaffolding
  - international-coordination-game
  - international-summits
  - international-regimes
  - irreversibility-threshold
  - ea-biosecurity-scope
  - lab-behavior
  - large-language-models
  - misaligned-catastrophe
  - model-organisms-of-misalignment
  - multi-actor-landscape
  - open-source
  - parameter-interaction-network
  - preference-optimization
  - process-supervision
  - provable-safe
  - provably-safe
  - refusal-training
  - representation-engineering
  - reward-hacking-taxonomy
  - safety-culture-equilibrium
  - safety-research
  - slow-takeoff-muddle
  - sparse-autoencoders
  - technical-pathways
  - us-executive-order
  - us-state-legislation
  - whistleblower-dynamics
  - winner-take-all-concentration
  - anthropic-pledge-enforcement
  - anthropic-impact
  - anthropic-pre-ipo-daf-transfers
  - centre-for-long-term-resilience
  - china-ai-regulations
  - eli-lifland
  - epistemic-orgs-overview
  - goodfire
  - maim
  - projecting-compute-spending
  - mesa-optimization-analysis
  - trust-cascade-model
  - expert-opinion
  - biosecurity-orgs-overview
  - dustin-moskovitz
  - ea-funding-absorption-capacity
  - elon-musk-philanthropy
  - financial-stability-risks-ai-capex
  - ftx-collapse-ea-funding-lessons
  - automation-bias-cascade
  - autonomous-weapons-proliferation
  - seoul-declaration
  - why-alignment-easy
  - us-aisi
  - anthropic-core-views
  - compounding-risks-analysis
  - geopolitics
  - surveillance
  - open-vs-closed
  - sycophancy-feedback-loop
  - agent-foundations
  - ai-impacts
  - cea
  - claude-code-espionage-2025
  - concentrated-compute-cybersecurity-risk
  - coe-ai-convention
  - epistemic-collapse
  - frontier-ai-comparison
  - mats
  - meta-ai
  - openai
  - peter-thiel-philanthropy
  - redwood-research
  - robin-hanson
  - schmidt-futures
  - government-authority-commercial-ai-infrastructure
  - content-authentication
  - winner-take-all
  - epistemic-infrastructure
  - enfeeblement
  - autonomous-weapons
  - xpt
  - output-filtering
  - adversarial-training
  - preference-manipulation
  - deliberation
  - authentication-collapse
  - authoritarian-tools
  - collective-intelligence
  - cooperative-ai
  - disinformation-electoral-impact
  - epistemic-collapse-threshold
  - epistemic-sycophancy
  - safety-orgs-epoch-ai
  - forecastbench
  - regulation-debate
  - interpretability-sufficient
  - long-timelines
  - multipolar-competition
  - neuro-symbolic
  - novel-unknown
  - pause-and-redirect
  - prediction-markets
  - reward-modeling
  - surveillance-chilling-effects
  - sycophancy
  - why-alignment-hard
  - expertise-atrophy-cascade
  - agi-development
  - public-education
  - trust-decline
  - anthropic
  - apollo-research
  - coefficient-giving
  - colorado-ai-act
  - cirl
  - light-scaffolding
  - nist-ai-rmf
  - optimistic
  - probing
  - scientific-corruption
  - societal-response
  - sparse-moe
  - uk-aisi
  - world-models
  - model-spec
  - ai-forecasting
  - mit-ai-risk-repository
  - steganography
  - expertise-atrophy
  - automation-bias
  - learned-helplessness
  - trust-erosion-dynamics
  - ai-enabled-untraceable-misuse
  - collective-epistemics-design-sketches
  - epistemic-virtue-evals
  - nti-bio
  - openclaw-matplotlib-incident-2026
  - vitalik-buterin-philanthropy
  - wikipedia-and-ai
  - x-com-epistemics
  - ssm-mamba
  - case-against-xrisk
  - deepfake-detection
  - disinformation
  - fri
  - media-policy-feedback-loop
  - agi-timeline-debate
  - arc
  - knowledge-monopoly
  - expertise-atrophy-progression
  - post-incident-recovery
  - reliability-tracking
  - reality-fragmentation
  - deepfakes
  - erosion-of-agency
  - ltff
  - structural
  - public-opinion
  - provenance-tracing
  - community-notes-for-everything
  - longtermwiki-impact
  - securedna
  - stampy-aisafety-info
  - the-sequences
  - disinformation-detection-race
  - rhetoric-highlighting
  - aligned-agi
  - securebio
  - deepfakes-authentication-crisis
  - ai-for-human-reasoning-fellowship
  - trust-cascade
  - ai-assisted-knowledge-management
  - arb-research
  - coalition-for-epidemic-preparedness-innovations
  - founders-fund
  - futuresearch
  - giving-pledge
  - gpai
  - good-judgment
  - ibbis
  - lionheart-ventures
  - nuno-sempere
  - seldon-lab
  - texas-traiga
  - vidur-kapur
  - hewlett-foundation
  - xai
  - blueprint-biosecurity
  - deep-learning-era
  - sff
  - economic-disruption
  - labor-transition
  - fraud-sophistication-curve
  - historical-revisionism
  - jaan-tallinn
  - minimal-scaffolding
  - public-opinion-evolution
  - 1day-sooner
  - 80000-hours
  - ai-forecasting-benchmark
  - legal-evidence-crisis
  - cyber-psychosis
  - fraud
  - cais
  - canada-aida
  - economic-labor
  - epistemic-orgs-epoch-ai
  - fli
  - govai
  - grokipedia
  - scaling-debate
  - mainstream-era
  - metaculus
  - microsoft
  - neuromorphic
  - table
  - pause-ai
  - red-queen-bio
  - pause-debate
  - community-notes
  - demis-hassabis
  - vara
  - venture-capital-overview
  - biological-organoid
  - chai
  - squiggle
  - longview-philanthropy
  - miri
  - manifund
  - accident-risks-table
  - doomer
  - ai-watch
  - chan-zuckerberg-initiative
  - ea-global
  - early-warnings
  - issa-rice
  - manifest
  - safety-approaches-table
  - swift-centre
  - timelines-wiki
  - fhi
  - far-ai
  - architecture-scenarios-table
  - deployment-architectures-table
  - eval-types-table
  - safety-generalizability-table
  - ai-futures-project
  - quri
  - samotsvety
  - conjecture
  - miri-era
  - council-on-strategic-risks
  - deepmind
  - holden-karnofsky
  - ilya-sutskever
  - is-ai-xrisk-real
  - lesswrong
  - lighthaven
  - lightning-rod-labs
  - org-watch
  - paul-christiano
  - roastmypost
  - stuart-russell
  - wikipedia-views
  - yoshua-bengio
  - whole-brain-emulation
  - yann-lecun
  - cset
  - dario-amodei
  - toby-ord
  - critical-insights
  - donations-list-website
  - elon-musk-predictions
  - secure-ai-project
  - yann-lecun-predictions
  - brain-computer-interfaces
  - cser
  - jan-leike
  - leopold-aschenbrenner
  - nick-bostrom
  - sam-altman
  - sentinel
  - squiggleai
  - elon-musk
  - geoffrey-hinton
  - chris-olah
  - eliezer-yudkowsky
  - center-for-applied-rationality
  - dan-hendrycks
  - elicit
  - genetic-enhancement
  - macarthur-foundation
  - manifold
  - rethink-priorities
  - sam-altman-predictions
  - evan-hubinger
  - helen-toner
  - neel-nanda
  - about
  - resources
  - gratified
  - insights
  - kalshi
  - metaforecast
  - polymarket
  - similar-projects
  - suffering-lock-in
  - eliezer-yudkowsky-predictions
  - connor-leahy
  - daniela-amodei
  - gwern
  - longterm-wiki
  - situational-awareness-lp
  - bridgewater-aia-labs
  - cause-effect-demo
  - controlled-vocabulary
  - risk-style-guide
  - turion
  - common-writing-principles
  - compute-monitoring
  - about-this-wiki
  - tags
  - graph
  - gap-analysis
  - models
  - models-style-guide
  - page-types
  - quantitative-claims
  - rating-system
  - response-style-guide
  - vipul-naik
  - ai-transition-model-style-guide
  - causal-diagram-visualization
  - knowledge-base
  - mermaid-diagrams
  - project-roadmap
  - research-reports
  - page-creator-pipeline
  - adaptability
  - adoption
  - adversarial-robustness
  - factors-ai-capabilities-overview
  - content-moderation
  - ai-control-concentration
  - ai-executive-order
  - ai-governance
  - factors-ai-ownership-overview
  - companies
  - countries
  - shareholders
  - field-building
  - ai-safety-summit
  - scaling-laws
  - scenarios-ai-takeover-overview
  - ai-takeover
  - ai-timelines
  - factors-ai-uses-overview
  - value-learning
  - ai-research-workflows
  - algorithms
  - alignment-robustness
  - anthropic-pages-refactor-notes
  - epistemic-tools-approaches-overview
  - automation-tools
  - autonomous-replication
  - benchmarking
  - bio-risk
  - biological-threat-exposure
  - cause-effect-diagrams
  - changelog
  - factors-civilizational-competence-overview
  - compute
  - compute-forecast-sketch
  - compute-thresholds
  - directory
  - content-database
  - coordination
  - coordination-capacity
  - cross-link-automation-proposal
  - cyber-offense
  - cyber-threat-exposure
  - alignment-deployment-overview
  - documentation-maintenance
  - dual-use
  - economic-power
  - economic-stability
  - enhancement-queue
  - entities
  - epistemics
  - epistemic-health
  - scenarios-long-term-lockin-epistemics
  - factors-civilizational-competence-epistemics
  - alignment-evaluation-overview
  - existential-catastrophe
  - existential-risk
  - diagram-naming-research
  - fast-takeoff
  - governance
  - governments
  - gradual
  - human-agency
  - human-expertise
  - human-oversight-quality
  - scenarios-human-catastrophe-overview
  - industries
  - information-authenticity
  - institutional-quality
  - interactive-views
  - international-coordination
  - alignment-interpretability-overview
  - interpretability-coverage
  - lab-safety-practices
  - scenarios-long-term-lockin-overview
  - long-term-trajectory
  - longterm-strategy
  - strategy-brainstorm
  - longtermwiki-value-proposition
  - longterm-vision
  - vision
  - factors-misalignment-potential-overview
  - factors-misuse-potential-overview
  - natural-abstractions
  - open-philanthropy
  - parameters-strategy
  - alignment-policy-overview
  - political-power
  - preference-authenticity
  - prosaic-alignment
  - racing-intensity
  - rapid
  - reality-coherence
  - recursive-ai-capabilities
  - regulatory-capacity
  - page-length-research
  - robot-threat-exposure
  - rogue-actor
  - factors-overview
  - safety-culture-strength
  - safety-capability-gap
  - diagrams
  - societal-resilience
  - societal-trust
  - state-actor
  - stub-style-guide
  - superintelligence
  - surprise-threat-exposure
  - architecture
  - table-candidates
  - technical-ai-safety
  - alignment-theoretical-overview
  - epistemic-tools-tools-overview
  - track-records-overview
  - alignment-training-overview
  - transformative-ai
  - factors-transition-turbulence-overview
  - outcomes-overview
  - scenarios-overview
  - values
  - gap-analysis-2026-02
