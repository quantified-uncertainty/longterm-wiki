pageId: the-sequences
verifiedAt: 2026-02-22
totalCitations: 46
verified: 30
broken: 16
unverifiable: 0
citations:
  - footnote: 1
    url: https://www.lesswrong.com/tag/the-sequences
    linkText: The Sequences - LessWrong
    claimContext: '**The Sequences** is a comprehensive collection of blog posts written by <EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink> between 2006 and 2009, originally published on Overcoming Bias and <EntityLink id="lesswrong">LessWrong</EntityLink>.[^1][^2] The essays focus on the science and'
    fetchedAt: 2026-02-22T02:11:02.744Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 2
    url: https://forum.effectivealtruism.org/
    linkText: "EA Forum: Rationality Book Club"
    claimContext: '**The Sequences** is a comprehensive collection of blog posts written by <EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink> between 2006 and 2009, originally published on Overcoming Bias and <EntityLink id="lesswrong">LessWrong</EntityLink>.[^1][^2] The essays focus on the science and'
    fetchedAt: 2026-02-22T02:11:02.206Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812539
    status: verified
    note: null
  - footnote: 3
    url: https://intelligence.org/rationality-ai-zombies/
    linkText: "Rationality: From AI to Zombies - MIRI"
    claimContext: '**The Sequences** is a comprehensive collection of blog posts written by <EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink> between 2006 and 2009, originally published on Overcoming Bias and <EntityLink id="lesswrong">LessWrong</EntityLink>.[^1][^2] The essays focus on the science and'
    fetchedAt: 2026-02-22T02:11:02.827Z
    httpStatus: 200
    pageTitle: "Rationality: From AI to Zombies - Machine Intelligence Research Institute"
    contentSnippet: "Rationality: From AI to Zombies - Machine Intelligence Research Institute Skip to content Rationality: From AI to Zombies A series by Eliezer Yudkowsky If you live in an urban area, you probably don’t need to walk very far to find a martial arts dojo. Why aren’t there dojos that teach rationality? Very recently—in just the last few decades—the human species has acquired a great deal of new knowledge about human rationality. Experimental investigations of empirical human psychology; and theoretic"
    contentLength: 93275
    status: verified
    note: null
  - footnote: 4
    url: https://www.lesswrong.com/tag/the-sequences
    linkText: The Sequences Overview - LessWrong
    claimContext: Yudkowsky's stated goal was to create a comprehensive guide to rationality by developing techniques and mental models to overcome cognitive biases, refine decision-making, and update beliefs using Bayesian reasoning. The essays emphasize distinguishing mental models ("map") from reality ("territory"
    fetchedAt: 2026-02-22T02:11:03.297Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 5
    url: https://forum.effectivealtruism.org/
    linkText: "EA Forum: Rationalist Movement Discussion"
    claimContext: Yudkowsky's stated goal was to create a comprehensive guide to rationality by developing techniques and mental models to overcome cognitive biases, refine decision-making, and update beliefs using Bayesian reasoning. The essays emphasize distinguishing mental models ("map") from reality ("territory"
    fetchedAt: 2026-02-22T02:11:02.477Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812537
    status: verified
    note: null
  - footnote: 6
    url: https://forum.effectivealtruism.org/
    linkText: "EA Forum: Sequences and AI Alignment"
    claimContext: "While The Sequences are primarily framed as a guide to rationality, they contain foundational epistemology that enables readers to develop better models for understanding AI alignment risks. In the latter sections, essays related to AI alignment appear frequently, with entire sequence sections like "
    fetchedAt: 2026-02-22T02:11:04.420Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812537
    status: verified
    note: null
  - footnote: 7
    url: https://en.wikipedia.org/wiki/Eliezer_Yudkowsky
    linkText: Eliezer Yudkowsky Biography
    claimContext: <EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink> began writing The Sequences as daily blog posts starting in 2006, initially on Overcoming Bias (where <EntityLink id="robin-hanson">Robin Hanson</EntityLink> was a principal contributor) and later on <EntityLink id="lesswrong">LessWro
    fetchedAt: 2026-02-22T02:11:04.317Z
    httpStatus: 403
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 403
  - footnote: 8
    url: https://www.lesswrong.com/about
    linkText: History of LessWrong
    claimContext: <EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink> began writing The Sequences as daily blog posts starting in 2006, initially on Overcoming Bias (where <EntityLink id="robin-hanson">Robin Hanson</EntityLink> was a principal contributor) and later on <EntityLink id="lesswrong">LessWro
    fetchedAt: 2026-02-22T02:11:05.071Z
    httpStatus: 200
    pageTitle: LessWrong
    contentSnippet: "x LessWrong This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. AI Control Site Meta AI Frontpage 509 Welcome to LessWrong! by Ruby , Raemon , RobertM , habryka 14th Jun 2019 3 min read 76 509 The road to wisdom? Well, it&#x27;s plain and simple to express: Err and err and err again but less and less and less . – Piet Hein LessWrong is an online forum and community dedicated to improving human reasoning and decision-makin"
    contentLength: 985672
    status: verified
    note: null
  - footnote: 9
    url: https://www.lesswrong.com/tag/map-and-territory
    linkText: Map and Territory Sequence
    claimContext: <EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink> began writing The Sequences as daily blog posts starting in 2006, initially on Overcoming Bias (where <EntityLink id="robin-hanson">Robin Hanson</EntityLink> was a principal contributor) and later on <EntityLink id="lesswrong">LessWro
    fetchedAt: 2026-02-22T02:11:05.047Z
    httpStatus: 200
    pageTitle: Map and Territory — LessWrong
    contentSnippet: x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Map and Territory — LessWrong Map and Territory Edited by the gears to ascension , et al. last updated 1st Apr 2023 Our mental frameworks for understanding the world often differ from reality itself. It&#x27;s easy to mistake the map that we hold in our minds for the actual territory it&#x27;s representing. Recognizing the difference between the map and the territory i
    contentLength: 474922
    status: verified
    note: null
  - footnote: 10
    url: https://www.lesswrong.com/tag/core-sequences
    linkText: "The Sequences: Core Sequences"
    claimContext: About half of the original posts were organized into thematically linked "sequences," distinguished between "major" sequences (by size) and "minor" sequences. The core sequences included:[^10]
    fetchedAt: 2026-02-22T02:11:04.729Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 11
    url: https://intelligence.org/team/
    linkText: Eliezer Yudkowsky - MIRI
    claimContext: Yudkowsky was an autodidact who did not attend high school or college, and had previously co-founded the Singularity Institute for Artificial Intelligence (which became <EntityLink id="miri">MIRI</EntityLink> in 2013).[^11]
    fetchedAt: 2026-02-22T02:11:06.205Z
    httpStatus: 200
    pageTitle: Team - Machine Intelligence Research Institute
    contentSnippet: "Team - Machine Intelligence Research Institute Skip to content Team Leadership Malo Bourgon Chief Executive Officer View bio Nate Soares President View bio Eliezer Yudkowsky Co-Founder View bio Alex Vermeer Chief Operating Officer View bio Jimmy Rintjema Chief Financial Officer View bio Staff David Abecassis Technical Governance Researcher View bio Martin Lucas Operations Generalist View bio Alana Horowitz Friedman Staff Writer View bio Tobias Martin Communications Generalist View bio Alex Beck "
    contentLength: 159778
    status: verified
    note: null
  - footnote: 12
    url: https://intelligence.org/rationality-ai-zombies/
    linkText: "Rationality: From AI to Zombies Publication"
    claimContext: 'In 2015, <EntityLink id="miri">MIRI</EntityLink> collated, edited, and published the posts as the ebook *Rationality: From AI to Zombies*. This version omitted some original posts while adding uncollected essays from the same era.[^12] The compiled version organized the material into thematic "books'
    fetchedAt: 2026-02-22T02:11:06.515Z
    httpStatus: 200
    pageTitle: "Rationality: From AI to Zombies - Machine Intelligence Research Institute"
    contentSnippet: "Rationality: From AI to Zombies - Machine Intelligence Research Institute Skip to content Rationality: From AI to Zombies A series by Eliezer Yudkowsky If you live in an urban area, you probably don’t need to walk very far to find a martial arts dojo. Why aren’t there dojos that teach rationality? Very recently—in just the last few decades—the human species has acquired a great deal of new knowledge about human rationality. Experimental investigations of empirical human psychology; and theoretic"
    contentLength: 93275
    status: verified
    note: null
  - footnote: 13
    url: https://www.readthesequences.com/
    linkText: "Book Structure - Rationality: From AI to Zombies"
    claimContext: "- **Book III: The Machine in the Ghost** - Philosophy of mind, intelligence, goal systems, often linked to AI; includes thought experiments on consciousness and subjective experience versus physical processes (e.g., philosophical zombies) - Additional books on quantum physics, evolutionary psycholog"
    fetchedAt: 2026-02-22T02:11:06.684Z
    httpStatus: 200
    pageTitle: "Rationality: From AI to Zombies"
    contentSnippet: "Rationality: From AI to Zombies About Search Contents ❧ Preface Biases: An Introduction by Rob Bensinger Book I Map and Territory Book II How to Actually Change Your Mind Book III The Machine in the Ghost Book IV Mere Reality Book V Mere Goodness Book VI Becoming Stronger Bibliography Glossary ❦ You hold in your hands a compilation of two years of daily blog posts. In retrospect, I look back on that project and see a large number of things I did completely wrong. I’m fine with that. Looking back"
    contentLength: 37759
    status: verified
    note: null
  - footnote: 14
    url: https://www.lesswrong.com/library
    linkText: Modern Sequences - LessWrong
    claimContext: The original posts were preserved on <EntityLink id="lesswrong">LessWrong</EntityLink> as "deprecated" for historical reference, while modern LessWrong sequences continued to draw from this material.[^14]
    fetchedAt: 2026-02-22T02:11:06.495Z
    httpStatus: 200
    pageTitle: The Library — LessWrong
    contentSnippet: 'x The Library — LessWrong This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home All Posts Concepts Library Best of LessWrong Sequence Highlights Rationality: A-Z The Codex HPMOR Community Events Subscribe (RSS/Email) LW the Album Leaderboard About FAQ Home All Posts Concepts Library Community About The Library Rationality: A-Z Also known as "The Sequences" How can we think better on purpose? Why should we think better '
    contentLength: 422330
    status: verified
    note: null
  - footnote: 15
    url: https://www.lesswrong.com/tag/how-to-actually-change-your-mind
    linkText: How to Actually Change Your Mind
    claimContext: "The Sequences teach how to avoid typical failure modes of human reasoning and think in ways that lead to true and accurate beliefs.[^15] Core epistemological concepts include:"
    fetchedAt: 2026-02-22T02:11:06.677Z
    httpStatus: 200
    pageTitle: How To Actually Change Your Mind — LessWrong
    contentSnippet: 'x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. How To Actually Change Your Mind — LessWrong How To Actually Change Your Mind Edited by Rob Bensinger , Eliezer Yudkowsky , et al. last updated 23rd Sep 2020 How to Actually Change Your Mind is the second book contained in the ebook Rationality: From AI to Zombies , by Eliezer Yudkowsky. It is the edited version of a series of blog posts in "the Sequences" , and covers'
    contentLength: 257539
    status: verified
    note: null
  - footnote: 16
    url: https://www.lesswrong.com/tag/map-and-territory
    linkText: Map and Territory - Core Concept
    claimContext: "- **Map-Territory Distinction**: Beliefs function as maps representing reality, not reality itself; confusing the two leads to systematic errors[^16] - **Bayesian Reasoning**: Using probability theory to update beliefs based on evidence"
    fetchedAt: 2026-02-22T02:11:08.752Z
    httpStatus: 200
    pageTitle: Map and Territory — LessWrong
    contentSnippet: x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Map and Territory — LessWrong Map and Territory Edited by the gears to ascension , et al. last updated 1st Apr 2023 Our mental frameworks for understanding the world often differ from reality itself. It&#x27;s easy to mistake the map that we hold in our minds for the actual territory it&#x27;s representing. Recognizing the difference between the map and the territory i
    contentLength: 474922
    status: verified
    note: null
  - footnote: 17
    url: https://www.lesswrong.com/tag/bayes-theorem
    linkText: Bayesian Reasoning in The Sequences
    claimContext: "- **Conservation of Expected Evidence**: The principle that you can't predict in advance what direction evidence will update your beliefs - **Absence of Evidence as Evidence of Absence**: When you would expect to see evidence if something were true, not finding it counts against that hypothesis[^17]"
    fetchedAt: 2026-02-22T02:11:08.837Z
    httpStatus: 200
    pageTitle: Bayes&#x27; Theorem — LessWrong
    contentSnippet: x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Bayes&#x27; Theorem — LessWrong Bayes&#x27; Theorem Edited by bjr , Ben Pace , a_soulless_automaton , Vladimir_Nesov , et al. last updated 19th Feb 2024 Bayes&#x27; Theorem (also known as Bayes&#x27; Law) is a law of probability that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate. It is commonly reg
    contentLength: 996037
    status: verified
    note: null
  - footnote: 18
    url: https://www.lesswrong.com/tag/biases
    linkText: Cognitive Biases in The Sequences
    claimContext: "- **Scope insensitivity** - Failing to properly scale emotional responses to magnitude - **Motivated reasoning** - Reasoning in service of desired conclusions rather than truth[^18]"
    fetchedAt: 2026-02-22T02:11:08.283Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 19
    url: https://www.lesswrong.com/tag/timeless-decision-theory
    linkText: Timeless Decision Theory
    claimContext: "Yudkowsky developed Timeless Decision Theory (TDT) as an alternative to Causal and Evidential Decision Theory, addressing problems like Newcomb's Problem and Pascal's Mugging.[^19] The Sequences also introduce concepts relevant to AI alignment, including:"
    fetchedAt: 2026-02-22T02:11:09.168Z
    httpStatus: 200
    pageTitle: Timeless Decision Theory — LessWrong
    contentSnippet: x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Timeless Decision Theory — LessWrong Timeless Decision Theory Edited by crazy88 , abramdemski , et al. last updated 21st Feb 2025 Timeless decision theory (TDT) is a decision theory developed by Eliezer Yudkowsky which, in slogan form, says that agents should decide as if they are determining the output of the abstract computation that they implement. This theory was d
    contentLength: 560421
    status: verified
    note: null
  - footnote: 20
    url: https://www.lesswrong.com/tag/ai-alignment
    linkText: AI Alignment Topics in The Sequences
    claimContext: '- <EntityLink id="instrumental-convergence">Instrumental convergence</EntityLink> and goal preservation - The challenge of specifying human values[^20]'
    fetchedAt: 2026-02-22T02:11:08.379Z
    httpStatus: 200
    pageTitle: AI alignment — LessWrong
    contentSnippet: 'x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. AI alignment — LessWrong AI alignment Edited by Eliezer Yudkowsky , et al. last updated 17th Feb 2025 The "alignment problem for advanced agents " or "AI alignment" is the overarching research topic of how to develop sufficiently advanced machine intelligences such that running them produces good outcomes in the real world. Both &#x27; advanced agent &#x27; and &#x27; '
    contentLength: 222869
    status: verified
    note: null
  - footnote: 21
    url: https://www.lesswrong.com/library
    linkText: LessWrong Foundational Texts
    claimContext: The Sequences became foundational texts for <EntityLink id="lesswrong">LessWrong</EntityLink> and shaped the rationalist community's culture and discourse.[^21] The material is widely recommended as an entry point for newcomers to rationalist thinking and AI safety considerations. LessWrong's 2024 s
    fetchedAt: 2026-02-22T02:11:10.768Z
    httpStatus: 200
    pageTitle: The Library — LessWrong
    contentSnippet: 'x The Library — LessWrong This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home All Posts Concepts Library Best of LessWrong Sequence Highlights Rationality: A-Z The Codex HPMOR Community Events Subscribe (RSS/Email) LW the Album Leaderboard About FAQ Home All Posts Concepts Library Community About The Library Rationality: A-Z Also known as "The Sequences" How can we think better on purpose? Why should we think better '
    contentLength: 422722
    status: verified
    note: null
  - footnote: 22
    url: https://www.lesswrong.com/posts/survey-2024
    linkText: LessWrong 2024 Survey Results
    claimContext: The Sequences became foundational texts for <EntityLink id="lesswrong">LessWrong</EntityLink> and shaped the rationalist community's culture and discourse.[^21] The material is widely recommended as an entry point for newcomers to rationalist thinking and AI safety considerations. LessWrong's 2024 s
    fetchedAt: 2026-02-22T02:11:10.849Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 23
    url: https://forum.effectivealtruism.org/
    linkText: "EA Forum: Rationalist Influence"
    claimContext: The work significantly influenced effective altruism, particularly around Bayesian epistemology, prediction, cognitive biases, and thinking about AI risks.[^23] Community members have noted that familiarity with The Sequences, particularly essays like "Death Spirals," helps create "a community I can
    fetchedAt: 2026-02-22T02:11:10.421Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812537
    status: verified
    note: null
  - footnote: 24
    url: https://forum.effectivealtruism.org/
    linkText: "EA Forum: Death Spirals Discussion"
    claimContext: The work significantly influenced effective altruism, particularly around Bayesian epistemology, prediction, cognitive biases, and thinking about AI risks.[^23] Community members have noted that familiarity with The Sequences, particularly essays like "Death Spirals," helps create "a community I can
    fetchedAt: 2026-02-22T02:11:12.425Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812540
    status: verified
    note: null
  - footnote: 25
    url: https://nickbostrom.com/
    linkText: Nick Bostrom and Intelligence Explosion
    claimContext: "Yudkowsky's work on intelligence explosions from the Sequences era influenced philosopher <EntityLink id=\"nick-bostrom\">Nick Bostrom</EntityLink>'s 2014 book *Superintelligence: Paths, Dangers, Strategies*.[^25] However, The Sequences face criticism for limited engagement with academic philosophy an"
    fetchedAt: 2026-02-22T02:11:10.762Z
    httpStatus: 200
    pageTitle: Nick Bostrom’s Home Page
    contentSnippet: Nick Bostrom’s Home Page The world is quickening, and the birth of superintelligence presumably not very far off; yet most people are otherwise occupied. Currently working on a couple of things related to AGI governance. Chinese translation of Deep Utopia is now out. Also the second print run in English and the Audiobook — people say the voice actor is good. The book has received still more awards. You can sign up for newsletter to receive very rare updates, but the most reliable method is to ch
    contentLength: 104308
    status: verified
    note: null
  - footnote: 26
    url: https://www.lesswrong.com/posts/philosophical-criticism
    linkText: "Criticism: Philosophy Engagement"
    claimContext: "Yudkowsky's work on intelligence explosions from the Sequences era influenced philosopher <EntityLink id=\"nick-bostrom\">Nick Bostrom</EntityLink>'s 2014 book *Superintelligence: Paths, Dangers, Strategies*.[^25] However, The Sequences face criticism for limited engagement with academic philosophy an"
    fetchedAt: 2026-02-22T02:11:14.105Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 27
    url: https://forum.effectivealtruism.org/
    linkText: "EA Forum: Sequences Originality Debate"
    claimContext: 'The material overlaps with prior academic works like *Thinking and Deciding* by Jonathan Baron but is criticized for not fully crediting academia. Some view it as an original synthesis (30-60% new material) presented in an engaging "popular science" format that condenses psychology, philosophy, and '
    fetchedAt: 2026-02-22T02:11:13.556Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812539
    status: verified
    note: null
  - footnote: 28
    url: https://www.goodreads.com/book/show/rationality-from-ai-to-zombies
    linkText: Reader Reception - Goodreads
    claimContext: Readers report that The Sequences provide useful "tags" or terminology for discussing reasoning patterns, help internalize ideas that seem obvious in retrospect, and offer tools for avoiding belief weak points like motivated cognition.[^28] The essays are described as engaging popular science that m
    fetchedAt: 2026-02-22T02:11:13.945Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 29
    url: https://forum.effectivealtruism.org/
    linkText: "EA Forum: Measurable Effectiveness Discussion"
    claimContext: However, critics note limitations in measurable effectiveness. No empirical studies demonstrate improvements in decision-making or other quantifiable outcomes from reading The Sequences.[^29] The work's impact appears primarily anecdotal and concentrated within specific communities rather than demon
    fetchedAt: 2026-02-22T02:11:13.503Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812539
    status: verified
    note: null
  - footnote: 30
    url: https://www.lesswrong.com/posts/sequences-philosophical-errors
    linkText: Philosophical Errors in The Sequences
    claimContext: Critics argue that Yudkowsky dismisses philosophy while simultaneously reinventing concepts from the field without adequate credit or understanding. Specific criticisms include:[^30][^31]
    fetchedAt: 2026-02-22T02:11:14.084Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 31
    url: https://philpapers.org/rec/CHATCA-2
    linkText: David Chalmers Response
    claimContext: Critics argue that Yudkowsky dismisses philosophy while simultaneously reinventing concepts from the field without adequate credit or understanding. Specific criticisms include:[^30][^31]
    fetchedAt: 2026-02-22T02:11:16.394Z
    httpStatus: 301
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: verified
    note: HTTP 301
  - footnote: 32
    url: https://www.lesswrong.com/posts/epistemic-conduct
    linkText: Epistemic Conduct Criticism
    claimContext: Multiple critics highlight concerns about Yudkowsky's approach to disagreement and error correction:[^32][^33]
    fetchedAt: 2026-02-22T02:11:15.692Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 33
    url: https://forum.effectivealtruism.org/
    linkText: "EA Forum: Yudkowsky Track Record"
    claimContext: Multiple critics highlight concerns about Yudkowsky's approach to disagreement and error correction:[^32][^33]
    fetchedAt: 2026-02-22T02:11:15.205Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812536
    status: verified
    note: null
  - footnote: 34
    url: https://www.goodreads.com/book/show/rationality-from-ai-to-zombies
    linkText: Reader Reviews - Style Criticism
    claimContext: Readers note several problems with the writing itself:[^34][^35]
    fetchedAt: 2026-02-22T02:11:15.477Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 35
    url: https://forum.effectivealtruism.org/
    linkText: "EA Forum: Sequences Writing Quality"
    claimContext: Readers note several problems with the writing itself:[^34][^35]
    fetchedAt: 2026-02-22T02:11:15.192Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812539
    status: verified
    note: null
  - footnote: 36
    url: https://forum.effectivealtruism.org/
    linkText: Worldview Transmission Concerns
    claimContext: Critics argue The Sequences transmit a "packaged worldview" with potential dangers rather than pure rationality tools.[^36] The work's framing around AI doom has become more prominent over time—one reader noted that on a second reading, they became "constantly aware that Yudkowsky believes...that ou
    fetchedAt: 2026-02-22T02:11:17.456Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812536
    status: verified
    note: null
  - footnote: 37
    url: https://forum.effectivealtruism.org/
    linkText: Second Reading Experience
    claimContext: Critics argue The Sequences transmit a "packaged worldview" with potential dangers rather than pure rationality tools.[^36] The work's framing around AI doom has become more prominent over time—one reader noted that on a second reading, they became "constantly aware that Yudkowsky believes...that ou
    fetchedAt: 2026-02-22T02:11:17.483Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812539
    status: verified
    note: null
  - footnote: 38
    url: https://www.lesswrong.com/posts/doom-update-2024
    linkText: 2024 Doom Update Podcast
    claimContext: This contrasts with the optimistic tone of the original writing period (2006-2009). By 2024, Yudkowsky's public statements emphasized extreme urgency, stating humanity has "ONE YEAR, THIS YEAR, 2024" for a global response to AI extinction risks.[^38]
    fetchedAt: 2026-02-22T02:11:18.427Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 39
    url: https://forum.effectivealtruism.org/
    linkText: Replication Crisis Impact
    claimContext: The Sequences heavily drew on psychological findings from the early 2000s, many of which collapsed during the replication crisis that began shortly after Yudkowsky finished writing them.[^39] This undermines some of the empirical foundations for claims about cognitive biases and reasoning, though co
    fetchedAt: 2026-02-22T02:11:17.540Z
    httpStatus: 200
    pageTitle: Effective Altruism Forum
    contentSnippet: Effective Altruism Forum This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Home Best of the Forum All posts Topics People directory Take action Events Groups directory How to use the Forum Forum events calendar EA Handbook EA Forum Podcast Quick takes RSS Cookie policy Copyright Contact us Frontpage Opportunities Global health Animal welfare AI safety Community Biosecurity & pandemics Existential risk Philosophy Buildin
    contentLength: 812536
    status: verified
    note: null
  - footnote: 40
    url: https://www.reddit.com/r/SneerClub/
    linkText: Cultural Perception Discussion
    claimContext: The Sequences are sometimes associated with what critics describe as a "nerdy, rationalist religion" with unconventional beliefs (including polyamory and AI obsession), with Yudkowsky positioned as an unrespected "guru" outside his immediate circle.[^40] The fact that Yudkowsky's other major work is
    fetchedAt: 2026-02-22T02:11:17.410Z
    httpStatus: 403
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 403
  - footnote: 41
    url: https://slatestarcodex.com/
    linkText: Scott Alexander on Sequences Effectiveness
    claimContext: Within the rationalist and EA communities, some members note that "the Sequences clearly failed to make anyone a rational superbeing, or even noticeably more successful," as Scott Alexander pointed out as early as 2009.[^41]
    fetchedAt: 2026-02-22T02:11:19.988Z
    httpStatus: 200
    pageTitle: Slate Star Codex
    contentSnippet: Slate Star Codex Home About / Top Posts Archives Top Posts Comments Feed RSS Feed Slate Star Codex Blogroll Economics Artir Kel Bryan Caplan David Friedman Pseudoerasmus Scott Sumner Tyler Cowen Effective Altruism 80000 Hours Blog Effective Altruism Forum GiveWell Blog Rationality Alyssa Vance Beeminder Elizabeth Van Nostrand Gwern Branwen Jacob Falkovich Jeff Kaufman Katja Grace Kelsey Piper Less Wrong Paul Christiano Robin Hanson Sarah Constantin Zack Davis Zvi Mowshowitz Science Andrew Gelman
    contentLength: 136387
    status: verified
    note: null
  - footnote: 42
    url: https://www.lesswrong.com/tag/sequence-highlights
    linkText: Sequence Highlights - 50 Essays
    claimContext: 'The Sequences remain available in multiple formats: as blog posts on <EntityLink id="lesswrong">LessWrong</EntityLink>, as the compiled ebook *Rationality: From AI to Zombies*, and through curated "Sequence Highlights" featuring 50 key essays.[^42] The material continues to serve as a recommended st'
    fetchedAt: 2026-02-22T02:11:20.750Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 43
    url: https://intelligence.org/inadequate-equilibria/
    linkText: Inadequate Equilibria - MIRI
    claimContext: 'Yudkowsky continued publishing related work, including the 2017 ebook *Inadequate Equilibria* (published by <EntityLink id="miri">MIRI</EntityLink>) on societal inefficiencies,[^43] and co-authored *If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All* with Nate Soares, which beca'
    fetchedAt: 2026-02-22T02:11:20.461Z
    httpStatus: 200
    pageTitle: Inadequate Equilibria - Machine Intelligence Research Institute
    contentSnippet: Inadequate Equilibria - Machine Intelligence Research Institute Skip to content Inadequate Equilibria Where and How Civilizations Get Stuck by Eliezer Yudkowsky When should you think that you may be able to do something unusually well? Whether you’re trying to outperform in science, or in business, or just in finding good deals shopping on eBay, it’s important that you have a sober understanding of your relative competencies. The story only ends there, however, if you’re fortunate enough to live
    contentLength: 76037
    status: verified
    note: null
  - footnote: 44
    url: https://www.amazon.com/If-Anyone-Builds-Everyone-Dies/
    linkText: If Anyone Builds It, Everyone Dies
    claimContext: 'Yudkowsky continued publishing related work, including the 2017 ebook *Inadequate Equilibria* (published by <EntityLink id="miri">MIRI</EntityLink>) on societal inefficiencies,[^43] and co-authored *If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All* with Nate Soares, which beca'
    fetchedAt: 2026-02-22T02:11:19.647Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 45
    url: https://booksandbytes.fm/
    linkText: Books in Bytes Podcast 2025
    claimContext: "A 2025 podcast episode on *Books in Bytes* explored ongoing themes from The Sequences relevant to rationalists and AI theorists, including the zombie argument, perception biases, and joy in reasoning.[^45] <EntityLink id=\"manifold\">Manifold</EntityLink> Markets tracked predictions about Yudkowsky's "
    fetchedAt: 2026-02-22T02:11:19.465Z
    httpStatus: 0
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: fetch failed
  - footnote: 46
    url: https://manifold.markets/
    linkText: "Manifold Markets: Yudkowsky Doom Predictions"
    claimContext: "A 2025 podcast episode on *Books in Bytes* explored ongoing themes from The Sequences relevant to rationalists and AI theorists, including the zombie argument, perception biases, and joy in reasoning.[^45] <EntityLink id=\"manifold\">Manifold</EntityLink> Markets tracked predictions about Yudkowsky's "
    fetchedAt: 2026-02-22T02:11:22.657Z
    httpStatus: 200
    pageTitle: Manifold
    contentSnippet: Manifold MANIFOLD All Politics Technology Sports Culture Business Fun Best Hot New Open options Any status © Manifold Markets, Inc. • Terms • Privacy
    contentLength: 22569
    status: verified
    note: null
