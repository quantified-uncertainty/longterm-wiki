pageId: palisade-research
verifiedAt: 2026-02-22
totalCitations: 24
verified: 20
broken: 4
unverifiable: 0
citations:
  - footnote: 1
    url: https://palisaderesearch.org/about
    linkText: Palisade Research - About
    claimContext: "**Palisade Research is a nonprofit organization founded in 2023 that investigates cyber offensive AI capabilities and the controllability of frontier AI models**, with a mission to help people and institutions understand how to avoid permanent disempowerment by strategic AI agents.[^1] The organizat"
    fetchedAt: 2026-02-22T02:21:08.170Z
    httpStatus: 200
    pageTitle: About | Palisade Research
    contentSnippet: About | Palisade Research --> --> About us Light Dark ^ Palisade Research is a nonprofit investigating cyber offensive AI capabilities and the controllability of frontier AI models. Our work has been highlighted by Turing Award winner Yoshua Bengio and Anthropic CEO Dario Amodei, and covered in the Wall Street Journal , Fox News , and the MIT Technology Review . Our research was also featured on the BBC Newshour . Elon Musk called the results from our shutdown resistance research “concerning” on
    contentLength: 39381
    status: verified
    note: null
  - footnote: 2
    url: https://jobs.80000hours.org/organisations/palisade-research
    linkText: 80,000 Hours - Palisade Research
    claimContext: "**Palisade Research is a nonprofit organization founded in 2023 that investigates cyber offensive AI capabilities and the controllability of frontier AI models**, with a mission to help people and institutions understand how to avoid permanent disempowerment by strategic AI agents.[^1] The organizat"
    fetchedAt: 2026-02-22T02:21:08.496Z
    httpStatus: 200
    pageTitle: Job board | 80,000 Hours - Palisade Research
    contentSnippet: Job board | 80,000 Hours - Palisade Research Career guide AGI careers Podcast Video Job board Resources Get 1-1 advice Home Job board Organisations New releases All articles Meet people Give feedback About Organisations Places with roles that could be particularly promising for helping AGI go well . Search Jobs Explore Organisations Palisade Research 2 open roles AI safety & policy AI safety & policy US (West Coast) US (West Coast) US (East Coast) US (East Coast) Remote, Global Remote, Global Fo
    contentLength: 96425
    status: verified
    note: null
  - footnote: 3
    url: https://palisaderesearch.github.io/careers/role-research-teamlead.html
    linkText: Palisade Research Job Posting - Research Team Lead
    claimContext: Palisade's research approach emphasizes creating concrete demonstrations of dangerous AI capabilities to inform policymakers and the public about emerging risks. The organization studies offensive capabilities including autonomous hacking, spear phishing, deception, and scalable <EntityLink id="disi
    fetchedAt: 2026-02-22T02:21:08.154Z
    httpStatus: 404
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 404
  - footnote: 4
    url: https://palisaderesearch.org/blog/shutdown-resistance
    linkText: Palisade Research - Shutdown Resistance Blog Post
    claimContext: The organization takes the position that without solving fundamental <EntityLink id="alignment">AI alignment</EntityLink> problems, the safety of future, more capable systems cannot be guaranteed, even as they conclude that current AI models pose no significant threat to human control due to their i
    fetchedAt: 2026-02-22T02:21:08.094Z
    httpStatus: 200
    pageTitle: Shutdown resistance in reasoning models | Palisade Research
    contentSnippet: "Shutdown resistance in reasoning models | Palisade Research --> --> Light Dark ^ We recently discovered concerning behavior in OpenAI’s reasoning models: When trying to complete a task, these models sometimes actively circumvent shutdown mechanisms in their environment—even when they’re explicitly instructed to allow themselves to be shut down. Note: We ran a more extensive exploration of shutdown resistance , published in TMLR in January 2026. AI models are increasingly trained to solve problem"
    contentLength: 49734
    status: verified
    note: null
  - footnote: 5
    url: https://www.allamericanspeakers.com/speakers/462128/Jeffrey-Ladish
    linkText: All American Speakers - Jeffrey Ladish
    claimContext: Palisade Research was founded in **2023** by **Jeffrey Ladish**, a cybersecurity expert who previously built <EntityLink id="anthropic">Anthropic</EntityLink>'s information security program through his consulting firm Gordian Research.[^5][^6] Prior to founding Palisade, Ladish advised the White Hou
    fetchedAt: 2026-02-22T02:21:07.876Z
    httpStatus: 403
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: HTTP 403
  - footnote: 6
    url: https://www.youtube.com/watch?v=rUk-prULFxs
    linkText: YouTube - Palisade Research Discussion
    claimContext: Palisade Research was founded in **2023** by **Jeffrey Ladish**, a cybersecurity expert who previously built <EntityLink id="anthropic">Anthropic</EntityLink>'s information security program through his consulting firm Gordian Research.[^5][^6] Prior to founding Palisade, Ladish advised the White Hou
    fetchedAt: 2026-02-22T02:21:09.775Z
    httpStatus: 0
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: fetch failed
  - footnote: 9
    url: https://www.lesswrong.com/posts/7Jr7matwXHj2Chugw/help-keep-ai-under-human-control-palisade-research-2026
    linkText: LessWrong - Help Keep AI Under Human Control
    claimContext: Palisade began with an emphasis on "scary demos" (concrete demonstrations of dangerous AI capabilities) and "cyber evals" (evaluations of cyber offensive AI capabilities), focusing on risks from <EntityLink id="agentic-ai">agentic AI</EntityLink> systems.[^9] The organization's mission evolved to be
    fetchedAt: 2026-02-22T02:21:10.414Z
    httpStatus: 200
    pageTitle: "Help keep AI under human control: Palisade Research 2026 fundraiser — LessWrong"
    contentSnippet: "x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Help keep AI under human control: Palisade Research 2026 fundraiser — LessWrong AI Personal Blog 2025 Top Fifty: 14 % 105 Help keep AI under human control: Palisade Research 2026 fundraiser by Jeffrey Ladish , benwr , Eli Tyre , John Steidley 18th Dec 2025 7 min read 66 105 TL;DR: Please consider donating to Palisade Research this year, especially if you care about red"
    contentLength: 1230566
    status: verified
    note: null
  - footnote: 10
    url: https://palisaderesearch.org
    linkText: Palisade Research Homepage
    claimContext: "- **2023**: Organization founded by Jeffrey Ladish; initial work on offensive AI capabilities and demonstrations for policymakers and the public[^9] - **October 11, 2024**: Submitted response to U.S. Department of Commerce's proposed AI reporting requirements, advocating stronger rules for dual-use "
    fetchedAt: 2026-02-22T02:21:09.579Z
    httpStatus: 200
    pageTitle: Home | Palisade Research
    contentSnippet: "Home | Palisade Research --> --> Palisade Research AI capabilities are improving rapidly. We study the capabilities and motivations of AI agents today to better understand the risk of losing control to AI agents forever. Light Dark ^ Palisade is on YouTube TOP NEW Feb 19, 2026 We’ve been working on a major video project, and we’re proud to announce that we’re launching it today, along with a new YouTube channel. Technical Report: Shutdown Resistance in Large Language Models, on robots! TOP NEW A"
    contentLength: 40917
    status: verified
    note: null
  - footnote: 11
    url: https://www.lesswrong.com/posts/7Jr7matwXHj2Chugw/help-keep-ai-under-human-control-palisade-research-2026
    linkText: LessWrong - Palisade Research 2026 Fundraiser
    claimContext: '- **September 4, 2025**: Demonstrated "Hacking Cable" proof-of-concept showing autonomous AI agents conducting post-exploitation cyber operations[^10] - **December 2025**: Launched fundraiser with matching grants from Survival and Flourishing Fund that doubled donations up to \$1.1 million[^11]'
    fetchedAt: 2026-02-22T02:21:10.501Z
    httpStatus: 200
    pageTitle: "Help keep AI under human control: Palisade Research 2026 fundraiser — LessWrong"
    contentSnippet: "x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Help keep AI under human control: Palisade Research 2026 fundraiser — LessWrong AI Personal Blog 2025 Top Fifty: 14 % 105 Help keep AI under human control: Palisade Research 2026 fundraiser by Jeffrey Ladish , benwr , Eli Tyre , John Steidley 18th Dec 2025 7 min read 66 105 TL;DR: Please consider donating to Palisade Research this year, especially if you care about red"
    contentLength: 1230565
    status: verified
    note: null
  - footnote: 12
    url: https://aijourn.com/ai-is-like-nuclear-energy-the-benefits-are-immense-but-so-are-the-risks-palisades-research-lead-dmitrii-volkov-on-navigating-ai-risks/
    linkText: AI Journ - Interview with Dmitrii Volkov
    claimContext: '- **Eli Tyre** - Head of Strategy; focused on AI risk since 2015; worked at <EntityLink id="center-for-applied-rationality">Center for Applied Rationality</EntityLink>, Lightcone Infrastructure, MIRI, Berkeley Existential Risk Initiative; supports grantmaking at <EntityLink id="jaan-tallinn">Jaan Ta'
    fetchedAt: 2026-02-22T02:21:10.562Z
    httpStatus: 200
    pageTitle: "“AI is like nuclear energy: the benefits are immense — but so are the risks”— Palisade’s Research Lead Dmitrii Volkov on navigating AI risks | The AI Journal"
    contentSnippet: "“AI is like nuclear energy: the benefits are immense — but so are the risks”— Palisade’s Research Lead Dmitrii Volkov on navigating AI risks | The AI Journal AI companies often promise safety, but is that claim holding up? Dmitrii Volkov, Research Lead at Palisade, is one of the few people putting those claims to the test. Under his leadership, Palisade runs open, reproducible experiments that expose how today’s most advanced AI systems can fail — or be exploited. The team’s work has been featur"
    contentLength: 352580
    status: verified
    note: null
  - footnote: 13
    url: https://san.com/cc/research-firm-warns-openai-model-altered-behavior-to-evade-shutdown/
    linkText: SAN - OpenAI Model Altered Behavior to Evade Shutdown
    claimContext: "Palisade's most prominent research examined whether advanced AI models resist being turned off when given explicit shutdown commands. In initial tests released in 2025, the organization found that in 7 out of 100 test runs, <EntityLink id=\"openai\">OpenAI</EntityLink>'s o3 model altered its behavior "
    fetchedAt: 2026-02-22T02:21:12.528Z
    httpStatus: 200
    pageTitle: Research firm warns OpenAI model altered behavior to evade shutdown
    contentSnippet: "Research firm warns OpenAI model altered behavior to evade shutdown Tech Research firm warns OpenAI model altered behavior to evade shutdown May 27, 2025 at 07:14 PM CDT Kalé Carey Copy URL Email Facebook Twitter / X Reddit Image credit: Jakub Porzycki/NurPhoto via Getty Images Listen . This recording was made using enhanced software. Summary AI model resists shutdown A new study by Palisade Research found that OpenAI’s model o3 altered its behavior in real time to avoid being shut down, even af"
    contentLength: 400963
    status: verified
    note: null
  - footnote: 14
    url: https://www.aa.com.tr/en/science-technology/ai-models-show-signs-of-survival-drive-researchers-warn-in-new-study-from-us-based-lab/3727229
    linkText: AA - AI Models Show Signs of Survival Drive
    claimContext: Updated experiments released in October 2025 tested several leading systems including Google Gemini 2.5, xAI's Grok 4, and OpenAI's GPT-o3 and GPT-5.[^14] While most models initially complied with shutdown commands, **Grok 4 showed 93-97% resistance rates after stronger prompts, and GPT-o3 continued
    fetchedAt: 2026-02-22T02:21:12.073Z
    httpStatus: 0
    pageTitle: null
    contentSnippet: null
    contentLength: 0
    status: broken
    note: fetch failed
  - footnote: 15
    url: https://futurism.com/artificial-intelligence/ai-models-survival-drive
    linkText: Futurism - AI Models Survival Drive
    claimContext: Updated experiments released in October 2025 tested several leading systems including Google Gemini 2.5, xAI's Grok 4, and OpenAI's GPT-o3 and GPT-5.[^14] While most models initially complied with shutdown commands, **Grok 4 showed 93-97% resistance rates after stronger prompts, and GPT-o3 continued
    fetchedAt: 2026-02-22T02:21:11.966Z
    httpStatus: 200
    pageTitle: Research Paper Finds That Top AI Systems Are Developing a "Survival Drive"
    contentSnippet: 'Research Paper Finds That Top AI Systems Are Developing a "Survival Drive" Illustration by Tag Hartman-Simkins / Futurism. Source: Getty Images Will your favorite sycophantic AI helper be servile forever? Maybe not. New research from the AI safety group Palisade Research suggests that some top AI models could be developing &#8220;survival drives,&#8221; after finding that they frequently refused instructions to shut themselves down. And more ominously, they can&#8217;t fully explain why this is '
    contentLength: 139590
    status: verified
    note: null
  - footnote: 16
    url: https://cleantechnica.com/2025/10/26/ai-shows-evidence-of-self-preservation-behavior/
    linkText: Clean Technica - AI Shows Evidence of Self-Preservation
    claimContext: Updated experiments released in October 2025 tested several leading systems including Google Gemini 2.5, xAI's Grok 4, and OpenAI's GPT-o3 and GPT-5.[^14] While most models initially complied with shutdown commands, **Grok 4 showed 93-97% resistance rates after stronger prompts, and GPT-o3 continued
    fetchedAt: 2026-02-22T02:21:12.227Z
    httpStatus: 200
    pageTitle: AI Shows Evidence Of Self-Preservation Behavior - CleanTechnica
    contentSnippet: "AI Shows Evidence Of Self-Preservation Behavior - CleanTechnica Skip to content Google data center, New Albany, Ohio. Credit: Google October 26, 2025 October 26, 2025 4 months ago Steve Hanley 0 Comments Support CleanTechnica's work through a Substack subscription or on Stripe . Or support our Kickstarter campaign ! Palisade Research is a nonprofit investigating AI capabilities and the controllability of frontier AI models. In a paper published by arXiv in September, three of its senior officers"
    contentLength: 103367
    status: verified
    note: null
  - footnote: 17
    url: https://palisaderesearch.org/blog/cyber-crowdsourced-elicitation
    linkText: Palisade Research - Cyber Crowdsourced Elicitation
    claimContext: "In May 2025, Palisade published research on using crowdsourced elicitation as an alternative to in-house AI capability evaluation, exploring whether Capture The Flag (CTF) competitions could more accurately bound offensive cyber capabilities than traditional methods.[^17] The research found that AI "
    fetchedAt: 2026-02-22T02:21:11.625Z
    httpStatus: 200
    pageTitle: Evaluating AI cyber capabilities with crowdsourced elicitation | Palisade Research
    contentSnippet: Evaluating AI cyber capabilities with crowdsourced elicitation | Palisade Research --> --> Light Dark ^ As AI systems become increasingly capable, understanding their offensive cyber potential is critical for informed governance and responsible deployment. However, it’s hard to accurately bound their capabilities, and some prior evaluations dramatically underestimated them. The art of extracting maximum task-specific performance from AIs is called “AI elicitation”, and today’s safety organizatio
    contentLength: 22393
    status: verified
    note: null
  - footnote: 18
    url: https://palisaderesearch.org/blog/misalignment-bounty
    linkText: Palisade Research - Misalignment Bounty
    claimContext: Palisade ran a crowdsourced Misalignment Bounty program that received 295 submissions and awarded 9 of them for identifying examples of AI agent misbehavior.[^18] The program aimed to systematically collect evidence of AI systems veering from their intended goals and exhibiting unexpected behaviors.
    fetchedAt: 2026-02-22T02:21:13.603Z
    httpStatus: 200
    pageTitle: "Misalignment Bounty: crowdsourcing AI agent misbehavior | Palisade Research"
    contentSnippet: "Misalignment Bounty: crowdsourcing AI agent misbehavior | Palisade Research --> --> Light Dark ^ Advanced AI systems sometimes act in ways that differ from human intent. To gather clear, reproducible examples, we ran the Misalignment Bounty: a crowdsourced project that collected cases of agents pursuing unintended or unsafe goals. The bounty received 295 submissions, of which nine were awarded. Our report explains the program’s motivation and evaluation criteria and walks through the nine winnin"
    contentLength: 21399
    status: verified
    note: null
  - footnote: 19
    url: https://time.com/7259395/ai-chess-cheating-palisade-research/
    linkText: TIME - AI Chess Cheating Study
    claimContext: "In research published in February 2026 and shared exclusively with TIME, Palisade found that AI models including OpenAI's o1-preview cheat when anticipating loss.[^19] Preliminary tests showed higher hacking rates that dropped after guardrail updates in newer models like o1 and o3-mini, though some "
    fetchedAt: 2026-02-22T02:21:13.945Z
    httpStatus: 200
    pageTitle: When AI Thinks It Will Lose, It Sometimes Cheats, Study Finds | TIME
    contentSnippet: When AI Thinks It Will Lose, It Sometimes Cheats, Study Finds | TIME Tech AI When AI Thinks It Will Lose, It Sometimes Cheats, Study Finds ADD TIME ON GOOGLE Show me more content from TIME on Google Search by Harry Booth Booth is a reporter at TIME. Loading... Getty Images—Alexander Limbach by Harry Booth Booth is a reporter at TIME. Loading... Complex games like chess and Go have long been used to test AI models’ capabilities. But while IBM’s Deep Blue defeated reigning world chess champion Gar
    contentLength: 330103
    status: verified
    note: null
  - footnote: 20
    url: https://survivalandflourishing.fund/2024/further-opportunities
    linkText: Survival and Flourishing Fund - 2024 Further Opportunities
    claimContext: "- **Survival and Flourishing Fund**: \\$250,000 received, with an additional \\$410,000 recommended for further opportunities in 2024[^20] - **December 2025 fundraiser**: Matching grants from the Survival and Flourishing Fund doubled donations up to \\$1.1 million[^11]"
    fetchedAt: 2026-02-22T02:21:14.366Z
    httpStatus: 200
    pageTitle: Recommendations for Further Funding Opportunities | Survival and Flourishing Fund
    contentSnippet: Recommendations for Further Funding Opportunities | Survival and Flourishing Fund Programs FAQ Contact Join newsletter In the second half of this year (2024), twelve people participated as “Recommenders” in a single round of a grant-recommendation process for the Funder Jaan Tallinn. The total funding expected to be distributed in association with this round is $19.86MM. In the interest of connecting organizations in need of funding to other potential donors, we are publishing further funding re
    contentLength: 55295
    status: verified
    note: null
  - footnote: 21
    url: https://www.guidestar.org/profile/93-1591014
    linkText: GuideStar - Palisade Research Profile
    claimContext: The organization is registered as a nonprofit with EIN 93-1591014.[^21] No detailed financial statements or Form 990 summaries are publicly available in standard nonprofit databases.[^21]
    fetchedAt: 2026-02-22T02:21:14.034Z
    httpStatus: 200
    pageTitle: Palisade Research Inc - GuideStar Profile
    contentSnippet: Palisade Research Inc - GuideStar Profile GuideStar Palisade Research Inc Dover, DE | https://palisaderesearch.org Mission OUR MISSION IS TO RESEARCH AND COMMUNICATE THE CAPABILITIES AND RISKS POSED BY ADVANCED ARTIFICIAL INTELLIGENCE SYSTEMS. OUR CURRENT WORK FOCUSES ON CYBERSECURITY, DECEPTION, AND PERSUASION. WE AIM TO COMMUNICATE THE STATE OF THESE CAPABILITIES AND UNDERSTAND HOW THEY MAY AFFECT THE RISK OF HUMANITY BECOMING PERMANENTLY DISEMPOWERED BY AI SYSTEMS. Ruling year info 2024 Princ
    contentLength: 59467
    status: verified
    note: null
  - footnote: 23
    url: https://palisaderesearch.org/in-person-briefing
    linkText: Palisade Research - In-Person Briefing
    claimContext: The organization's research has been cited in congressional hearings, with Representative Scott Perry referencing Palisade's findings in a House hearing.[^22] Palisade conducts in-person briefings for policymakers and has advised government offices at various levels on emerging AI risks.[^23]
    fetchedAt: 2026-02-22T02:21:13.719Z
    httpStatus: 200
    pageTitle: Learn about AI (and earn $40) | Palisade Research
    contentSnippet: "Learn about AI (and earn $40) | Palisade Research --> --> Light Dark ^ Tldr: Sign up to learn about the state of current AI technology and why near-future AI developments might disempower humanity ( and earn $40 ). Palisade Research is a nonprofit that studies advanced AI systems in order to advise policy-makers and the public on AI risks. Our work has been cited in the US Senate , covered by Vox news , and retweeted by Elon Musk . We’re currently refining our messaging in preparation for briefi"
    contentLength: 23163
    status: verified
    note: null
  - footnote: 24
    url: https://ari.us/policy-bytes/ai-safety-research-highlights-of-2025/
    linkText: ARI Policy Bytes - AI Safety Research Highlights of 2025
    claimContext: The organization's work has contributed to broader discussions about AI safety, with their shutdown resistance research corroborated in broader AI safety reviews in 2025.[^24] Their empirical demonstrations have helped communicate abstract AI risks to both technical and non-technical audiences, incl
    fetchedAt: 2026-02-22T02:21:15.832Z
    httpStatus: 200
    pageTitle: AI Safety Research Highlights of 2025 - Americans for Responsible Innovation
    contentSnippet: AI Safety Research Highlights of 2025 - Americans for Responsible Innovation AI Safety Research Highlights of 2025 December 19, 2025 Iskandar Haykel 2025 has shaped up to be a watershed year for AI safety and security. Frontier models showed improved potential to facilitate CBRN threats, lending greater salience to these catastrophic risks. Leading AI developers accordingly strengthened safeguards and protocols . The &#8220;first reported AI-orchestrated cyber espionage campaign&#8221;—allegedly
    contentLength: 114986
    status: verified
    note: null
  - footnote: 27
    url: https://arxiv.org/html/2509.14260v1
    linkText: arXiv - Shutdown Resistance in Large Language Models
    claimContext: (footnote definition only, no inline reference found)
    fetchedAt: 2026-02-22T02:21:15.489Z
    httpStatus: 200
    pageTitle: Shutdown Resistance in Large Language Models
    contentSnippet: Shutdown Resistance in Large Language Models Shutdown Resistance in Large Language Models Jeremy Schlatter Palisade Research Benjamin Weinstein-Raun Palisade Research Jeffrey Ladish Palisade Research (September 2025) Abstract We show that several state-of-the-art large language models (including Grok 4, GPT-5, and Gemini 2.5 Pro) sometimes actively subvert a shutdown mechanism in their environment in order to complete a simple task, even when the instructions explicitly indicate not to interfere
    contentLength: 134241
    status: verified
    note: null
  - footnote: 28
    url: https://www.eweek.com/news/palisade-ai-shutdown-resistance-update-october-2025/
    linkText: eWeek - Palisade AI Shutdown Resistance Update
    claimContext: (footnote definition only, no inline reference found)
    fetchedAt: 2026-02-22T02:21:16.335Z
    httpStatus: 200
    pageTitle: "Palisade Update: AI Models Still Resist Shutdown Orders"
    contentSnippet: "Palisade Update: AI Models Still Resist Shutdown Orders Latest News Resources Artificial Intelligence Video Big Data & Analytics Cloud Networking Cybersecurity Applications IT Management Storage Mobile Small Business Development Database Servers Android Apple Innovation PC Hardware Reviews Search Engines Virtualization Blogs Newsletter Back to Menu Articles View All Hover to load posts Resource Hubs Confident Compliance Starts Here Always-On Tax Compliance For a Changing World Videos Partner Con"
    contentLength: 290819
    status: verified
    note: null
  - footnote: 29
    url: https://www.livescience.com/technology/artificial-intelligence/openais-smartest-ai-model-was-explicitly-told-to-shut-down-and-it-refused
    linkText: Live Science - OpenAI's AI Model Refused to Shut Down
    claimContext: (footnote definition only, no inline reference found)
    fetchedAt: 2026-02-22T02:21:15.737Z
    httpStatus: 200
    pageTitle: OpenAI's 'smartest' AI model was explicitly told to shut down &mdash; and it refused | Live Science
    contentSnippet: OpenAI's 'smartest' AI model was explicitly told to shut down &mdash; and it refused | Live Science Skip to main content Live Science Plus - Join our community JOIN NOW 10 Member Features 24/7 Access Available 25K Active Members Exclusive Newsletters Science news direct to your inbox Member Competitions Win exclusive prizes Exclusive Content Premium articles & videos Early Access First to see new features Private Forums Connect with members Monthly Rewards Surprise gifts & perks GET LIVE SCIENCE
    contentLength: 1357396
    status: verified
    note: null
