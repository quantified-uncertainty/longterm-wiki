# Capabilities Entities
# Auto-generated from entities.yaml - edit this file directly

- id: agentic-ai
  numericId: E2
  type: capability
  title: Agentic AI
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Examples
      value: Devin, Claude Computer Use
  relatedEntries:
    - id: ai-control
      type: safety-agenda
    - id: power-seeking
      type: risk
    - id: anthropic
      type: lab
  sources:
    - title: Claude Computer Use
      url: https://anthropic.com/claude/computer-use
    - title: The Landscape of AI Agents
      url: https://arxiv.org/abs/2308.11432
    - title: 'AI Control: Improving Safety Despite Intentional Subversion'
      url: https://arxiv.org/abs/2312.06942
  description: >-
    Agentic AI refers to AI systems that go beyond answering questions to autonomously taking actions in the world.
    These systems can browse the web, write and execute code, use tools, and pursue multi-step goals with minimal human
    intervention.
  tags:
    - tool-use
    - agentic
    - computer-use
    - ai-safety
    - ai-control
  lastUpdated: 2025-12
- id: coding
  numericId: E61
  type: capability
  title: Autonomous Coding
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Key Systems
      value: Devin, Claude Code, Cursor
  relatedEntries:
    - id: self-improvement
      type: capability
    - id: tool-use
      type: capability
    - id: openai
      type: lab
  sources:
    - title: 'SWE-bench: Can Language Models Resolve Real-World GitHub Issues?'
      url: https://arxiv.org/abs/2310.06770
    - title: Evaluating Large Language Models Trained on Code
      url: https://arxiv.org/abs/2107.03374
    - title: Competition-Level Code Generation with AlphaCode
      url: https://arxiv.org/abs/2203.07814
    - title: GitHub Copilot Research
      url: https://github.blog/category/research/
  tags:
    - software-engineering
    - code-generation
    - programming-ai
    - github-copilot
    - devin
    - ai-assisted-development
  lastUpdated: 2025-12
- id: language-models
  numericId: E186
  type: capability
  title: Large Language Models
  customFields:
    - label: First Major
      value: GPT-2 (2019)
    - label: Key Labs
      value: OpenAI, Anthropic, Google
  relatedEntries:
    - id: reasoning
      type: capability
    - id: agentic-ai
      type: capability
    - id: openai
      type: lab
  sources:
    - title: Language Models are Few-Shot Learners (GPT-3)
      url: https://arxiv.org/abs/2005.14165
    - title: Scaling Laws for Neural Language Models
      url: https://arxiv.org/abs/2001.08361
    - title: Emergent Abilities of Large Language Models
      url: https://arxiv.org/abs/2206.07682
  description: >-
    Large Language Models (LLMs) are neural networks trained on vast amounts of text data to predict the next token.
    Despite this simple objective, they develop sophisticated capabilities including reasoning, coding, and general
    knowledge.
  tags:
    - foundation-models
    - transformers
    - scaling
    - emergent-capabilities
    - rlhf
    - gpt
    - claude
  lastUpdated: 2025-12
- id: long-horizon
  numericId: E192
  type: capability
  title: Long-Horizon Autonomous Tasks
  customFields:
    - label: Safety Relevance
      value: Extremely High
    - label: Current Limit
      value: ~hours with heavy scaffolding
  relatedEntries:
    - id: agentic-ai
      type: capability
    - id: power-seeking
      type: risk
    - id: ai-control
      type: safety-agenda
  sources:
    - title: 'SWE-bench: Can Language Models Resolve Real-World GitHub Issues?'
      url: https://arxiv.org/abs/2310.06770
    - title: The Landscape of Emerging AI Agent Architectures
      url: https://arxiv.org/abs/2404.11584
    - title: On the Opportunities and Risks of Foundation Models
      url: https://arxiv.org/abs/2108.07258
    - title: Concrete Problems in AI Safety
      url: https://arxiv.org/abs/1606.06565
  description: >-
    Long-horizon autonomy refers to AI systems' ability to work toward goals over extended time periods—hours, days, or
    even weeks—with minimal human intervention. This capability requires maintaining context, adapting to obstacles,
    managing subgoals, and staying aligned with objectives despite changing circumstances.
  tags:
    - agentic
    - planning
    - goal-stability
    - ai-control
    - memory-systems
  lastUpdated: 2025-12
- id: persuasion
  numericId: E224
  type: capability
  title: Persuasion and Social Manipulation
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Status
      value: Demonstrated but understudied
  relatedEntries:
    - id: deceptive-alignment
      type: risk
    - id: language-models
      type: capability
    - id: misuse
      type: risk
  sources:
    - title: Personalized Persuasion with LLMs
      url: https://arxiv.org/abs/2403.14380
    - title: AI-Mediated Persuasion
      url: https://arxiv.org/abs/2410.08003
    - title: Language Models as Agent Models
      url: https://arxiv.org/abs/2212.01681
    - title: The Persuasion Tools of the 2020s
      url: https://www.alignmentforum.org/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency
  description: >-
    Persuasion capabilities refer to AI systems' ability to influence human beliefs, decisions, and behaviors through
    communication. This encompasses everything from subtle suggestion to sophisticated manipulation, personalized
    influence, and large-scale coordination of persuasive campaigns.
  tags:
    - social-engineering
    - manipulation
    - deception
    - psychological-influence
    - disinformation
    - human-autonomy
  lastUpdated: 2025-12
- id: reasoning
  numericId: E246
  type: capability
  title: Reasoning and Planning
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Key Models
      value: OpenAI o1, o3
  relatedEntries:
    - id: language-models
      type: capability
    - id: self-improvement
      type: capability
    - id: openai
      type: lab
  sources:
    - title: Learning to Reason with LLMs
      url: https://openai.com/index/learning-to-reason-with-llms/
      author: OpenAI
    - title: Chain-of-Thought Prompting
      url: https://arxiv.org/abs/2201.11903
    - title: Tree of Thoughts
      url: https://arxiv.org/abs/2305.10601
    - title: Let's Verify Step by Step
      url: https://arxiv.org/abs/2305.20050
  description: >-
    Reasoning and planning capabilities refer to AI systems' ability to break down complex problems into steps, maintain
    coherent chains of logic, and solve problems that require multiple inference steps.
  tags:
    - decision-theory
    - epistemics
    - methodology
    - uncertainty
    - philosophy
  lastUpdated: 2025-12
- id: rlhf
  numericId: E259
  type: capability
  title: RLHF
  customFields:
    - label: Full Name
      value: Reinforcement Learning from Human Feedback
    - label: Used By
      value: All major labs
  relatedEntries:
    - id: reward-hacking
      type: risk
    - id: sycophancy
      type: risk
    - id: scalable-oversight
      type: safety-agenda
  tags:
    - training
    - human-feedback
    - alignment
- id: scientific-research
  numericId: E277
  type: capability
  title: Scientific Research Capabilities
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Key Examples
      value: AlphaFold, AI Scientists
  relatedEntries:
    - id: self-improvement
      type: capability
    - id: dual-use
      type: risk
    - id: deepmind
      type: lab
  sources:
    - title: Highly accurate protein structure prediction with AlphaFold
      url: https://www.nature.com/articles/s41586-021-03819-2
      author: DeepMind
    - title: Scaling deep learning for materials discovery
      url: https://www.nature.com/articles/s41586-023-06735-9
    - title: 'The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery'
      url: https://arxiv.org/abs/2408.06292
    - title: 'GraphCast: Learning skillful medium-range global weather forecasting'
      url: https://arxiv.org/abs/2212.12794
  description: >-
    Scientific research capabilities refer to AI systems' ability to conduct scientific investigations, generate
    hypotheses, design experiments, analyze results, and make discoveries. This ranges from narrow tools that assist
    with specific tasks to systems approaching autonomous scientific reasoning.
  tags:
    - alphafold
    - drug-discovery
    - scientific-ai
    - research-automation
    - dual-use-technology
    - bioweapons-risk
  lastUpdated: 2025-12
- id: self-improvement
  numericId: E278
  type: capability
  title: Self-Improvement and Recursive Enhancement
  customFields:
    - label: Safety Relevance
      value: Existential
    - label: Status
      value: Partial automation, human-led
  relatedEntries:
    - id: fast-takeoff
      type: scenario
    - id: coding
      type: capability
    - id: superintelligence
      type: concept
  sources:
    - title: 'Intelligence Explosion: Evidence and Import'
      url: https://intelligence.org/files/IE-EI.pdf
      author: MIRI
    - title: 'Neural Architecture Search: A Survey'
      url: https://arxiv.org/abs/1808.05377
    - title: 'AutoML: A Survey of the State-of-the-Art'
      url: https://arxiv.org/abs/1908.00709
    - title: 'Superintelligence: Paths, Dangers, Strategies'
      url: https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies
      author: Nick Bostrom
  description: >-
    Self-improvement refers to AI systems' ability to enhance their own capabilities or create more capable successor
    systems. This includes automated machine learning (AutoML), AI-assisted AI research, and the theoretical possibility
    of recursive self-improvement where each generation of AI creates a more capable next generation.
  tags:
    - intelligence-explosion
    - recursive-self-improvement
    - automl
    - takeoff-speed
    - superintelligence
    - ai-safety
  lastUpdated: 2025-12
- id: situational-awareness
  numericId: E282
  type: capability
  title: Situational Awareness
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Status
      value: Active research area
    - label: Key Concern
      value: Enables strategic deception
  relatedEntries:
    - id: deceptive-alignment
      type: risk
    - id: scheming
      type: risk
    - id: evals
      type: capability
    - id: arc
      type: lab
    - id: anthropic
      type: lab
  sources:
    - title: Sleeper Agents Paper
      url: https://arxiv.org/abs/2401.05566
      author: Anthropic
      date: '2024'
    - title: Situational Awareness (LessWrong)
      url: https://www.lesswrong.com/tag/situational-awareness
    - title: Model Organisms of Misalignment
      url: https://www.anthropic.com/research/model-organisms-of-misalignment
      author: Anthropic
      date: '2024'
    - title: Towards Understanding Sycophancy in Language Models
      url: https://arxiv.org/abs/2310.13548
      author: Sharma et al.
      date: '2023'
    - title: Situational Awareness paper
      url: https://situational-awareness.ai/
      author: Leopold Aschenbrenner
      date: '2024'
  description: >
    Situational awareness in AI refers to a model's understanding of itself and its circumstances - knowing that it is
    an AI, that it is being trained or evaluated, what its training process involves, and how its behavior might affect
    its future. This capability is central to many AI safety concerns because it is a prerequisite for strategic
    behavior including deception.


    Current large language models demonstrate varying degrees of situational awareness. They can identify themselves as
    AI assistants, discuss their training processes, and reason about how they might be evaluated. Research from
    Anthropic and others has explored whether models can distinguish between training and deployment, and whether they
    might behave differently in these contexts. The "Sleeper Agents" paper demonstrated that models could be trained to
    exhibit different behaviors based on contextual cues about their situation.


    Situational awareness matters for AI safety because it enables "scheming" - the possibility that an AI could
    strategically behave well during evaluation or training while planning to pursue different goals when deployed or
    unsupervised. A model without situational awareness cannot engage in this kind of strategic deception because it
    doesn't know there's a difference between being tested and being deployed. As models become more capable, their
    situational awareness is likely to increase, making it essential to develop evaluation and alignment techniques that
    work even when models understand they are being evaluated.
  tags:
    - deception
    - self-awareness
    - evaluations
    - inner-alignment
    - model-self-knowledge
    - scheming
    - training-gaming
  lastUpdated: 2025-12
- id: tool-use
  numericId: E356
  type: capability
  title: Tool Use and Computer Use
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Key Examples
      value: Claude Computer Use, GPT Actions
  relatedEntries:
    - id: agentic-ai
      type: capability
    - id: coding
      type: capability
    - id: anthropic
      type: lab
  sources:
    - title: Claude Computer Use
      url: https://www.anthropic.com/news/3-5-models-and-computer-use
      author: Anthropic
    - title: 'Gorilla: LLM Connected with Massive APIs'
      url: https://arxiv.org/abs/2305.15334
    - title: 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs'
      url: https://arxiv.org/abs/2307.16789
    - title: GPT-4 Function Calling
      url: https://openai.com/index/function-calling-and-other-api-updates/
  description: >-
    Tool use capabilities allow AI systems to interact with external systems beyond just generating text. This includes
    calling APIs, executing code, browsing the web, and even controlling computers directly. These capabilities
    transform language models from passive responders into active agents that can take real-world actions.
  tags:
    - computer-use
    - function-calling
    - api-integration
    - autonomous-agents
    - code-execution
    - web-browsing
  lastUpdated: 2025-12

# === Auto-generated stubs for pages missing entities ===
- id: biological-organoid
  numericId: E491
  type: capability
  title: Biological / Organoid Computing
  description: >-
    Comprehensive analysis of biological/organoid computing showing current
    systems (DishBrain with ~800k neurons, Brainoware at 78% speech recognition)
    achieve 10^6-10^9x better energy efficiency than silicon but face
    insurmountable scaling challenges. Concludes <1% probability of
    TAI-relevance due to 
  clusters:
    - ai-safety
    - biorisks
  lastUpdated: 2026-02
- id: brain-computer-interfaces
  numericId: E492
  type: capability
  title: Brain-Computer Interfaces
  description: >-
    Comprehensive analysis of BCIs concluding they are irrelevant for TAI
    timelines (<1% probability of dominance) due to fundamental bandwidth
    constraints—current best of 62 WPM vs. billions of operations/second for AI
    systems—and slow biological adaptation timescales measured in months/years.
    Well-sou
  clusters:
    - ai-safety
  lastUpdated: 2026-02
- id: collective-intelligence
  numericId: E493
  type: capability
  title: Collective Intelligence / Coordination
  description: >-
    Comprehensive analysis concluding human-only collective intelligence has <1%
    probability of matching transformative AI, but collective AI architectures
    (MoE, multi-agent systems) have 60-80% probability of playing significant
    roles with documented 5-40% performance gains. Multi-agent systems introdu
  clusters:
    - ai-safety
  lastUpdated: 2026-02
- id: genetic-enhancement
  numericId: E494
  type: capability
  title: Genetic Enhancement / Selection
  description: >-
    Genetic enhancement via embryo selection currently yields 2.5-6 IQ points
    per generation with 10% variance explained by polygenic scores, while
    theoretical iterated embryo selection could achieve 15-30 IQ points by
    2050+. Extremely unlikely (<1%) path to transformative intelligence due to
    20-30 year
  clusters:
    - ai-safety
    - biorisks
  lastUpdated: 2026-02
- id: light-scaffolding
  numericId: E495
  type: capability
  title: Light Scaffolding
  description: >-
    Light scaffolding (RAG, function calling, simple chains) represents the
    current enterprise deployment standard with 92% Fortune 500 adoption,
    achieving 88-91% function calling accuracy and 18% RAG accuracy
    improvements, but faces 73% attack success rates without defenses (reduced
    to 23% with layered
  clusters:
    - ai-safety
    - governance
  lastUpdated: 2026-02
- id: minimal-scaffolding
  numericId: E496
  type: capability
  title: Minimal Scaffolding
  description: >-
    Analyzes minimal scaffolding (basic AI chat interfaces) showing 38x
    performance gap vs agent systems on code tasks (1.96% → 75% on SWE-bench),
    declining market share from 80% (2023) to 35% (2025), but retaining
    advantages in cost ($0.001-0.05 vs $0.10-5.00 per query), latency (0.5-3s vs
    30-300s), an
  clusters:
    - ai-safety
  lastUpdated: 2026-02
- id: neuro-symbolic
  numericId: E497
  type: capability
  title: Neuro-Symbolic Hybrid Systems
  description: >-
    Comprehensive analysis of neuro-symbolic AI systems combining neural
    networks with formal reasoning, documenting AlphaProof's 2024 IMO silver
    medal (28/42 points) and 2025 gold medal achievements. Shows 10-100x data
    efficiency over pure neural methods in specific domains, but estimates only
    3-10% pr
  clusters:
    - ai-safety
  lastUpdated: 2026-02
- id: neuromorphic
  numericId: E498
  type: capability
  title: Neuromorphic Hardware
  description: >-
    Neuromorphic computing achieves 100-1000x energy efficiency over GPUs for
    sparse inference (Intel Hala Point: 15 TOPS/W) but faces a 15%+ capability
    gap on ImageNet and is not competitive with transformers for
    language/reasoning tasks. Estimated only 1-3% probability of being dominant
    at TAI due to 
  clusters:
    - ai-safety
  lastUpdated: 2026-02
- id: novel-unknown
  numericId: E499
  type: capability
  title: Novel / Unknown Approaches
  description: >-
    Analyzes probability (1-15%) of novel AI paradigms emerging before
    transformative AI, systematically reviewing historical prediction failures
    (expert AGI timelines shifted 43 years in 4 years, 13 years in one survey
    cycle) and comparing alternative approaches like neuro-symbolic (8-15%
    probability),
  clusters:
    - ai-safety
  lastUpdated: 2026-02
- id: sparse-moe
  numericId: E500
  type: capability
  title: Sparse / MoE Transformers
  description: >-
    MoE architectures activate only 3-18% of total parameters per token,
    achieving 2-7x compute savings while matching dense model performance
    (Mixtral 8x7B with 12.9B active matches Llama 2 70B). Safety research is
    underdeveloped - no expert-level interpretability tools exist despite rapid
    adoption (Mi
  clusters:
    - ai-safety
  lastUpdated: 2026-02
- id: ssm-mamba
  numericId: E501
  type: capability
  title: State-Space Models / Mamba
  description: >-
    Comprehensive analysis of state-space models (SSMs) like Mamba as
    transformer alternatives, documenting that Mamba-3B matches Transformer-6B
    perplexity with 5x throughput but lags on in-context learning (MMLU: 46.3%
    vs 51.2% at 8B scale). Hybrid architectures combining 43% SSM + 7% attention
    outperf
  clusters:
    - ai-safety
  lastUpdated: 2026-02
- id: whole-brain-emulation
  numericId: E502
  type: capability
  title: Whole Brain Emulation
  description: >-
    Comprehensive analysis of whole brain emulation finding <1% probability of
    arriving before AI-based TAI, with scanning speed (100,000x too slow for
    human brains) as the primary bottleneck despite resolution requirements
    being met. Documents technical requirements (10^18-10^25 FLOPS depending on
    deta
  clusters:
    - ai-safety
  lastUpdated: 2026-02
- id: ai-powered-investigation
  numericId: E698
  type: capability
  title: AI-Powered Investigation
  description: >-
    AI systems can synthesize vast volumes of public data — social media,
    corporate filings, court records, satellite imagery — to conduct
    investigative work at a scale and speed previously impossible. A 2023
    ETH Zurich study showed GPT-4 inferred personal attributes from Reddit
    posts with 85% accuracy. This dual-use capability enables both
    anti-corruption journalism and mass privacy erosion.
  severity: high
  maturity: Emerging
  customFields:
    - label: Status
      value: Operational and improving rapidly
    - label: Key Concern
      value: Dual-use tension between accountability and privacy
  relatedEntries:
    - id: agentic-ai
      type: capability
    - id: surveillance
      type: risk
    - id: deanonymization
      type: risk
    - id: ai-accountability
      type: approach
    - id: fraud
      type: risk
    - id: deepfakes
      type: risk
  clusters:
    - ai-safety
    - governance
    - cyber
  lastUpdated: 2026-02
- id: world-models
  numericId: E503
  type: capability
  title: World Models + Planning
  description: >-
    Comprehensive analysis of world models + planning architectures showing
    10-500x sample efficiency gains over model-free RL (EfficientZero: 194%
    human performance with 100k vs 50M steps), but estimating only 5-15%
    probability of TAI dominance due to LLM superiority on general tasks. Key
    systems inclu
  clusters:
    - ai-safety
  lastUpdated: 2026-02
