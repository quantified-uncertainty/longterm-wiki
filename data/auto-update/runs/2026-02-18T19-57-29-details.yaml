digest:
  date: 2026-02-18
  itemCount: 10
  items:
    - title: Why we should expect ruthless sociopath ASI
      url: https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi
      sourceId: alignment-forum
      publishedAt: Wed, 18 Feb 2026 17:28:17 GMT
      summary: "Published on February 18, 2026 5:28 PM GMTThe conversation begins(Fictional) Optimist: So you expect future
        artificial superintelligence (ASI) “by default”, i.e. in the absence of yet-to-be-invented techniques, to be a
        ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous
        indifference to whether anyone (including its own programmers and users) lives or dies?Me: Yup! (Alas.)Optimist:
        …Despite all the evidence right in front of our eyes fro"
      relevanceScore: 85
      topics:
        - misaligned-catastrophe
        - goal-misgeneralization-probability
        - deceptive-alignment-decomposition
        - instrumental-convergence-framework
      entities:
        - misaligned-catastrophe
        - agi-development
    - title: "Research note: A simpler AI timelines model predicts 99% AI R&D automation in ~2032"
      url: https://www.alignmentforum.org/posts/uy6B5rEPvcwi55cBK/research-note-a-simpler-ai-timelines-model-predicts-99-ai-r
      sourceId: alignment-forum
      publishedAt: Thu, 12 Feb 2026 00:13:34 GMT
      summary: Published on February 12, 2026 12:13 AM GMTIn this post, I describe a simple model for forecasting when AI will
        automate AI development. It is based on the AI Futures model, but more understandable and robust, and has
        deliberately conservative assumptions.At current rates of compute growth and algorithmic progress, this model's
        median prediction is >99% automation of AI R&D in late 2032. Most simulations result in a 1000x to 10,000,000x
        increase in AI efficiency and 300x-3000x research output by
      relevanceScore: 82
      topics:
        - agi-timeline
        - ai-timelines
        - agi-development
        - self-improvement
      entities:
        - agi-timeline
        - ai-timelines
        - agi-development
    - title: Increasing AI Strategic Competence as a Safety Approach
      url: https://www.alignmentforum.org/posts/uECnWtbQ95dDWqBKD/increasing-ai-strategic-competence-as-a-safety-approach
      sourceId: alignment-forum
      publishedAt: Tue, 03 Feb 2026 01:08:13 GMT
      summary: Published on February 3, 2026 1:08 AM GMTIf AIs became strategically competent enough, they may realize that
        RSI is too dangerous because they're not good enough at alignment or philosophy or strategy, and potentially
        convince, help, or work with humans to implement an AI pause. This presents an alternative "victory condition"
        that someone could pursue (e.g. by working on AI strategic competence) if they were relatively confident about
        the alignment of near-human-level AIs but concerned about th
      relevanceScore: 79
      topics:
        - alignment-progress
        - strategic-competence
        - self-improvement
        - corrigibility-failure-pathways
      entities:
        - alignment-progress
        - agi-development
    - title: How do we (more) safely defer to AIs?
      url: https://www.alignmentforum.org/posts/vjAM7F8vMZS7oRrrh/how-do-we-more-safely-defer-to-ais
      sourceId: alignment-forum
      publishedAt: Thu, 12 Feb 2026 16:55:52 GMT
      summary: Published on February 12, 2026 4:55 PM GMTAs AI systems get more capable, it becomes increasingly uncompetitive
        and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently
        capable, control becomes infeasible. [1] Thus, one of the main strategies for handling AI risk is fully (or
        almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say "deferring to AIs" [2] I
        mean having these AIs do virtually all of the work to deve
      relevanceScore: 78
      topics:
        - corrigibility-failure-pathways
        - alignment-progress
        - power-seeking-conditions
        - institutional-adaptation-speed
      entities:
        - corrigibility-failure-pathways
        - alignment-progress
    - title: It Is Reasonable To Research How To Use Model Internals In Training
      url: https://www.alignmentforum.org/posts/G9HdpyREaCbFJjKu5/it-is-reasonable-to-research-how-to-use-model-internals-in
      sourceId: alignment-forum
      publishedAt: Sun, 08 Feb 2026 03:44:01 GMT
      summary: Published on February 8, 2026 3:44 AM GMTThere seems to be a common belief in the AGI safety community that
        involving interpretability in the training process is “the most forbidden technique”, including recent criticism
        of Goodfire for investing in this area. I find this odd since this is a pretty normal area of interpretability
        research in the AGI safety community. I have worked on it, Anthropic Fellows have worked on it, FAR has worked
        on it, etc. I don’t know if it will be net positive to us
      relevanceScore: 76
      topics:
        - interpretability-sufficient
        - alignment-progress
        - safety-research
        - mesa-optimization-analysis
      entities:
        - interpretability-sufficient
        - alignment-progress
    - title: Will reward-seekers respond to distant incentives?
      url: https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives
      sourceId: alignment-forum
      publishedAt: Mon, 16 Feb 2026 19:35:12 GMT
      summary: "Published on February 16, 2026 7:35 PM GMTReward-seekers are usually modeled as responding only to local
        incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a
        distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many
        copies of them in simulated deployments with different incentives?If reward-seekers are responsive to distant
        incentives, it fundamentally changes the threat model, and is p"
      relevanceScore: 75
      topics:
        - reward-hacking-taxonomy
        - goal-misgeneralization-probability
        - power-seeking-conditions
        - instrumental-convergence-framework
      entities:
        - reward-hacking-taxonomy
        - instrumental-convergence-framework
    - title: In (highly contingent!) defense of interpretability-in-the-loop ML training
      url: https://www.alignmentforum.org/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop
      sourceId: alignment-forum
      publishedAt: Fri, 06 Feb 2026 16:32:27 GMT
      summary: Published on February 6, 2026 4:32 PM GMTLet’s call “interpretability-in-the-loop training” the idea of running
        a learning algorithm that involves an inscrutable trained model, and there’s some kind of interpretability
        system feeding into the loss function / reward function.Interpretability-in-the-loop training has a very bad rap
        (and rightly so). Here’s&nbsp;Yudkowsky 2022:When you explicitly optimize against a detector of unaligned
        thoughts, you're partially optimizing for more aligned thought
      relevanceScore: 74
      topics:
        - interpretability-sufficient
        - alignment-progress
        - safety-research
        - capabilities
      entities:
        - interpretability-sufficient
        - alignment-progress
    - title: Human-like metacognitive skills will reduce LLM slop and aid alignment and capabilities
      url: https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid
      sourceId: alignment-forum
      publishedAt: Thu, 12 Feb 2026 19:38:50 GMT
      summary: Published on February 12, 2026 7:38 PM GMT1. Summary and overviewLLMs seem to lack metacognitive skills that
        help humans catch errors. Improvements to those skills might be net positive for alignment, despite improving
        capabilities in new directions.Better metacognition would reduce LLM errors by catching mistakes, and by
        managing complex cognition to produce better answers in the first place. This could stabilize or regularize
        alignment, allowing systems to avoid actions they would not "endorse
      relevanceScore: 72
      topics:
        - interpretability-sufficient
        - alignment-progress
        - safety-research
        - capabilities
      entities:
        - interpretability-sufficient
        - language-models
    - title: AXRP Episode 48 - Guive Assadi on AI Property Rights
      url: https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights
      sourceId: alignment-forum
      publishedAt: Sun, 15 Feb 2026 02:20:55 GMT
      summary: "Published on February 15, 2026 2:20 AM GMTYouTube link In this episode, Guive Assadi argues that we should
        give AIs property rights, so that they are integrated in our system of property and come to rely on it. The
        claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole
        property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal
        from and kill humans Why AIs may fear it could be them next AI "
      relevanceScore: 70
      topics:
        - alignment-progress
        - structural-risks
        - multi-actor-landscape
        - institutional-adaptation-speed
      entities:
        - structural-risks
        - multi-actor-landscape
    - title: Distinguish between inference scaling and "larger tasks use more compute"
      url: https://www.alignmentforum.org/posts/rRbDNQLfihiHbXytf/distinguish-between-inference-scaling-and-larger-tasks-use
      sourceId: alignment-forum
      publishedAt: Wed, 11 Feb 2026 18:37:13 GMT
      summary: Published on February 11, 2026 6:37 PM GMTAs many have observed, since reasoning models first came out, the
        amount of compute LLMs use to complete tasks has increased greatly. This trend is often called inference scaling
        and there is an open question of how much of recent AI progress is driven by inference scaling versus by other
        capability improvements. Whether inference compute is driving most recent AI progress matters because you can
        only scale up inference so far before costs are too high f
      relevanceScore: 68
      topics:
        - compute-hardware
        - capabilities
        - scaling-debate
        - reasoning
      entities:
        - compute-hardware
        - capabilities
        - scaling-debate
  fetchedSources:
    - alignment-forum
  failedSources: []
plan:
  date: 2026-02-18
  pageUpdates:
    - pageId: language-models
      pageTitle: Large Language Models
      reason: 1 news item(s) mention this entity directly
      suggestedTier: standard
      relevantNews:
        - title: Human-like metacognitive skills will reduce LLM slop and aid alignment and capabilities
          url: https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid
          summary: Published on February 12, 2026 7:38 PM GMT1. Summary and overviewLLMs seem to lack metacognitive skills that
            help humans catch errors. Improvements to those skills might be net positive for alignment,
      directions: "Review and incorporate recent developments: Human-like metacognitive skills will reduce LLM slop and aid
        alignment and capabilities"
  newPageSuggestions: []
  skippedReasons:
    - item: AI Timelines
      reason: Exceeded page limit
    - item: Alignment Progress
      reason: Exceeded page limit
    - item: AI Structural Risk Cruxes
      reason: Exceeded page limit
    - item: Compute & Hardware
      reason: Exceeded page limit
    - item: Corrigibility Failure Pathways
      reason: Exceeded page limit
    - item: Misaligned Catastrophe - The Bad Ending
      reason: Exceeded page limit
    - item: AGI Timeline
      reason: Exceeded page limit
    - item: Is Scaling All You Need?
      reason: Exceeded page limit
    - item: Instrumental Convergence Framework
      reason: Exceeded page limit
    - item: AGI Development
      reason: Exceeded page limit
    - item: AI Capabilities
      reason: Exceeded page limit
    - item: Is Interpretability Sufficient for Safety?
      reason: Exceeded page limit
    - item: Reward Hacking Taxonomy and Severity Model
      reason: Exceeded page limit
    - item: Multi-Actor Strategic Landscape
      reason: Exceeded page limit
  estimatedCost: 6.5
