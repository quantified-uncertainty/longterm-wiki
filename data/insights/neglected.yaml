insights:
  - id: "303"
    insight: >-
      Multi-agent AI dynamics are understudied: interactions between multiple AI systems could produce emergent risks
      not present in single-agent scenarios.
    source: /knowledge-base/cruxes/structural-risks
    tags:
      - multi-agent
      - emergence
      - coordination
    type: neglected
    surprising: 2.2
    important: 2.8
    actionable: 2.5
    neglected: 3.8
    compact: 3.2
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "304"
    insight: >-
      Non-Western perspectives on AI governance are systematically underrepresented in safety discourse, creating
      potential blind spots and reducing policy legitimacy.
    source: /ai-transition-model/factors/civilizational-competence/governance
    tags:
      - governance
      - diversity
      - epistemics
    type: neglected
    surprising: 1.5
    important: 2.5
    actionable: 2.8
    neglected: 3.5
    compact: 3.3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "715"
    insight: >-
      AI safety discourse may have epistemic monoculture: small community with shared assumptions could have systematic
      blind spots.
    source: /ai-transition-model/factors/civilizational-competence/epistemics
    tags:
      - epistemics
      - community
      - diversity
    type: neglected
    surprising: 2
    important: 2.5
    actionable: 2.8
    neglected: 3
    compact: 3.2
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "719"
    insight: >-
      Power concentration from AI may matter more than direct AI risk: transformative AI controlled by few could reshape
      governance without 'takeover'.
    source: /ai-transition-model/scenarios/long-term-lockin/political-power
    tags:
      - power-concentration
      - governance
      - structural-risk
    type: neglected
    surprising: 2
    important: 3.3
    actionable: 2.2
    neglected: 3
    compact: 3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "823"
    insight: >-
      Flash dynamics - AI systems interacting faster than human reaction time - may create qualitatively new systemic
      risks, yet this receives minimal research attention.
    source: /knowledge-base/cruxes/structural-risks
    tags:
      - flash-dynamics
      - speed
      - systemic
      - neglected
    type: neglected
    surprising: 2.2
    important: 2.8
    actionable: 2.5
    neglected: 3.5
    compact: 3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "824"
    insight: >-
      Values crystallization risk - AI could lock in current moral frameworks before humanity develops sufficient wisdom
      - is discussed theoretically but has no active research program.
    source: /ai-transition-model/scenarios/long-term-lockin/values
    tags:
      - lock-in
      - values
      - neglected
      - longtermism
    type: neglected
    surprising: 1.8
    important: 3
    actionable: 2
    neglected: 3.5
    compact: 3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: risk-portfolio-2
    insight: >-
      Governance/policy research is significantly underfunded: currently receives $18M (14% of funding) but optimal
      allocation for medium-timeline scenarios is 20-25%, creating a $7-17M annual funding gap.
    source: /knowledge-base/models/analysis-models/ai-risk-portfolio-analysis/
    tags:
      - funding-gap
      - governance
      - neglected-area
    type: neglected
    surprising: 2.5
    important: 4
    actionable: 4
    neglected: 3.5
    compact: 3
    added: 2025-01-22T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: risk-portfolio-3
    insight: >-
      Agent safety is severely underfunded at $8.2M (6% of funding) versus the optimal 10-15% allocation, representing a
      $7-12M annual gap—a high-value investment opportunity with substantial room for marginal contribution.
    source: /knowledge-base/models/analysis-models/ai-risk-portfolio-analysis/
    tags:
      - agent-safety
      - funding-gap
      - high-neglectedness
    type: neglected
    surprising: 3
    important: 3.5
    actionable: 4
    neglected: 4
    compact: 3.5
    added: 2025-01-22T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: goal-misgeneralization-probability-81
    insight: >-
      The evaluation-to-deployment shift represents the highest risk scenario (Type 4 extreme shift) with 27.7% base
      misgeneralization probability, yet this critical transition receives insufficient attention in current safety
      practices.
    source: /knowledge-base/models/risk-models/goal-misgeneralization-probability/
    tags:
      - evaluation-deployment
      - distribution-shift
      - safety-practices
    type: neglected
    surprising: 3
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: international-regimes-48
    insight: >-
      Only 7 of 193 UN member states participate in the seven most prominent AI governance initiatives, while 118
      countries (mostly in the Global South) are entirely absent from AI governance discussions as of late 2024.
    source: /knowledge-base/responses/governance/compute-governance/international-regimes/
    tags:
      - global-governance
      - participation-gaps
      - developing-countries
    type: neglected
    surprising: 3
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: openai-foundation-1
    insight: >-
      The OpenAI Foundation holds $130 billion in equity but faces a fundamental incentive problem: selling shares to
      fund philanthropy would depress the stock price, creating pressure to hold assets indefinitely rather than deploy
      them for charitable purposes.
    source: /knowledge-base/organizations/funders/openai-foundation/
    tags:
      - governance
      - philanthropy
      - incentives
    type: neglected
    surprising: 3
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: openai-foundation-3
    insight: >-
      The foundation's Safety and Security Committee consists of only four part-time volunteers with no dedicated staff,
      tasked with overseeing development of potentially transformative AGI systems.
    source: /knowledge-base/organizations/funders/openai-foundation/
    tags:
      - safety
      - governance
      - resources
    type: neglected
    surprising: 3.5
    important: 4
    actionable: 3.5
    neglected: 3.5
    compact: 4
    added: 2026-02-03T00:00:00.000Z
  - id: schmidt-futures-20
    insight: >-
      The organization's AI Safety Science program addresses what it describes as significant underfunding by providing
      not just grants up to $500K but also computational resources from CAIS and OpenAI API access.
    source: /knowledge-base/organizations/funders/schmidt-futures/
    tags:
      - ai-safety
      - funding-gaps
      - computational-resources
    type: neglected
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 3.5
    added: 2026-02-03T00:00:00.000Z
  - id: coefficient-giving-26
    insight: >-
      Coefficient Giving (formerly Open Philanthropy) deployed only ~$50M to AI safety in 2024, with 68% going to
      evaluations/benchmarking rather than core alignment research, despite representing ~60% of all external AI safety
      funding.
    source: /knowledge-base/organizations/funders/coefficient-giving/
    tags:
      - funding
      - ai-safety
      - alignment
    type: neglected
    surprising: 3
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: hewlett-foundation-42
    insight: >-
      The Hewlett Foundation allocated over $8 million to AI cybersecurity research (including $2M to Georgetown CSET
      and $5M to FAMU) while explicitly avoiding AI alignment or existential risk work, distinguishing it from other
      major AI safety funders.
    source: /knowledge-base/organizations/funders/hewlett-foundation/
    tags:
      - ai-safety
      - funding
      - cybersecurity
      - alignment
    type: neglected
    surprising: 3
    important: 3
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: sff-47
    insight: >-
      Jaan Tallinn simultaneously funds three distinct grantmaking mechanisms (SFF S-process, Speculation Grants with
      ~$16M budget, and Lightspeed Grants) with different speed-information tradeoffs, from 1-2 week to 3-6 month
      decisions.
    source: /knowledge-base/organizations/funders/sff/
    tags:
      - funding-mechanisms
      - grantmaking
      - portfolio-theory
    type: neglected
    surprising: 3
    important: 2.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: fli-49
    insight: >-
      FLI transferred $368 million to three entities controlled by the same four people (Tegmark, Chita-Tegmark,
      Aguirre, Tallinn) in December 2022, while their 2023 operational income was only $624,714.
    source: /knowledge-base/organizations/funders/fli/
    tags:
      - governance
      - financial-transparency
      - nonprofit-management
    type: neglected
    surprising: 3.5
    important: 3
    actionable: 2.5
    neglected: 3.5
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: nist-ai-2
    insight: >-
      NIST's $47.7M annual AI budget funds all U.S. federal AI standards, evaluation, and safety coordination—less than
      a single AI startup's Series A—with persistent underfunding forcing discussions about private-foundation funding
      for critical government infrastructure.
    source: /knowledge-base/organizations/government/nist-ai/
    tags:
      - funding-constraints
      - government-capacity
      - institutional-underfunding
      - AI-governance
    type: neglected
    surprising: 4
    important: 4
    actionable: 3
    neglected: 4
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: pause-moratorium-1
    insight: >-
      Despite 33,000+ signatures on the March 2023 AI pause letter, no major jurisdiction has implemented mandatory
      training pauses—revealing a disconnect between stated concern and policy traction that deserves more analysis.
    source: /knowledge-base/responses/governance/pause-moratorium/
    tags:
      - policy-gap
      - implementation
      - advocacy-effectiveness
    type: neglected
    surprising: 2.5
    important: 3
    actionable: 3
    neglected: 3
    compact: 3.5
    added: 2026-02-03T00:00:00.000Z
  - id: metr-3
    insight: >-
      METR—the primary organization that OpenAI, Anthropic, and Google DeepMind rely on to determine whether frontier AI
      models are safe to deploy—operates with approximately 30 specialists and \$17M in funding, creating a single point
      of failure in global AI safety infrastructure.
    source: /knowledge-base/organizations/safety/metr/
    tags:
      - ai-governance
      - organizational-capacity
      - ai-safety-infrastructure
    type: neglected
    surprising: 3
    important: 4
    actionable: 4
    neglected: 3
    compact: 3
    added: 2026-02-03T00:00:00.000Z
