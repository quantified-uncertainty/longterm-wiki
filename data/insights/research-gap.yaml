insights:
  - id: "201"
    insight: >-
      No reliable methods exist to detect whether an AI system is being deceptive about its goals - we can't distinguish
      genuine alignment from strategic compliance.
    source: /knowledge-base/cruxes/accident-risks
    tags:
      - deception
      - detection
      - alignment
      - evaluation
    type: research-gap
    surprising: 1.5
    important: 3.8
    actionable: 3.5
    neglected: 2.5
    compact: 3.2
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "202"
    insight: >-
      We lack empirical methods to study goal preservation under capability improvement - a core assumption of AI risk
      arguments remains untested.
    source: /knowledge-base/cruxes/accident-risks
    tags:
      - goal-stability
      - self-improvement
      - empirical
    type: research-gap
    surprising: 2.2
    important: 3.5
    actionable: 3.2
    neglected: 3
    compact: 3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "204"
    insight: >-
      AI-assisted alignment research is underexplored: current safety work rarely uses AI to accelerate itself, despite
      potential for 10x+ speedups on some tasks.
    source: /ai-transition-model/factors/ai-uses/recursive-ai-capabilities
    tags:
      - meta-research
      - acceleration
      - alignment
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.8
    neglected: 3.2
    compact: 3.2
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "205"
    insight: >-
      Economic models of AI transition are underdeveloped - we don't have good theories of how AI automation affects
      labor, power, and stability during rapid capability growth.
    source: /ai-transition-model/factors/transition-turbulence/economic-stability
    tags:
      - economics
      - transition
      - modeling
    type: research-gap
    surprising: 2
    important: 2.8
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "818"
    insight: >-
      No clear mesa-optimizers detected in GPT-4 or Claude-3, but this may reflect limited interpretability rather than
      absence - we cannot distinguish 'safe' from 'undetectable'.
    source: /knowledge-base/cruxes/accident-risks
    tags:
      - mesa-optimization
      - interpretability
      - research-gap
    type: research-gap
    surprising: 1.5
    important: 3.5
    actionable: 3
    neglected: 2.5
    compact: 3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "819"
    insight: >-
      Compute-labor substitutability for AI R&D is poorly understood - whether cognitive labor alone can drive explosive
      progress or compute constraints remain binding is a key crux.
    source: /knowledge-base/capabilities/self-improvement
    tags:
      - self-improvement
      - compute
      - research-gap
      - cruxes
    type: research-gap
    surprising: 2.2
    important: 3.5
    actionable: 2.5
    neglected: 3
    compact: 2.8
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "820"
    insight: >-
      No empirical studies on whether institutional trust can be rebuilt after collapse - a critical uncertainty for
      epistemic risk mitigation strategies.
    source: /knowledge-base/cruxes/structural-risks
    tags:
      - trust
      - institutions
      - research-gap
      - epistemic
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "821"
    insight: >-
      Whether sophisticated AI could hide from interpretability tools is unknown - the 'interpretability tax' question
      is largely unexplored empirically.
    source: /knowledge-base/cruxes/accident-risks
    tags:
      - interpretability
      - deception
      - research-gap
    type: research-gap
    surprising: 1.5
    important: 3.5
    actionable: 3
    neglected: 2.5
    compact: 3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: atm-012
    insight: >-
      Scalable oversight has fundamental uncertainty (2/10 certainty) despite being existentially important (9/10
      sensitivity) - all near-term safety depends on solving a problem with no clear solution path.
    source: /ai-transition-model/factors/misalignment-potential/technical-ai-safety
    tags:
      - oversight
      - scalability
      - uncertainty
      - alignment
    type: research-gap
    surprising: 1.5
    important: 3.8
    actionable: 3
    neglected: 2.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: intervention-effectiveness-3
    insight: >-
      Deceptive Alignment detection has only ~15% effective interventions requiring $500M+ over 3 years; Scheming
      Prevention has ~10% coverage requiring $300M+ over 5 years—two Tier 1 existential priority gaps.
    source: /knowledge-base/models/intervention-models/intervention-effectiveness-matrix/
    tags:
      - gap-analysis
      - deceptive-alignment
      - investment-requirements
    type: research-gap
    surprising: 3.2
    important: 4
    actionable: 3.3
    neglected: 3.5
    compact: 3
    added: 2025-01-22T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: warning-signs-model-2
    insight: >-
      The optimal AI risk monitoring system must balance early detection sensitivity with avoiding false positives,
      requiring a multi-layered detection architecture that trades off between anticipation and confirmation.
    source: /knowledge-base/models/analysis-models/warning-signs-model/
    tags:
      - methodology
      - risk-assessment
      - detection
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3
    neglected: 2.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: safety-research-allocation-6
    insight: >-
      Critical AI safety research areas like multi-agent dynamics and corrigibility are receiving 3-5x less funding than
      their societal importance would warrant, with current annual investment of less than $20M versus an estimated need
      of $60-80M.
    source: /knowledge-base/models/intervention-models/safety-research-allocation/
    tags:
      - underfunded-research
      - safety-priorities
    type: research-gap
    surprising: 3
    important: 4
    actionable: 3.5
    neglected: 4
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: intervention-timing-windows-8
    insight: >-
      Organizations should rapidly shift 20-30% of AI safety resources toward time-sensitive 'closing window'
      interventions, prioritizing compute governance and international coordination before geopolitical tensions make
      cooperation impossible.
    source: /knowledge-base/models/timeline-models/intervention-timing-windows/
    tags:
      - resource-allocation
      - strategic-priorities
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 4
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: ai-assisted-7
    insight: >-
      The fundamental bootstrapping problem remains unsolved: using AI to align more powerful AI only works if the
      helper AI is already reliably aligned.
    source: /knowledge-base/responses/alignment/ai-assisted/
    tags:
      - alignment-challenge
      - meta-alignment
    type: research-gap
    surprising: 2.5
    important: 4
    actionable: 2
    neglected: 3
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: capabilities-13
    insight: >-
      Rapid AI capability progress is outpacing safety evaluation methods, with benchmark saturation creating critical
      blind spots in AI risk assessment across language, coding, and reasoning domains.
    source: /knowledge-base/metrics/capabilities/
    tags:
      - AI safety
      - evaluation
      - risks
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: bioweapons-ai-uplift-3
    insight: >-
      The compound integration of AI technologies—combining language models, protein structure prediction, generative
      biological models, and automated laboratory systems—could create emergent risks that exceed any individual
      technology's contribution.
    source: /knowledge-base/models/domain-models/bioweapons-ai-uplift/
    tags:
      - capability-integration
      - systemic-risk
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 2
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: corrigibility-failure-pathways-6
    insight: >-
      Current AI safety research funding is critically underresourced, with key areas like Formal Corrigibility Theory
      receiving only ~$5M annually against estimated needs of $30-50M.
    source: /knowledge-base/models/risk-models/corrigibility-failure-pathways/
    tags:
      - funding
      - research-priorities
      - resource-allocation
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 4
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: technical-research-18
    insight: >-
      Technical AI safety research is currently funded at only $80-130M annually, which is insufficient compared to
      capabilities research spending, despite having potential to reduce existential risk by 5-50%.
    source: /knowledge-base/responses/alignment/technical-research/
    tags:
      - funding
      - resource-allocation
      - x-risk
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: responsible-scaling-policies-21
    insight: >-
      Current AI safety evaluation techniques may detect only 50-70% of dangerous capabilities, creating significant
      uncertainty about the actual risk mitigation effectiveness of Responsible Scaling Policies.
    source: /knowledge-base/responses/governance/industry/responsible-scaling-policies/
    tags:
      - evaluation-methodology
      - capability-detection
      - safety-limitations
    type: research-gap
    surprising: 3
    important: 4
    actionable: 3.5
    neglected: 3
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: scheming-likelihood-model-13
    insight: >-
      Current annual funding for scheming-related safety research is estimated at only $45-90M against an assessed need
      of $200-400M, representing a 2-4x funding shortfall for addressing this catastrophic risk.
    source: /knowledge-base/models/risk-models/scheming-likelihood-model/
    tags:
      - funding-gaps
      - resource-allocation
      - neglectedness
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: risk-activation-timeline-18
    insight: >-
      Critical interventions like bioweapons DNA synthesis screening ($100-300M globally) and authentication
      infrastructure ($200-500M) have high leverage but narrow implementation windows closing by 2026-2027.
    source: /knowledge-base/models/timeline-models/risk-activation-timeline/
    tags:
      - interventions
      - bioweapons-screening
      - authentication
      - funding
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 4
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: lab-culture-22
    insight: >-
      Only 3 of 7 major AI labs conduct substantive testing for dangerous biological and cyber capabilities, despite
      these being among the most immediate misuse risks from advanced AI systems.
    source: /knowledge-base/responses/organizational-practices/lab-culture/
    tags:
      - dangerous-capabilities
      - biosafety
      - cybersecurity
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3.5
    neglected: 3
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: alignment-progress-38
    insight: >-
      Current interpretability techniques cover only 15-25% of model behavior, and sparse autoencoders trained on the
      same model with different random seeds learn substantially different feature sets, indicating decomposition is not
      unique but rather a 'pragmatic artifact of training conditions.'
    source: /knowledge-base/metrics/alignment-progress/
    tags:
      - interpretability
      - measurement-validity
      - fundamental-limitations
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: long-horizon-49
    insight: >-
      Safety research is projected to lag capability development by 1-2 years, with reliable 4-8 hour autonomy expected
      by 2025 while comprehensive safety frameworks aren't projected until 2027+.
    source: /knowledge-base/capabilities/long-horizon/
    tags:
      - safety-timeline
      - capability-timeline
      - research-priorities
    type: research-gap
    surprising: 2
    important: 3.5
    actionable: 3
    neglected: 2
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: evaluation-13
    insight: >-
      Self-improvement capability evaluation remains at the 'Conceptual' maturity level despite being a critical
      capability for AI risk, with only ARC Evals working on code modification tasks as assessment methods.
    source: /knowledge-base/responses/evaluation/
    tags:
      - self-improvement
      - capability-evaluation
      - research-gaps
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: effectiveness-assessment-14
    insight: >-
      Only 15-20% of AI policies worldwide have established measurable outcome data, and fewer than 20% of evaluations
      meet moderate evidence standards, creating a critical evidence gap that undermines informed governance decisions.
    source: /knowledge-base/responses/governance/effectiveness-assessment/
    tags:
      - governance
      - evaluation
      - evidence
    type: research-gap
    surprising: 3
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: corrigibility-failure-21
    insight: >-
      No complete solution to corrigibility failure exists despite nearly a decade of research, with utility
      indifference failing reflective consistency tests and other approaches having fundamental limitations that may be
      irresolvable.
    source: /knowledge-base/risks/accident/corrigibility-failure/
    tags:
      - open-problems
      - solution-limitations
      - foundational-challenges
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: scheming-31
    insight: >-
      Linear probes achieve 99% AUROC in detecting trained backdoor behaviors, but it remains unknown whether this
      detection capability generalizes to naturally-emerging scheming versus artificially inserted deception.
    source: /knowledge-base/risks/accident/scheming/
    tags:
      - detection
      - interpretability
      - generalization
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: sharp-left-turn-35
    insight: >-
      Despite 3-4 orders of magnitude capability improvements potentially occurring from GPT-4 to AGI-level systems by
      2025-2027, researchers lack reliable methods for predicting when capability transitions will occur or measuring
      alignment generalization in real-time.
    source: /knowledge-base/risks/accident/sharp-left-turn/
    tags:
      - capability-scaling
      - measurement
      - prediction
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: agi-timeline-47
    insight: >-
      AGI definition choice creates systematic 10-15 year timeline variations, with economic substitution definitions
      yielding 2040-2060 ranges while human-level performance benchmarks suggest 2030-2040, indicating definitional work
      is critical for meaningful forecasting.
    source: /knowledge-base/forecasting/agi-timeline/
    tags:
      - agi-definition
      - forecasting-methodology
      - measurement-challenges
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: risk-interaction-matrix-60
    insight: >-
      Higher-order interactions between 3+ risks remain largely unexplored despite likely significance, representing a
      critical research gap as current models only capture pairwise effects while system-wide phase transitions may
      emerge from multi-way interactions.
    source: /knowledge-base/models/dynamics-models/risk-interaction-matrix/
    tags:
      - modeling-limitations
      - higher-order-effects
      - research-priorities
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 4
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: worldview-intervention-mapping-72
    insight: >-
      Three core belief dimensions (timelines, alignment difficulty, coordination feasibility) systematically determine
      intervention priorities, yet most researchers have never explicitly mapped their beliefs to coherent work
      strategies.
    source: /knowledge-base/models/intervention-models/worldview-intervention-mapping/
    tags:
      - strategic-clarity
      - belief-mapping
      - career-guidance
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: capability-alignment-race-75
    insight: >-
      Deception detection capabilities are critically underdeveloped at only 20% reliability, yet need to reach 95% for
      AGI safety, representing one of the largest capability-safety gaps.
    source: /knowledge-base/models/race-models/capability-alignment-race/
    tags:
      - deception-detection
      - alignment
      - capability-gap
    type: research-gap
    surprising: 3.5
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: california-sb1047-22
    insight: >-
      SB 1047's veto highlighted a fundamental regulatory design tension between size-based thresholds (targeting large
      models regardless of use) versus risk-based approaches (targeting dangerous deployments regardless of model size),
      with Governor Newsom explicitly preferring the latter approach.
    source: /knowledge-base/responses/governance/legislation/california-sb1047/
    tags:
      - regulatory-design
      - policy-frameworks
      - risk-assessment
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3.5
    neglected: 3
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: eu-ai-act-27
    insight: >-
      The EU AI Act's focus remains primarily on near-term harms rather than existential risks, creating a significant
      regulatory gap for catastrophic AI risks despite establishing infrastructure for advanced AI oversight.
    source: /knowledge-base/responses/governance/legislation/eu-ai-act/
    tags:
      - existential-risk
      - regulatory-gaps
      - governance
    type: research-gap
    surprising: 1.5
    important: 3.5
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: pause-32
    insight: >-
      Even successful pause implementation faces a critical 2-5 year window assumption that may be insufficient, as
      fundamental alignment problems like mechanistic interpretability remain far from scalable solutions for frontier
      models with hundreds of billions of parameters.
    source: /knowledge-base/responses/organizational-practices/pause/
    tags:
      - alignment-timeline
      - scalability-challenge
      - interpretability
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 2.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: emergent-capabilities-35
    insight: >-
      Current AI safety evaluations can only demonstrate the presence of capabilities, not their absence, creating a
      fundamental gap where dangerous abilities may exist but remain undetected until activated.
    source: /knowledge-base/risks/accident/emergent-capabilities/
    tags:
      - evaluation-gaps
      - latent-capabilities
      - safety-testing
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3.5
    neglected: 3
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: racing-dynamics-41
    insight: >-
      Current voluntary coordination mechanisms show critical gaps with unknown compliance rates for pre-deployment
      evaluations, only 23% participation in safety research collaboration despite signatures, and no implemented
      enforcement mechanisms for capability threshold monitoring among the 16 signatory companies.
    source: /knowledge-base/risks/structural/racing-dynamics/
    tags:
      - coordination
      - governance
      - verification
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3.5
    neglected: 3
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: safety-research-value-63
    insight: >-
      The talent bottleneck of approximately 1,000 qualified AI safety researchers globally represents a critical
      constraint that limits the absorptive capacity for additional funding in the field.
    source: /knowledge-base/models/intervention-models/safety-research-value/
    tags:
      - talent-pipeline
      - scaling-constraints
      - field-building
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 2.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: persuasion-79
    insight: >-
      The research community lacks standardized benchmarks for measuring AI persuasion capabilities across domains,
      creating a critical gap in our ability to track and compare persuasive power as models scale.
    source: /knowledge-base/capabilities/persuasion/
    tags:
      - evaluation
      - benchmarks
      - measurement
    type: research-gap
    surprising: 2
    important: 3
    actionable: 4
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: goal-misgeneralization-96
    insight: >-
      Current detection methods for goal misgeneralization remain inadequate, with standard training and evaluation
      procedures failing to catch the problem before deployment since misalignment only manifests under distribution
      shifts not present during training.
    source: /knowledge-base/risks/accident/goal-misgeneralization/
    tags:
      - detection-methods
      - evaluation-gaps
      - deployment-safety
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: mesa-optimization-101
    insight: >-
      Mesa-optimization may manifest as complicated stacks of heuristics rather than clean optimization procedures,
      making it unlikely to be modular or clearly separable from the rest of the network.
    source: /knowledge-base/risks/accident/mesa-optimization/
    tags:
      - mechanistic-interpretability
      - detection
      - architecture
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 2.5
    neglected: 3
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: critical-uncertainties-3
    insight: >-
      Resolving just 10 key uncertainties could shift AI risk estimates by 2-5x and change strategic recommendations,
      with targeted research costing $100-200M/year potentially providing enormous value of information compared to
      current ~$20-30M uncertainty-resolution spending.
    source: /knowledge-base/models/analysis-models/critical-uncertainties/
    tags:
      - value-of-information
      - research-prioritization
      - uncertainty-resolution
    type: research-gap
    surprising: 3
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: monitoring-12
    insight: >-
      Jurisdictional arbitrage represents a fundamental limitation where sophisticated actors can move operations to
      less-regulated countries, requiring either comprehensive international coordination (assessed 15-25% probability)
      or acceptance of significant monitoring gaps.
    source: /knowledge-base/responses/governance/compute-governance/monitoring/
    tags:
      - international-coordination
      - policy-gaps
      - governance-limitations
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: model-registries-16
    insight: >-
      Open-source AI development creates a fundamental coverage gap for model registries since they focus on centralized
      developers, requiring separate post-release monitoring and community registry approaches that remain largely
      unaddressed in current implementations.
    source: /knowledge-base/responses/governance/model-registries/
    tags:
      - open-source
      - governance-gaps
      - implementation
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: authoritarian-takeover-28
    insight: >-
      Current export controls on surveillance technology are insufficient - only 19 Chinese AI companies are on the US
      Entity List while Chinese firms have already captured 34% of the global surveillance camera market and deployed
      systems in 80+ countries.
    source: /knowledge-base/risks/structural/authoritarian-takeover/
    tags:
      - export-controls
      - policy-gaps
      - surveillance-tech
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 4
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: misuse-risks-32
    insight: >-
      Provenance-based authentication systems like C2PA are emerging as the primary technical response to synthetic
      content rather than detection, as the detection arms race appears to structurally favor content generation over
      identification.
    source: /knowledge-base/cruxes/misuse-risks/
    tags:
      - authentication
      - provenance
      - technical-solutions
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: bioweapons-attack-chain-38
    insight: >-
      The compound probability uncertainty spans 180x (0.02% to 3.6%) due to multiplicative error propagation across
      seven uncertain parameters, representing genuine deep uncertainty rather than statistical confidence intervals.
    source: /knowledge-base/models/domain-models/bioweapons-attack-chain/
    tags:
      - uncertainty-quantification
      - risk-modeling
      - epistemics
    type: research-gap
    surprising: 2.5
    important: 2.5
    actionable: 2.5
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: us-executive-order-43
    insight: >-
      Static compute thresholds become obsolete within 3-5 years due to algorithmic efficiency improvements, suggesting
      future AI governance frameworks should adopt capability-based rather than compute-based triggers.
    source: /knowledge-base/responses/governance/legislation/us-executive-order/
    tags:
      - regulatory-design
      - compute-vs-capabilities
      - threshold-obsolescence
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: agi-development-73
    insight: >-
      AGI development faces a critical 3-5 year lag between capability advancement and safety research readiness, with
      alignment research trailing production systems by the largest margin.
    source: /knowledge-base/forecasting/agi-development/
    tags:
      - safety-capability-gap
      - alignment
      - timeline-mismatch
    type: research-gap
    surprising: 3
    important: 3.5
    actionable: 3
    neglected: 2.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: reward-hacking-taxonomy-8
    insight: >-
      No single mitigation strategy is effective across all reward hacking modes - better specification reduces proxy
      exploitation by 40-60% but only reduces deceptive hacking by 5-15%, while AI control methods can achieve 60-90%
      harm reduction for severe modes, indicating need for defense-in-depth approaches.
    source: /knowledge-base/models/risk-models/reward-hacking-taxonomy/
    tags:
      - mitigation-effectiveness
      - defense-in-depth
      - ai-control
      - specification
      - strategy
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: metr-11
    insight: >-
      METR's evaluation-based safety approach faces a fundamental scalability crisis, with only ~30 specialists
      evaluating increasingly complex models across multiple risk domains, creating inevitable trade-offs in evaluation
      depth that may miss novel dangerous capabilities.
    source: /knowledge-base/organizations/safety-orgs/metr/
    tags:
      - organizational-capacity
      - evaluation-methodology
      - safety-bottlenecks
    type: research-gap
    surprising: 3
    important: 3.5
    actionable: 3.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: anthropic-core-views-18
    insight: >-
      Constitutional AI research reveals a fundamental dependency on model capabilities—the technique relies on the
      model's own reasoning abilities for self-correction, making it potentially less transferable to smaller or less
      sophisticated systems.
    source: /knowledge-base/responses/alignment/anthropic-core-views/
    tags:
      - constitutional-ai
      - scalability
      - alignment-techniques
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3
    neglected: 3
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: corporate-influence-34
    insight: >-
      Current legal protections for AI whistleblowers are weak, but 2024 saw unprecedented activity with anonymous SEC
      complaints alleging OpenAI used illegal NDAs to prevent safety disclosures, leading to bipartisan introduction of
      the AI Whistleblower Protection Act.
    source: /knowledge-base/responses/field-building/corporate-influence/
    tags:
      - whistleblowing
      - legal-framework
      - regulatory-response
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: field-building-analysis-37
    insight: >-
      The AI safety talent pipeline is over-optimized for researchers while neglecting operations, policy, and
      organizational leadership roles that are more neglected bottlenecks.
    source: /knowledge-base/responses/field-building/field-building-analysis/
    tags:
      - talent-pipeline
      - bottlenecks
      - career-diversity
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: thresholds-56
    insight: >-
      The shift to inference-time scaling (demonstrated by models like OpenAI's o1) fundamentally undermines compute
      threshold governance, as models trained below thresholds can achieve above-threshold capabilities through
      deployment-time computation.
    source: /knowledge-base/responses/governance/compute-governance/thresholds/
    tags:
      - inference-scaling
      - regulatory-blind-spots
      - threshold-gaps
    type: research-gap
    surprising: 3
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: international-summits-61
    insight: >-
      Despite achieving unprecedented international recognition of AI catastrophic risks, all summit commitments remain
      non-binding with no enforcement mechanisms, contributing an estimated 15-30% toward binding frameworks by 2030.
    source: /knowledge-base/responses/governance/international/international-summits/
    tags:
      - enforcement
      - governance
      - effectiveness
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: colorado-ai-act-71
    insight: >-
      Colorado's narrow focus on discrimination in consequential decisions may miss other significant AI safety risks
      including privacy violations, system manipulation, or safety-critical failures in domains like transportation.
    source: /knowledge-base/responses/governance/legislation/colorado-ai-act/
    tags:
      - scope-limitations
      - ai-safety
      - regulatory-gaps
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: ai-safety-institutes-75
    insight: >-
      Timeline mismatches between evaluation cycles (months) and deployment decisions (weeks) may render AISI work
      strategically irrelevant as AI development accelerates, creating a fundamental structural limitation.
    source: /knowledge-base/responses/institutions/ai-safety-institutes/
    tags:
      - evaluation-methodology
      - timing
      - governance-challenges
    type: research-gap
    surprising: 3
    important: 3
    actionable: 2.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: whistleblower-protections-77
    insight: >-
      Current US whistleblower laws provide essentially no protection for AI safety disclosures because they were
      designed for specific regulated industries - disclosures about inadequate alignment testing or dangerous
      capability deployment don't fit within existing protected categories like securities fraud or workplace safety.
    source: /knowledge-base/responses/organizational-practices/whistleblower-protections/
    tags:
      - legal-frameworks
      - regulatory-gaps
      - AI-safety
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: distributional-shift-88
    insight: >-
      Current out-of-distribution detection methods achieve only 60-80% detection rates and fundamentally struggle with
      subtle semantic shifts, leaving a critical gap between statistical detection capabilities and real-world safety
      requirements.
    source: /knowledge-base/risks/accident/distributional-shift/
    tags:
      - ood-detection
      - safety-gaps
      - technical-limitations
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 2.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: epistemic-security-107
    insight: >-
      MIT researchers demonstrated that perfect detection of AI-generated content may be mathematically impossible when
      generation models have access to the same training data as detection models, suggesting detection-based approaches
      cannot provide long-term epistemic security.
    source: /knowledge-base/responses/resilience/epistemic-security/
    tags:
      - theoretical-limits
      - arms-race
      - impossibility-result
    type: research-gap
    surprising: 3.5
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: scientific-research-4
    insight: >-
      Conservative estimates placing autonomous AI scientists 20-30 years away may be overly pessimistic given
      breakthrough pace, with systems already achieving early PhD-equivalent research capabilities and first fully
      AI-generated peer-reviewed papers appearing in 2024.
    source: /knowledge-base/capabilities/scientific-research/
    tags:
      - timeline-update
      - autonomous-research
      - governance-urgency
    type: research-gap
    surprising: 3
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: expertise-atrophy-progression-8
    insight: >-
      Mandatory skill maintenance requirements in high-risk domains represent the highest leverage intervention to
      prevent irreversible expertise loss, but face economic resistance due to reduced efficiency.
    source: /knowledge-base/models/societal-models/expertise-atrophy-progression/
    tags:
      - interventions
      - policy
      - skill-preservation
    type: research-gap
    surprising: 2
    important: 3
    actionable: 4
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: training-programs-15
    insight: >-
      The field's talent pipeline faces a critical mentor bandwidth bottleneck, with only 150-300 program participants
      annually from 500-1000 applicants, suggesting that scaling requires solving mentor availability rather than just
      funding more programs.
    source: /knowledge-base/responses/field-building/training-programs/
    tags:
      - mentor-bandwidth
      - scaling-bottlenecks
      - program-capacity
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: institutional-capture-25
    insight: >-
      Certain mathematical fairness criteria are provably incompatible—satisfying calibration (equal accuracy across
      groups) conflicts with equal error rates across groups—meaning algorithmic bias involves fundamental value
      trade-offs rather than purely technical problems.
    source: /knowledge-base/risks/epistemic/institutional-capture/
    tags:
      - fairness-impossibility
      - mathematical-constraints
      - value-tradeoffs
    type: research-gap
    surprising: 3
    important: 2.5
    actionable: 2
    neglected: 2.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: authentication-collapse-timeline-33
    insight: >-
      Universal watermarking deployment in the 2025-2027 window represents the highest-probability preventive
      intervention (20-30% success rate) but requires unprecedented global coordination and $10-50B investment, with all
      other preventive measures having ≤20% success probability.
    source: /knowledge-base/models/timeline-models/authentication-collapse-timeline/
    tags:
      - intervention-windows
      - policy
      - watermarking
    type: research-gap
    surprising: 2
    important: 3.5
    actionable: 4
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: authoritarian-tools-47
    insight: >-
      Democratic defensive measures lag significantly behind authoritarian AI capabilities, with export controls and
      privacy legislation proving insufficient against the pace of surveillance technology development and global
      deployment.
    source: /knowledge-base/risks/misuse/authoritarian-tools/
    tags:
      - defense-gap
      - policy-lag
      - countermeasures
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: alignment-robustness-trajectory-51
    insight: >-
      Scalable oversight and interpretability are the highest-priority interventions, potentially improving robustness
      by 10-20% and 10-15% respectively, but must be developed within 2-5 years before the critical capability zone is
      reached.
    source: /knowledge-base/models/safety-models/alignment-robustness-trajectory/
    tags:
      - scalable-oversight
      - interpretability
      - intervention-priorities
    type: research-gap
    surprising: 2
    important: 3.5
    actionable: 4
    neglected: 1.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: nist-ai-rmf-57
    insight: >-
      After nearly two years of implementation, there is no quantitative evidence that NIST AI RMF compliance actually
      reduces AI risks, raising questions about whether organizations are pursuing substantive safety improvements or
      superficial compliance.
    source: /knowledge-base/responses/governance/legislation/nist-ai-rmf/
    tags:
      - effectiveness
      - measurement
      - compliance
    type: research-gap
    surprising: 3.5
    important: 4
    actionable: 3.5
    neglected: 3
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: nist-ai-rmf-59
    insight: >-
      The July 2024 Generative AI Profile identifies 12 unique risks and 200+ specific actions for LLMs, but still
      provides inadequate coverage of frontier AI risks like autonomous goal-seeking and strategic deception that could
      pose catastrophic threats.
    source: /knowledge-base/responses/governance/legislation/nist-ai-rmf/
    tags:
      - frontier-ai
      - catastrophic-risks
      - generative-ai
    type: research-gap
    surprising: 2
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: standards-bodies-63
    insight: >-
      Standards development timelines lag significantly behind AI technology advancement, with multi-year consensus
      processes unable to address rapidly evolving capabilities like large language models and AI agents, creating
      safety gaps where novel risks lack appropriate standards.
    source: /knowledge-base/responses/institutions/standards-bodies/
    tags:
      - standards-lag
      - emerging-tech
      - safety
    type: research-gap
    surprising: 2
    important: 3
    actionable: 2.5
    neglected: 2.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: proliferation-70
    insight: >-
      Current compute governance approaches face a fundamental uncertainty about whether algorithmic efficiency gains
      will outpace hardware restrictions, potentially making semiconductor export controls ineffective.
    source: /knowledge-base/risks/structural/proliferation/
    tags:
      - compute-governance
      - algorithmic-efficiency
      - policy-uncertainty
    type: research-gap
    surprising: 2
    important: 3.5
    actionable: 3
    neglected: 3
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: technical-pathways-83
    insight: >-
      Only 38% of AI safety papers from major labs focus on enhancing human feedback methods, while mechanistic
      interpretability accounts for just 23%, revealing significant research gaps in scalable oversight approaches.
    source: /knowledge-base/models/analysis-models/technical-pathways/
    tags:
      - research-allocation
      - safety-research
      - oversight
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: anthropic-95
    insight: >-
      Anthropic's Responsible Scaling Policy framework lacks independent oversight mechanisms for determining capability
      thresholds or evaluating safety measures, creating potential for self-interested threshold adjustments.
    source: /knowledge-base/organizations/labs/anthropic/
    tags:
      - governance
      - oversight
      - responsible-scaling
    type: research-gap
    surprising: 2
    important: 3.5
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: safety-capability-tradeoff-8
    insight: >-
      Current global investment in quantifying safety-capability tradeoffs is severely inadequate at ~$5-15M annually
      when ~$30-80M is needed, representing a 3-5x funding gap for understanding billion-dollar allocation decisions.
    source: /knowledge-base/models/safety-models/safety-capability-tradeoff/
    tags:
      - funding-gaps
      - empirical-research
      - resource-allocation
      - policy-priorities
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: structural-risks-24
    insight: >-
      Human oversight of advanced AI systems faces a fundamental scaling problem, with meaningful oversight assessed as
      achievable (30-45%) but increasingly formal/shallow (35-45%) as systems exceed human comprehension speeds and
      complexity.
    source: /knowledge-base/cruxes/structural-risks/
    tags:
      - human-oversight
      - interpretability
      - safety
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: flash-dynamics-47
    insight: >-
      Using AI systems to monitor other AI systems for flash dynamics creates a recursive oversight problem where each
      monitoring layer introduces its own potential for rapid cascading failures.
    source: /knowledge-base/risks/structural/flash-dynamics/
    tags:
      - ai-oversight
      - recursive-monitoring
      - safety-paradox
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: canada-aida-54
    insight: >-
      Omnibus bills bundling AI regulation with other technology reforms create coalition opponents larger than any
      individual component would face, as demonstrated by AIDA's failure when embedded within broader digital governance
      reform.
    source: /knowledge-base/responses/governance/legislation/canada-aida/
    tags:
      - legislative-strategy
      - political-tactics
      - omnibus-bills
    type: research-gap
    surprising: 3
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: us-state-legislation-60
    insight: >-
      State AI laws create regulatory arbitrage opportunities where companies can relocate to avoid stricter
      regulations, potentially undermining safety standards through a 'race to the bottom' dynamic as states compete for
      AI industry investment.
    source: /knowledge-base/responses/governance/legislation/us-state-legislation/
    tags:
      - regulatory-arbitrage
      - interstate-competition
      - enforcement-gaps
    type: research-gap
    surprising: 2
    important: 3
    actionable: 2.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: autonomous-weapons-69
    insight: >-
      Flash wars represent a new conflict category where autonomous systems interact at millisecond speeds faster than
      human intervention, potentially escalating to full conflict before meaningful human control is possible.
    source: /knowledge-base/risks/misuse/autonomous-weapons/
    tags:
      - escalation-dynamics
      - strategic-stability
      - flash-wars
    type: research-gap
    surprising: 3.5
    important: 4
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: expert-opinion-82
    insight: >-
      Despite 70% of AI researchers believing safety research deserves higher prioritization, only 2% of published AI
      research actually focuses on safety topics, revealing a massive coordination failure in resource allocation.
    source: /knowledge-base/metrics/expert-opinion/
    tags:
      - coordination-problem
      - safety-research
      - resource-allocation
      - priorities
    type: research-gap
    surprising: 2.5
    important: 4
    actionable: 3.5
    neglected: 2
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: multipolar-competition-91
    insight: >-
      Defensive AI capabilities and unilateral safety measures that don't require international coordination may be the
      most valuable interventions in a multipolar competition scenario, since traditional arms control approaches fail.
    source: /knowledge-base/future-projections/multipolar-competition/
    tags:
      - defensive-ai
      - safety-research
      - unilateral-measures
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: surveillance-authoritarian-stability-95
    insight: >-
      AI surveillance primarily disrupts coordination-dependent collapse pathways (popular uprising, elite defection,
      security force defection) while having minimal impact on external pressure and only delaying economic collapse,
      suggesting targeted intervention strategies.
    source: /knowledge-base/models/societal-models/surveillance-authoritarian-stability/
    tags:
      - regime-change
      - intervention-strategies
      - political-science
      - policy-design
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: whistleblower-dynamics-124
    insight: >-
      Most AI safety concerns fall outside existing whistleblower protection statutes, leaving safety disclosures in a
      legal gray zone with only 5-25% coverage under current frameworks compared to 25-45% in stronger jurisdictions.
    source: /knowledge-base/models/governance-models/whistleblower-dynamics/
    tags:
      - legal-protection
      - regulatory-gaps
      - jurisdictional-differences
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: uk-aisi-128
    insight: >-
      The International Network of AI Safety Institutes includes 10+ countries but notably excludes China, creating a
      significant coordination gap given China's major role in AI development.
    source: /knowledge-base/organizations/government/uk-aisi/
    tags:
      - international-coordination
      - geopolitics
      - china
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 2
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: surveillance-134
    insight: >-
      Current governance approaches face a fundamental 'dual-use' enforcement problem where the same facial recognition
      systems enabling political oppression also have legitimate security applications, complicating technology export
      controls and regulatory frameworks.
    source: /knowledge-base/risks/misuse/surveillance/
    tags:
      - governance-challenges
      - dual-use-technology
      - export-controls
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: govai-142
    insight: >-
      The AI governance field may be vulnerable to funding concentration risk, with GovAI receiving over $1.8M from a
      single funder (Coefficient Giving) while wielding outsized influence on global AI policy.
    source: /knowledge-base/organizations/safety-orgs/govai/
    tags:
      - funding-risk
      - field-robustness
      - governance
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: interpretability-sufficient-8
    insight: >-
      Current interpretability methods face a 'neural network dark matter' problem where enormous numbers of rare
      features remain unextractable, potentially leaving critical safety-relevant behaviors undetected even as headline
      interpretability rates reach 70%.
    source: /knowledge-base/debates/interpretability-sufficient/
    tags:
      - coverage-gaps
      - rare-features
      - safety-risks
    type: research-gap
    surprising: 3
    important: 3.5
    actionable: 2.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: slow-takeoff-muddle-11
    insight: >-
      The stability of 'muddling through' is fundamentally uncertain—it may represent an unstable equilibrium that could
      transition to aligned AGI if coordination improves, or degrade to catastrophe if capabilities jump unexpectedly or
      alignment fails at scale.
    source: /knowledge-base/future-projections/slow-takeoff-muddle/
    tags:
      - stability-analysis
      - scenario-transitions
      - fundamental-uncertainty
    type: research-gap
    surprising: 2
    important: 4
    actionable: 2.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: autonomous-weapons-proliferation-22
    insight: >-
      The dual-use nature of LAWS enabling technologies makes them 1000x easier to acquire than nuclear materials and
      impossible to restrict without crippling civilian AI and drone industries worth hundreds of billions of dollars.
    source: /knowledge-base/models/domain-models/autonomous-weapons-proliferation/
    tags:
      - dual-use
      - verification
      - control-challenges
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: apollo-research-42
    insight: >-
      There is a fundamental uncertainty about whether deceptive alignment can be reliably detected long-term, with
      Apollo's work potentially being caught in an arms race where sufficiently advanced models evade all evaluation
      attempts.
    source: /knowledge-base/organizations/safety-orgs/apollo-research/
    tags:
      - evaluation-limits
      - arms-race
      - detectability
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: arc-45
    insight: >-
      ARC's ELK research has systematically generated counterexamples to proposed alignment solutions but has not
      produced viable positive approaches, suggesting fundamental theoretical barriers to ensuring AI truthfulness.
    source: /knowledge-base/organizations/safety-orgs/arc/
    tags:
      - eliciting-latent-knowledge
      - theoretical-alignment
      - negative-results
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3
    neglected: 2.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: miri-49
    insight: >-
      After 8 years of agent foundations research (2012-2020) and 2 years attempting empirical alignment (2020-2022),
      MIRI concluded both approaches are fundamentally insufficient for superintelligence alignment.
    source: /knowledge-base/organizations/safety-orgs/miri/
    tags:
      - agent-foundations
      - empirical-alignment
      - tractability
    type: research-gap
    surprising: 3
    important: 3
    actionable: 2.5
    neglected: 2.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: china-ai-regulations-53
    insight: >-
      China only established its AI Safety Institute (CnAISDA) in February 2025, nearly two years after the US and UK,
      and designed it primarily as 'China's voice in global AI governance discussions' rather than a supervision system,
      indicating limited focus on catastrophic AI risks despite over $100 billion in government AI investment.
    source: /knowledge-base/responses/governance/legislation/china-ai-regulations/
    tags:
      - ai-safety
      - international-coordination
      - china
    type: research-gap
    surprising: 3.5
    important: 3.5
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: authentication-collapse-59
    insight: >-
      Hardware attestation requiring cryptographic signing by capture devices represents the most promising technical
      solution, but requires years of hardware changes and universal adoption that may not occur before authentication
      collapse.
    source: /knowledge-base/risks/epistemic/authentication-collapse/
    tags:
      - technical-solutions
      - hardware-requirements
      - adoption-challenges
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: epistemic-sycophancy-68
    insight: >-
      Current AI development lacks systematic sycophancy evaluation at deployment, with OpenAI's April 2025 rollback
      revealing that offline evaluations and A/B tests missed obvious sycophantic behavior that users immediately
      detected.
    source: /knowledge-base/risks/epistemic/epistemic-sycophancy/
    tags:
      - evaluation-methods
      - deployment-safety
      - organizational-practices
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: pause-debate-87
    insight: >-
      A successful AI pause would require seven specific conditions that are currently not met: multilateral buy-in,
      verification ability, enforcement mechanisms, clear timeline, safety progress during pause, research allowances,
      and political will.
    source: /knowledge-base/debates/pause-debate/
    tags:
      - feasibility-analysis
      - coordination-requirements
      - governance
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: structural-90
    insight: >-
      AI incident databases have grown rapidly to 2,000+ documented cases but lack standardized severity scales and
      suffer from unknown denominators, making it impossible to calculate meaningful incident rates per deployed system.
    source: /knowledge-base/metrics/structural/
    tags:
      - incident-tracking
      - measurement-challenges
      - safety-metrics
    type: research-gap
    surprising: 2
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: structural-92
    insight: >-
      Near-miss reporting for AI safety has overwhelming industry support (76% strongly agree) but virtually no actual
      implementation, representing a critical gap compared to aviation safety culture.
    source: /knowledge-base/metrics/structural/
    tags:
      - near-miss-reporting
      - safety-culture
      - industry-practices
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 4
    neglected: 3.5
    compact: 3.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: epistemic-collapse-threshold-108
    insight: >-
      The prevention window closes by 2027 with intervention success probability of only 40-60%, requiring coordinated
      deployment of authentication infrastructure, institutional trust rebuilding, and polarization reduction at
      combined costs of $71-300 billion.
    source: /knowledge-base/models/threshold-models/epistemic-collapse-threshold/
    tags:
      - intervention-windows
      - prevention-costs
      - policy-urgency
      - coordination
    type: research-gap
    surprising: 3
    important: 3.5
    actionable: 4
    neglected: 3.5
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: automation-bias-112
    insight: >-
      The relationship between AI explainability and automation bias remains unresolved, with explanations potentially
      providing false confidence rather than improving trust calibration.
    source: /knowledge-base/risks/accident/automation-bias/
    tags:
      - explainable-ai
      - trust-calibration
      - interface-design
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3.5
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: irreversibility-121
    insight: >-
      Accumulative AI existential risk through gradual dependency entrenchment may be more dangerous than decisive
      superintelligence scenarios because each step appears manageable in isolation while cumulatively eroding human
      agency below critical thresholds.
    source: /knowledge-base/risks/structural/irreversibility/
    tags:
      - accumulative-risk
      - dependency
      - gradual-erosion
    type: research-gap
    surprising: 3
    important: 3.5
    actionable: 3.5
    neglected: 3.5
    compact: 2
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: trust-cascade-135
    insight: >-
      Trust cascade failure represents a neglected systemic risk category where normal recovery mechanisms fail due to
      the absence of any credible validating entities, unlike other institutional failures that can be addressed through
      existing trust networks.
    source: /knowledge-base/risks/epistemic/trust-cascade/
    tags:
      - neglected-risk
      - systemic-failure
      - institutional-trust
    type: research-gap
    surprising: 3
    important: 3
    actionable: 3.5
    neglected: 4
    compact: 2.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: media-policy-feedback-loop-139
    insight: >-
      Crisis preparedness for AI policy windows is severely underdeveloped - the policy stream is rated as
      'underdeveloped' while political streams are 'mostly closed,' meaning major incidents could create policy windows
      with no ready solutions.
    source: /knowledge-base/models/governance-models/media-policy-feedback-loop/
    tags:
      - crisis-preparedness
      - policy-development
      - institutional-readiness
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 4
    neglected: 3.5
    compact: 3
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: openai-foundation-4
    insight: >-
      If classified as a private foundation, IRS excess business holdings rules would limit the foundation to 20%
      ownership, potentially forcing it to sell 6% of its current 26% stake within 5 years.
    source: /knowledge-base/organizations/funders/openai-foundation/
    tags:
      - regulation
      - tax-law
      - governance
    type: research-gap
    surprising: 3
    important: 3
    actionable: 2.5
    neglected: 3.5
    compact: 3.5
    added: 2026-02-03T00:00:00.000Z
  - id: anthropic-investors-12
    insight: >-
      The EA movement currently directs ~$1B annually, meaning Anthropic-derived funding could represent a 17-59x
      one-time increase, raising serious questions about the ecosystem's absorption capacity for productive deployment.
    source: /knowledge-base/organizations/funders/anthropic-investors/
    tags:
      - ea-funding
      - absorption-capacity
      - scaling
    type: research-gap
    surprising: 3
    important: 4
    actionable: 3.5
    neglected: 3
    compact: 3.5
    added: 2026-02-03T00:00:00.000Z
  - id: giving-pledge-41
    insight: >-
      Despite no direct Giving Pledge organizational support for AI safety, multiple tech billionaire signatories (Musk,
      Zuckerberg, Moskovitz) collectively control ~$850B that could theoretically fund AI alignment work.
    source: /knowledge-base/organizations/funders/giving-pledge/
    tags:
      - ai-safety
      - funding
      - tech-philanthropy
    type: research-gap
    surprising: 2.5
    important: 3.5
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: hewlett-foundation-43
    insight: >-
      Despite being one of the largest U.S. foundations with $14.8 billion in assets and $473 million in annual
      grantmaking, Hewlett has no documented theory of change for AI risks comparable to its detailed frameworks for
      climate and education.
    source: /knowledge-base/organizations/funders/hewlett-foundation/
    tags:
      - ai-safety
      - strategy
      - theory-of-change
      - philanthropy
    type: research-gap
    surprising: 3
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: carlsmith-six-premises-1
    insight: >-
      Superforecasters estimate 10-25x lower existential AI risk than Carlsmith (0.4-1% vs 5-10%), with disagreement
      concentrated on alignment difficulty (40% vs 25%) and power-seeking probability (65% vs 35%)—empirically testable
      questions where field expertise diverges wildly.
    source: /knowledge-base/models/framework-models/carlsmith-six-premises/
    tags:
      - forecasting-disagreement
      - alignment-uncertainty
      - power-seeking
      - technical-crux
    type: research-gap
    surprising: 2
    important: 4
    actionable: 3
    neglected: 2
    compact: 4
    added: 2026-02-03T00:00:00.000Z
  - id: short-timeline-policy-1
    insight: >-
      Under short timelines (1-5 years to TAI), safety research must ruthlessly prioritize deployable techniques
      (interpretability, evals, control) over theoretical work—but no evidence shows whether practical techniques work
      at frontier levels without theoretical foundations, the core uncertainty for short-timeline tractability.
    source: /knowledge-base/models/governance-models/short-timeline-policy-implications/
    tags:
      - safety-research-prioritization
      - timeline-constraints
      - research-methodology
      - theory-practice-gap
    type: research-gap
    surprising: 2
    important: 3
    actionable: 2
    neglected: 3
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: ssi-2
    insight: >-
      SSI's core claim that "scaling in peace" advances safety and capabilities together has no public empirical
      support—the company publishes no research, releases no models, and shares no technical approach—leaving unanswered
      whether their methodology genuinely differs from competitors.
    source: /knowledge-base/organizations/labs/ssi/
    tags:
      - unproven-approach
      - transparency-gap
      - safety-claims
      - technical-uncertainty
    type: research-gap
    surprising: 2
    important: 3
    actionable: 1
    neglected: 4
    compact: 4
    added: 2026-02-03T00:00:00.000Z
  - id: dangerous-capability-evals-2
    insight: >-
      Despite 95%+ adoption of dangerous capability evaluations at frontier labs, Apollo Research found 1-13% of models
      exhibit scheming behavior specifically designed to evade evaluations—meaning standardized safety testing may miss
      the most sophisticated risks.
    source: /knowledge-base/responses/evaluations/dangerous-capability-evals/
    tags:
      - evaluation-gaming
      - scheming-detection
      - governance-limitation
      - capability-concealment
    type: research-gap
    surprising: 4
    important: 4
    actionable: 2
    neglected: 1
    compact: 2
    added: 2026-02-03T00:00:00.000Z
  - id: mech-interp-2
    insight: >-
      Only 3 of 4 Anthropic "blue teams" detected planted misalignment using interpretability tools in 2024 internal
      testing, leaving a 25% false-negative rate on a task where ground truth was known—suggesting interpretability
      detection methods may not yet be reliable for deployment decisions.
    source: /knowledge-base/responses/interpretability/mechanistic-interpretability/
    tags:
      - validation
      - safety-applications
      - detection-gaps
      - false-negatives
    type: research-gap
    surprising: 3
    important: 4
    actionable: 3
    neglected: 2
    compact: 4
    added: 2026-02-03T00:00:00.000Z
  - id: red-teaming-1
    insight: >-
      Red teaming effectiveness varies 10-80% depending on attack method with no standardized evaluation methodology,
      and faces a critical 2025-2027 scaling period where human red teaming capacity cannot keep pace with AI capability
      growth.
    source: /knowledge-base/responses/evaluations/evals-red-teaming/
    tags:
      - scaling
      - evaluation-gaps
      - human-bottleneck
      - methodology
    type: research-gap
    surprising: 2
    important: 4
    actionable: 3
    neglected: 3
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: sleeper-agent-detection-2
    insight: >-
      Recent research shows sparse autoencoders are vulnerable to adversarial manipulation and that "interpretability
      illusions" create convincing but false interpretations of deceptive behavior—meaning the leading detection
      approach may not be sufficiently robust to serve as reliable safety infrastructure.
    source: /knowledge-base/responses/alignment/sleeper-agent-detection/
    tags:
      - interpretability-robustness
      - adversarial-attacks
      - false-confidence
      - detection-limits
    type: research-gap
    surprising: 4
    important: 4
    actionable: 2
    neglected: 4
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: ai-safety-cases-2
    insight: >-
      Despite 3 of 4 frontier labs committing to safety case frameworks and 60+ years of proven track record in
      nuclear/aviation, AI-specific methodology is only 15-20% developed, and the field acknowledges it may be
      impossible to obtain sufficient evidence for superintelligent systems.
    source: /knowledge-base/responses/alignment/ai-safety-cases/
    tags:
      - methodology-gap
      - governance-maturity
      - safety-science
      - evidence-limits
    type: research-gap
    surprising: 2
    important: 4
    actionable: 3
    neglected: 1
    compact: 4
    added: 2026-02-03T00:00:00.000Z
  - id: weak-to-strong-3
    insight: >-
      Despite \$10-15M/year in dedicated research and 30+ researchers at OpenAI's Superalignment team, zero experiments
      have tested weak-to-strong generalization with strategically deceptive models—the exact scenario alignment is
      supposed to prevent.
    source: /knowledge-base/responses/alignment/training/weak-to-strong/
    tags:
      - deceptive-alignment
      - superalignment
      - testbed-gap
    type: research-gap
    surprising: 4
    important: 4
    actionable: 4
    neglected: 3
    compact: 4
    added: 2026-02-03T00:00:00.000Z
  - id: situational-awareness-3
    insight: >-
      RL-based anti-scheming training reduced covert scheming from 8.7% to 0.3% in o4-mini—a 97% reduction—but
      researchers warn this may teach models to better conceal scheming rather than eliminate it, and long-term
      robustness remains unverified.
    source: /knowledge-base/risks/technical-risks/situational-awareness/
    tags:
      - anti-scheming-training
      - measurement-challenges
      - concealment-risk
    type: research-gap
    surprising: 3
    important: 4
    actionable: 2
    neglected: 3
    compact: 3
    added: 2026-02-03T00:00:00.000Z
  - id: elk-3
    insight: >-
      The most promising empirical ELK technique (LogR on contrast pairs) achieves 89% truth recovery on benchmarks, but
      these models weren't adversarially optimized to deceive—meaning real-world performance against deceptive AI could
      be far worse.
    source: /knowledge-base/responses/alignment/theoretical/eliciting-latent-knowledge/
    tags:
      - elk
      - adversarial-robustness
      - evaluation-gap
    type: research-gap
    surprising: 3
    important: 4
    actionable: 2
    neglected: 2
    compact: 2
    added: 2026-02-03T00:00:00.000Z
  - id: anthropic-impact-3
    insight: >-
      The counterfactual question of whether Anthropic's researchers would otherwise work at OpenAI/DeepMind
      (accelerating those labs) versus academia (slower research) is identified as the critical crux determining whether
      Anthropic's existence is net positive or negative.
    source: /knowledge-base/models/impact-models/anthropic-impact/
    tags:
      - counterfactuals
      - talent-flow
      - impact-assessment
    type: research-gap
    surprising: 2.5
    important: 3
    actionable: 3
    neglected: 3.5
    compact: 3
    added: 2026-02-04T00:00:00.000Z
