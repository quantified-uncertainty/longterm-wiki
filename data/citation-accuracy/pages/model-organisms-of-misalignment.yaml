- pageId: model-organisms-of-misalignment
  footnote: 56
  claimText: '**Creating dangerous models**: Intentionally building more hazardous AIs that could act catastrophically if deployed presents risks, potentially givin...'
  sourceTitle: What we didn&#x27;t cover in our October 2024 AI Alignment course
  url: https://blog.bluedot.org/p/not-covered-2410-alignment
  verdict: unsupported
  score: 0
  issues: The source does not discuss the risks of intentionally building more hazardous AIs, premature situational awareness, or the intensification of concerns as organisms become more coherent and capable.
  difficulty: hard
  checkedAt: '2026-02-20 16:31:50'
