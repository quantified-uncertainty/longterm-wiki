- pageId: mats
  footnote: 70
  claimText: 'Alumni feedback highlights specific challenges reported by MATS participants:'
  sourceTitle: MATS Alumni Impact Analysis — EA Forum
  url: https://forum.effectivealtruism.org/posts/kJA9q3SGycx6TXjcF/mats-alumni-impact-analysis
  verdict: unsupported
  score: 0
  issues: The source does not contain any alumni feedback highlighting specific challenges reported by MATS participants.
  difficulty: easy
  checkedAt: '2026-02-20 08:14:04'
- pageId: mats
  footnote: 60
  claimText: MATS receives grants from partner organizations to support its fellowship program. Financial support for scholars is coordinated through partner organ...
  sourceTitle: ML Alignment & Theory Scholars Funding | Complete Analysis | Extruct AI
  url: https://www.extruct.ai/hub/matsprogram-org-funding/
  verdict: unsupported
  score: 0.2
  issues: 'unsupported: The source does not mention that financial support for scholars is coordinated through partner organizations rather than directly by MATS.'
  difficulty: easy
  checkedAt: '2026-02-20 08:13:53'
- pageId: mats
  footnote: 17
  claimText: The program's core mission from inception was to train talented individuals for AI alignment research by addressing risks from unaligned AI through me...
  sourceTitle: MATS Summer 2023 Retrospective — LessWrong
  url: https://www.lesswrong.com/posts/zwf68YaySvXhWYCdh/mats-summer-2023-retrospective
  verdict: inaccurate
  score: 0.6
  issues: |-
    The source does not explicitly state that the program's core mission from inception was to train talented individuals for AI alignment research by addressing risks from unaligned AI through mentorship, training, logistics, and community access. It does mention that the program is for emerging AI safety researchers.
    The source does not explicitly state that the program evolved into an independent organization. It does mention that the program maintains hubs in both Berkeley and London.
  difficulty: medium
  checkedAt: '2026-02-20 08:12:52'
- pageId: mats
  footnote: 12
  claimText: Since its founding, MATS has trained over 446 researchers. The program has generated over 160 research publications with more than 9,000 citations, ad...
  sourceTitle: MATS | Effective Altruism
  url: https://www.effectivealtruism.org/opportunities/recdsVgelkD2qXd3P
  verdict: inaccurate
  score: 0.75
  issues: |-
    unsupported: Number of researchers trained (446)
    unsupported: Number of research publications (160)
    unsupported: Number of citations (9,000)
    unsupported: Advancing agendas in mechanistic interpretability, sparse feature analysis, activation engineering, and AI safety evaluation
    minor_issues: Google DeepMind instead of DeepMind
    unsupported: Founded new AI safety organizations like Apollo Research
  difficulty: medium
  checkedAt: '2026-02-20 08:12:39'
