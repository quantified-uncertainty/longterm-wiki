---
numericId: E837
title: Future Projections
description: Scenarios for how AI development might unfold
sidebar:
  label: Overview
  order: 0
clusters:
  - ai-safety
entityType: overview
---
import {EntityLink} from '@components/wiki';


## Overview

This section explores different scenarios for how AI development might unfold. Scenario analysis helps prepare for multiple futures rather than betting on a single prediction.

## Scenarios

### <EntityLink id="aligned-agi">Aligned AGI</EntityLink>
AI development succeeds with safety:
- Alignment research proves sufficient
- Safety measures are implemented effectively
- Humanity retains meaningful control
- *Probability estimate: 15-35%*

### <EntityLink id="misaligned-catastrophe">Misaligned Catastrophe</EntityLink>
Misaligned AI causes major harm:
- Alignment fails despite effort
- Deceptive or <EntityLink id="power-seeking">power-seeking AI</EntityLink> emerges
- Ranges from setback to <EntityLink id="existential-catastrophe">existential catastrophe</EntityLink>
- *Probability estimate: 5-25%*

### <EntityLink id="pause-and-redirect">Pause and Redirect</EntityLink>
Development is deliberately slowed:
- Government intervention or lab coordination
- More time bought for safety research
- Possible through regulation or crisis response
- *Probability estimate: 10-25%*

### <EntityLink id="slow-takeoff-muddle">Slow Takeoff Muddle</EntityLink>
Gradual change without clear transition:
- No single "AGI moment"
- Ongoing adaptation to incremental progress
- Neither utopia nor catastrophe
- *Probability estimate: 30-50%*

### <EntityLink id="multipolar-competition">Multipolar Competition</EntityLink>
Multiple powerful AI systems compete:
- No single winner-take-all
- International competition dynamics
- Complex coordination challenges
- *Probability estimate: 25-45%*

## Scenario Analysis Uses

Scenarios help:
- **Test robustness** - Which interventions help across scenarios?
- **Identify early signs** - What would indicate we're heading toward each?
- **Prepare contingencies** - What should we do if each materializes?
- **Communicate** - Make abstract risks concrete

## Key Uncertainties

The scenario we end up in depends heavily on:
- AGI timelines (sooner favors speed, later favors preparation)
- Alignment difficulty (harder favors pessimistic scenarios)
- Coordination success (better coordination enables pause/redirect)
- First-mover dynamics (monopoly vs. multipolar outcomes)
