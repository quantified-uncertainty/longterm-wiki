---
title: Dan Hendrycks
description: Director of CAIS, focuses on catastrophic AI risk reduction
sidebar:
  order: 15
quality: 19
llmSummary: Biographical overview of Dan Hendrycks, CAIS director who coordinated the May 2023 AI risk statement signed by major AI researchers. Covers his technical work on benchmarks (MMLU, ETHICS), robustness research, and institution-building efforts, emphasizing his focus on catastrophic AI risk as a global priority.
lastEdited: "2026-02-17"
readerImportance: 87
researchImportance: 39.5
ratings:
  novelty: 1.5
  rigor: 2
  actionability: 1
  completeness: 4
clusters: ["ai-safety","governance"]
entityType: person
---
import {DataInfoBox, DataExternalLinks, EntityLink} from '@components/wiki';

<DataExternalLinks pageId="dan-hendrycks" />

<DataInfoBox entityId="E89" />

## Background

Dan Hendrycks is the director of the <EntityLink id="cais">Center for AI Safety</EntityLink> (CAIS) and a researcher in machine learning safety and robustness. He completed his PhD in Computer Science at UC Berkeley, where he subsequently held a postdoctoral position before founding CAIS.

His research has focused on several areas within machine learning:
- Robustness of neural networks to distribution shift
- Out-of-distribution detection and uncertainty quantification
- Development of benchmarks for evaluating language models
- <EntityLink id="adversarial-robustness">Adversarial robustness</EntityLink> and natural adversarial examples

Through CAIS, Hendrycks has worked to connect technical research with policy discussions around AI risk, particularly focusing on scenarios involving catastrophic outcomes.

## Center for AI Safety

Hendrycks founded the <EntityLink id="cais">Center for AI Safety</EntityLink> as a nonprofit research organization. CAIS describes its mission as reducing societal-scale risks from artificial intelligence through research, field-building, and advocacy work.

The organization's activities include:
- Conducting technical <EntityLink id="safety-research">safety research</EntityLink> on topics such as robustness and evaluation methods
- Educational programs, including the ML Safety course curriculum
- Policy-oriented work on <EntityLink id="compute-governance">compute governance</EntityLink> and hardware-level interventions
- Coordination efforts within the AI safety research community

CAIS has served as an institutional platform for Hendrycks' work connecting technical researchers with policymakers and coordinating public statements on AI risk.

## Statement on AI Risk (May 2023)

In May 2023, Hendrycks coordinated a public statement that read: "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war." The statement was signed by researchers and executives from multiple AI organizations.

Signatories included <EntityLink id="geoffrey-hinton">Geoffrey Hinton</EntityLink>, <EntityLink id="yoshua-bengio">Yoshua Bengio</EntityLink>, <EntityLink id="sam-altman">Sam Altman</EntityLink> (<EntityLink id="openai">OpenAI</EntityLink>), <EntityLink id="demis-hassabis">Demis Hassabis</EntityLink> (<EntityLink id="deepmind">Google DeepMind</EntityLink>), and <EntityLink id="dario-amodei">Dario Amodei</EntityLink> (<EntityLink id="anthropic">Anthropic</EntityLink>), along with hundreds of other AI researchers.

The statement received coverage in major media outlets and was cited in subsequent policy discussions. Its brevity and high-profile signatories generated public attention to the topic of AI existential risk, though some commentators noted the statement's lack of specificity regarding which risks or interventions were being prioritized.

## Technical Research Contributions

### Benchmarks and Evaluation

Hendrycks has developed several benchmarks used in evaluating language models and AI systems:

**MMLU (Measuring Massive Multitask Language Understanding)**: A benchmark designed to test knowledge across multiple academic and professional domains. The benchmark is used by researchers to evaluate the breadth of capabilities in large language models.

**ETHICS Dataset**: A benchmark for evaluating whether language models can perform tasks related to moral reasoning across different ethical frameworks.

These benchmarks have been adopted by research groups evaluating new language models, providing standardized metrics for comparing systems.

### Robustness and Distribution Shift

Hendrycks' work on robustness has examined how neural networks perform when tested on data that differs from their training distribution. His research has included:

- Methods for detecting when inputs are out-of-distribution relative to training data
- Studies of how models fail when encountering natural variations in data
- Development of datasets containing "natural adversarial examples" that cause model failures without artificial perturbations
- Analysis of calibration in neural network predictions

This work connects to broader questions in <EntityLink id="technical-research">technical AI safety research</EntityLink> about how to ensure systems behave reliably in novel situations.

## Risk Assessment and Strategic Focus

Hendrycks has publicly positioned AI risk as comparable in priority to other large-scale threats. The May 2023 statement he coordinated explicitly equated mitigating AI extinction risk with addressing pandemics and nuclear war.

His activities through CAIS reflect a strategic approach combining multiple intervention points:
- Technical research aimed at improving system safety and evaluation
- Policy engagement with government bodies and international organizations
- Public communication to broaden awareness of AI risk concerns
- <EntityLink id="compute-governance">Compute governance</EntityLink> work exploring hardware-level interventions

Hendrycks has argued for attention to catastrophic and existential risk scenarios specifically, distinguishing these from other categories of AI-related harms. This prioritization shapes CAIS's research agenda and advocacy focus.

## CAIS Programs and Activities

### Research

CAIS conducts and supports research in several areas:

**Technical Safety**: Work on robustness, alignment techniques, and evaluation methodologies for AI systems. Research includes both empirical studies of current systems and development of new safety methods.

**<EntityLink id="compute-governance">Compute Governance</EntityLink>**: Investigation of interventions based on the hardware supply chain for AI systems, including tracking of compute resources and potential <EntityLink id="international-coordination">international coordination</EntityLink> mechanisms.

**ML Safety Education**: Development of curriculum materials for teaching machine learning safety concepts, aimed at integrating safety considerations into academic computer science programs.

### Advocacy and Field-Building

CAIS has organized workshops, maintained networks of researchers working on safety-related topics, and engaged with policymakers. The organization has provided testimony and technical input to legislative bodies considering AI regulation, including the U.S. Congress and bodies involved in developing the <EntityLink id="eu-ai-act">EU AI Act</EntityLink>.

Hendrycks has given public presentations and media interviews explaining AI risk concerns to non-technical audiences, contributing to broader public discourse on AI policy.

## Selected Publications

Hendrycks' research has been published in machine learning conferences and journals. Notable papers include:

- Work on out-of-distribution detection methods
- The MMLU benchmark paper on measuring multitask language understanding
- Research on natural adversarial examples showing real-world failure modes
- Papers on calibration and uncertainty in neural networks
- The ETHICS dataset for evaluating moral reasoning in language models

His publication record spans technical machine learning research and safety-focused evaluation work, with papers appearing in venues such as NeurIPS, ICML, and ICLR.

## Evolution of Research Focus

Hendrycks' early research concentrated on standard machine learning problems in robustness and uncertainty quantification. Over time, his work has increasingly emphasized connections between technical research and potential catastrophic risks from AI systems.

The founding of CAIS marked a shift toward more explicit focus on risk reduction as an organizing principle for research priorities. His current work combines continued technical research with institutional efforts to coordinate the AI safety research community and influence policy discussions.

## Perspectives and Debates

Hendrycks' approach to AI safety emphasizes catastrophic and existential risk scenarios as a top priority. This focus has been part of broader debates within the AI research community about how to allocate attention and resources across different categories of AI-related concerns.

Some researchers and commentators have argued that emphasis on long-term or extinction-level risks may divert attention from more immediate harms related to bias, fairness, and misuse of current AI systems. Others have questioned whether the framing of extinction risk is well-supported by available evidence or have noted tensions between different perspectives on AI safety that were unified under the brief May 2023 statement.

Hendrycks has maintained that catastrophic risks warrant prioritization while not dismissing other AI safety concerns. The design of CAIS programs reflects this prioritization, with research and advocacy efforts concentrated on scenarios involving large-scale harm.

The field of AI safety research contains diverse perspectives on which problems are most important, what methods are most promising, and how technical research should relate to policy and governance questions. Hendrycks' work represents one approach within this broader landscape, characterized by focus on measurable technical progress, public communication, and building consensus around specific risk framings.
