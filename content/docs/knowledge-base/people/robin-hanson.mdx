---
title: Robin Hanson
description: American economist known for pioneering prediction markets,
  proposing futarchy governance, and offering skeptical perspectives on AI
  existential risk
importance: 65
lastEdited: "2026-02-01"
update_frequency: 45
sidebar:
  order: 50
ratings:
  novelty: 4
  rigor: 6
  actionability: 3
  completeness: 8
quality: 53
llmSummary: Comprehensive biographical entry on Robin Hanson covering his
  contributions to prediction markets, futarchy governance, and skeptical AI
  safety positions. The page provides valuable context on a significant
  contrarian voice in AI risk discussions but offers limited novel insights
  beyond biographical summary.
clusters: ["epistemics","ai-safety"]
---
import {EntityLink, KeyPeople, KeyQuestions, Section} from '@components/wiki';

## Quick Assessment

| Aspect | Summary |
|--------|---------|
| **Primary Role** | Associate Professor of Economics at George Mason University; Research Associate at <EntityLink id="fhi">Future of Humanity Institute</EntityLink>, Oxford University[^1] |
| **Major Contributions** | Pioneered <EntityLink id="prediction-markets">prediction markets</EntityLink> (1988-present); invented Logarithmic Market Scoring Rule (LMSR); proposed futarchy governance system; originated Great Filter hypothesis[^2] |
| **Key Publications** | *The Age of Em* (2016); *The Elephant in the Brain* (2018, with Kevin Simler); 90+ academic publications with 5,200+ citations[^3] |
| **AI Safety Stance** | Skeptical of fast takeoff and existential risk scenarios; emphasizes gradual transitions allowing for testing and correction[^4] |
| **Controversies** | 2003 DARPA Policy Analysis Market cancellation; 2019 EA Munich disinvitation; characterized as "too far ahead of his time" and "America's creepiest economist"[^5] |

## Key Links

| Source | Link |
|--------|------|
| Official Website | [hanson.gmu.edu](https://hanson.gmu.edu) |
| Wikipedia | [en.wikipedia.org](https://en.wikipedia.org/wiki/Robin_Hanson) |

## Overview

Robin Dale Hanson is an American economist, author, and professor who has made influential contributions to prediction markets, governance theory, and futures studies. Born August 28, 1959, Hanson followed an unconventional path from physics to artificial intelligence research to economics, earning his PhD in social science from Caltech in 1997.[^6] He has since become one of the most distinctive voices in academic economics, known for applying economic reasoning to unconventional domains and challenging mainstream assumptions about technology, governance, and human behavior.

Hanson's signature contribution has been the development and advocacy of prediction markets—systems that allow people to bet on future events to aggregate information and improve forecasting accuracy. Since beginning this work in 1988, he has been the principal architect of numerous pioneering markets, including the first internal corporate prediction market at Xanadu (1990), the first web-based markets at Foresight Exchange (1994), and DARPA's controversial Policy Analysis Market (2001-2003).[^7] His invention of market scoring rules, particularly the Logarithmic Market Scoring Rule (LMSR), provided the mathematical foundation for many modern prediction platforms.[^8]

Beyond prediction markets, Hanson has proposed futarchy—a radical governance system in which democratic values would be set by voting but policies would be selected by betting markets based on which would best achieve those values.[^9] His research spans an unusually wide range, including the Great Filter hypothesis explaining the Fermi paradox, brain emulation economics (*The Age of Em*), hidden motives in human behavior (*The Elephant in the Brain*), and skeptical perspectives on AI existential risk. With over 90 publications, 5,200 citations, 370 invited talks, and 1,007 media mentions, Hanson maintains an active presence through his blog *Overcoming Bias* and frequent public engagement.[^10]

## Education and Early Career

Hanson's intellectual trajectory reflects his interdisciplinary interests. He earned a BS in physics from the University of California, Irvine in 1981, followed by an MS in physics and MA in Conceptual Foundations of Science from the University of Chicago in 1984.[^11] After his master's degrees, Hanson spent nine years as a research programmer working on artificial intelligence, Bayesian statistics, and hypertext publishing at institutions including Lockheed and NASA.[^12]

This period of applied AI research proved formative for Hanson's later economic work. In 1988, while working in industry, he began developing the concept of prediction markets—becoming the first researcher to write in detail about creating and subsidizing markets to improve estimates on diverse topics.[^13] His 1990 work at Xanadu resulted in the first internal corporate prediction market, demonstrating the practical viability of the approach.[^14]

Hanson returned to academia in the early 1990s, earning his PhD in social science from Caltech in 1997. His dissertation, titled *Four Puzzles in Information and Politics: Product Bans, Informed Voters, Social Insurance, and Persistent Disagreement*, reflected his interest in information aggregation and political economy.[^15] After a postdoctoral fellowship as a Robert Wood Johnson Foundation health policy scholar at UC Berkeley, he joined the economics faculty at George Mason University in 1999, where he remains an associate professor.[^16]

## Prediction Markets and Information Aggregation

Hanson's most sustained research program has focused on prediction markets—systems that harness the "wisdom of crowds" by allowing participants to bet on future events. The core insight is that market prices aggregate diverse information and incentivize accuracy, since participants profit from correct predictions and lose money on incorrect ones. This contrasts with traditional expert forecasting, where reputational concerns and social dynamics may distort honest assessment.[^17]

Over three decades, Hanson has been the principal architect of several landmark prediction market projects:

- **Xanadu (1990)**: The first internal corporate prediction market, demonstrating feasibility in organizational settings[^18]
- **Foresight Exchange (1994-ongoing)**: The first web-based prediction markets, making the technology publicly accessible[^19]
- **DARPA Policy Analysis Market (2001-2003)**: Markets for forecasting Middle East developments, controversially canceled amid political backlash[^20]
- **IARPA DAGGRE/SCICAST (2010-2015)**: Sophisticated combinatorial markets funded by intelligence agencies[^21]

Hanson's technical contributions include inventing market scoring rules like LMSR, which are now used in platforms such as Consensus Point (where Hanson served as Chief Scientist), Inkling Markets, and the Washington Stock Exchange.[^22] These scoring rules solve the problem of how to subsidize markets when natural trading volume is insufficient, allowing market makers to provide liquidity while managing risk. Hanson also developed technologies for conditional and combinatorial trading, and studied important issues like market manipulation and insider trading.[^23]

Experimental research has generally validated prediction market accuracy. Hanson's work with collaborators showed that combinatorial markets can effectively aggregate information, and surprisingly, that some forms of manipulation can actually improve market accuracy rather than distort it.[^24] A 2009 paper with Ryan Oprea demonstrated that "A Manipulator Can Aid Prediction Market Accuracy" under certain conditions, challenging conventional wisdom about market integrity.[^25]

## Futarchy and Governance Innovation

Hanson's most radical proposal for applying prediction markets is futarchy—a governance system that would retain democratic control over values while using betting markets to select policies. Under futarchy, citizens would vote on measurable objectives (such as national welfare metrics), and then prediction markets would determine which policies would best achieve those objectives. Policies predicted to improve outcomes would be implemented; those predicted to worsen outcomes would be rejected.[^26]

The futarchy proposal challenges fundamental assumptions about democratic governance. Traditional democracy aggregates preferences through voting, but voters often lack accurate information about policy consequences and face poor incentives for careful analysis. Futarchy aims to separate the "should" questions (values) from the "is" questions (beliefs about consequences), using democratic processes for the former and market mechanisms for the latter. As Hanson has written, the slogan is "vote on values, but bet on beliefs."[^27]

Critics have raised numerous objections to futarchy, including questions about how to specify welfare metrics without gaming, whether markets can handle complex long-term consequences, and whether the system would be vulnerable to manipulation by wealthy actors. Hanson acknowledges these challenges but argues they are not fundamentally different from problems in existing governance systems, and that empirical testing could address many concerns. Despite its theoretical appeal to some economists and forecasting enthusiasts, futarchy has not been adopted by any major government, though some blockchain projects have experimented with related governance mechanisms.[^28]

## The Great Filter and Long-Term Thinking

Beyond prediction markets, Hanson has made influential contributions to thinking about humanity's long-term future. He originated the Great Filter hypothesis, which attempts to explain the Fermi paradox—why we observe no evidence of alien civilizations despite the vast number of potentially habitable planets.[^29] The Great Filter proposes that there must be at least one extremely improbable step in the evolution from simple matter to expanding civilizations, either in our past or our future.

If the Great Filter lies in our past (for example, in the origin of life or the evolution of intelligence), then we are fortunate to have passed it and face relatively good prospects. But if the Great Filter lies ahead, then most civilizations at our stage fail to survive or expand. This has profound implications for existential risk assessment: discovering simple life on Mars or elsewhere would be "bad news," as it would suggest the Filter lies ahead of us rather than behind.[^30]

Hanson has also explored scenarios for humanity's far future, most extensively in his 2016 book *The Age of Em: Work, Love, and Life When Robots Rule the Earth*. The book presents a detailed economic analysis of a world dominated by brain emulations (ems)—digital copies of human minds running on computers. Drawing on economic theory, Hanson models how em societies would function, covering work patterns, social structures, reproduction, and competition in this radically different context. While speculative, the book represents one of the most thorough attempts to think rigorously about post-biological intelligence using economic tools.[^31]

## Hidden Motives and Human Behavior

Hanson's 2018 book *The Elephant in the Brain: Hidden Motives in Everyday Life*, co-authored with Kevin Simler, applies evolutionary psychology and signaling theory to explain human behavior. The central thesis is that much of what we do is driven by hidden motives we are reluctant to acknowledge—particularly status competition, coalition-building, and signaling desirable qualities to others.[^32]

The book examines domains including medicine, education, charity, politics, and religion, arguing that the stated reasons for these institutions often differ from their actual functions. For example, Hanson has long argued that medicine functions partly as a way to "show we care" rather than purely to improve health outcomes, pointing to evidence that marginal health spending often has minimal effects.[^33] Similarly, he views education as substantially about signaling intelligence and conscientiousness rather than purely about learning valuable skills.[^34]

This perspective has proven controversial. Critics argue it is overly cynical, neglects genuine altruism and learning, or lacks sufficient empirical support for strong claims about hidden motives dominating behavior. Defenders appreciate the book's willingness to question conventional narratives and its synthesis of research on self-deception, signaling, and social behavior. The debate reflects broader tensions between explaining behavior through conscious rational choice versus unconscious evolutionary pressures.[^35]

## AI Safety and Existential Risk

Within the AI safety community, Hanson is known for his skepticism toward arguments that advanced AI poses extreme existential risks. He has engaged in extended debates with figures like <EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink> about "fast takeoff" scenarios, in which recursive self-improvement could lead to an intelligence explosion, potentially resulting in human extinction if the AI is misaligned with human values.[^36]

Hanson's skepticism rests on several arguments. First, he expects AI transitions to be gradual, taking roughly a decade rather than occurring suddenly. This timeframe would allow for extensive testing to detect alignment problems before catastrophic failures occur.[^37] Second, he draws analogies to computer security: just as we detect and fix security vulnerabilities through real-world testing, we would observe misalignment in deployed systems before scaling them to existentially dangerous levels.[^38] Third, he argues that current AI systems (like <EntityLink id="language-models">large language models</EntityLink>) differ fundamentally from the full autonomous agents with hidden goals that would be required for deceptive misalignment scenarios.[^39]

In his 2023 post "AI Risk, Again," Hanson articulated what he considers reasonable versus overwrought fears about AI. Reasonable concerns include <EntityLink id="economic-disruption">economic disruption</EntityLink>, value drift over long timescales, or capability mismatches between different AI systems. Extreme scenarios involving humanity-destroying deceptive AI he considers highly improbable, arguing they rest on speculative threat models without supporting evidence from prototypes or analogous systems.[^40]

This position has drawn criticism from AI safety researchers who argue that Hanson underestimates the risks of <EntityLink id="deceptive-alignment">deceptive alignment</EntityLink>, where systems might appear aligned during training and testing but behave differently when deployed. They point to examples like <EntityLink id="mesa-optimization">mesa-optimization</EntityLink>, where learning systems develop internal objectives that differ from their training objectives. Hanson's response is that such risks remain theoretical and that premature regulation could slow beneficial AI development more than it reduces catastrophic risks.[^41]

Hanson's debate with Yudkowsky, published as *The Hanson-Yudkowsky AI-Foom Debate* in 2013, remains one of the most thorough exchanges on fast takeoff scenarios. While Yudkowsky emphasized potential for hyper-exponential growth once AI systems can improve themselves, Hanson argued that historical economic transitions (like the agricultural and industrial revolutions) provide better models, showing faster growth than previous eras but bounded by physical and economic constraints rather than unlimited explosion.[^42]

## Controversies and Criticisms

Hanson's career has been marked by several notable controversies. The most significant was the 2003 cancellation of DARPA's Policy Analysis Market (also called FutureMAP), which Hanson had helped design. The program aimed to use prediction markets to forecast political and military developments in the Middle East. After Senators Ron Wyden and Byron Dorgan criticized the program as a "terrorism futures market," DARPA shut it down amid broader backlash against the Total Information Awareness program.[^43]

Hanson expressed disappointment with the cancellation, arguing that the political controversy stemmed from misunderstanding the program's goals and from guilt-by-association with other controversial DARPA initiatives rather than genuine problems with using markets for geopolitical forecasting. Some analysts have since suggested that prediction markets might have provided valuable information for policymakers, and similar approaches have been explored in classified intelligence contexts.[^44]

A more recent controversy occurred in 2019, when Effective Altruism Munich disinvited Hanson from a conference following backlash to his social media posts and public writings. Critics found some of his framings insensitive or biased, particularly regarding gender dynamics and transactional relationships. The incident sparked debate within the effective altruism community about discourse norms, free speech, and whether organizations should platform controversial speakers.[^45] Defenders argued critics were uncharitable and that excluding heterodox thinkers undermines intellectual diversity, while others maintained that Hanson's framing was genuinely problematic and that avoiding controversy was reasonable for community events.[^46]

Economist Tyler Cowen, while respectful of Hanson's contributions, has articulated fundamental disagreements with his approach. Cowen views Hanson's focus on "overcoming bias" as aesthetically driven rather than pragmatic, arguing that biases are one problem among many (alongside laziness, fear, and diverse preferences) rather than the primary obstacle to better thinking. Cowen also rejects Hanson's clean separation between values and beliefs, his support for privatizing all law, and his tendency toward parsimonious theories that seek single mechanisms to explain complex phenomena.[^47]

These criticisms reflect a broader pattern: Hanson's provocative style and willingness to apply economic reasoning to sensitive topics generates both admirers who value his iconoclasm and critics who find his framings reductive or tone-deaf. Some observers argue that his less central controversial ideas have overshadowed his more valuable contributions to prediction markets and economic analysis.[^48]

## Funding and Institutional Support

Hanson has received significant research support throughout his career, though specific funding amounts are rarely disclosed. His most publicly documented grant came from <EntityLink id="open-philanthropy">Coefficient Giving</EntityLink>, which awarded him \$264,525 over three years to analyze scenarios for AI development, particularly "multipolar" outcomes where AI capabilities accumulate gradually across many systems rather than concentrating in a single superintelligent agent.[^49]

This grant reflected Coefficient Giving's early investment in AI safety research and their interest in supporting diverse perspectives, including those skeptical of fast takeoff scenarios. While much AI safety funding has gone to researchers emphasizing high existential risks, Coefficient Giving recognized value in Hanson's analysis of alternative scenarios and his economic modeling expertise.[^50]

Beyond this grant, Hanson has held research positions at George Mason University and the Future of Humanity Institute at Oxford, served as Chief Scientist at Consensus Point, and received support from organizations including DARPA and IARPA for prediction market projects. His work on health economics received earlier support from the Robert Wood Johnson Foundation during his postdoctoral fellowship at UC Berkeley.[^51]

## Recent Work and Current Focus

Hanson remains intellectually active through his blog *Overcoming Bias*, which has received over 8 million visits, and through frequent podcast appearances and interviews.[^52] Recent themes in his work include:

**Cultural drift**: Hanson argues that modern societies are experiencing maladaptive cultural evolution, with declining fertility rates representing a failure to address cultural changes that work against biological reproduction despite material abundance. He advocates for more "cultural entrepreneurship" to experiment with alternative social arrangements.[^53]

**Science funding mechanisms**: Building on earlier work comparing prizes versus grants, Hanson has proposed using prediction markets to evaluate academic reputations over long time horizons, with historians judging influence decades or centuries later. This would shift incentives away from short-term publication games toward work with lasting value.[^54]

**AI economics**: In a 2024 blog post, Hanson argued that AI as a general purpose technology will follow historical patterns, taking decades rather than years to produce major economic transformations. He criticized mainstream AI hype as ignoring standard economic analysis, pointing to past technology adoption curves for electricity, computers, and other innovations.[^55]

Hanson's work continues to challenge conventional wisdom and provoke debate. Whether discussing fertility decline, innovation policy, prediction markets, or AI trajectories, he applies economic reasoning in service of understanding what is rather than what people wish were true. This commitment to analysis over advocacy—combined with willingness to explore unpopular conclusions—ensures his work remains influential and controversial in equal measure.

## Key Uncertainties

Several important questions remain unresolved regarding Hanson's ideas and their implications:

**Prediction market adoption**: Despite decades of research demonstrating their accuracy, prediction markets remain niche tools rather than mainstream forecasting mechanisms. Whether this reflects genuine limitations (manipulation risks, liquidity problems, regulatory barriers) or merely status quo bias and political resistance remains contested.

**Futarchy feasibility**: No major government has seriously attempted futarchy, leaving its practical viability untested. Whether the theoretical benefits of separating values from beliefs can overcome implementation challenges—defining metrics, preventing gaming, maintaining legitimacy—is unknown.

**AI takeoff speeds**: The debate between gradual and fast AI development scenarios remains unresolved, with implications for both safety strategy and broader social planning. Hanson's gradualist position may prove correct, or rapid capability gains could vindicate faster takeoff models.

**Hidden motives magnitude**: While signaling and status competition clearly influence behavior, the extent to which hidden motives dominate versus complement stated motivations is difficult to measure. Hanson's strong claims about medicine, education, and charity may overstate the case or may accurately describe underappreciated dynamics.

**<EntityLink id="long-term-trajectory">Long-term trajectory</EntityLink>**: Whether Hanson's scenario analysis of ems, space colonization, and other far-future developments will prove prescient or will be overtaken by unanticipated technological paths remains to be seen.

## Sources

[^1]: [Robin Hanson bio - George Mason University Economics](https://economics.gmu.edu/people/rhanson)
[^2]: [Robin Hanson professional bio - hanson.gmu.edu](https://hanson.gmu.edu/bio)
[^3]: [Robin Hanson - Foresight Institute](https://foresight.org/people/robin-d-hanson/)
[^4]: [AI Risk, Again - Overcoming Bias](https://www.overcomingbias.com/p/ai-risk-again)
[^5]: [Robin Hanson: One Futurist To Rule Them All - The Politic](https://thepolitic.org/robin-hanson-one-futurist-to-rule-them-all/?curator=MediaREDEF)
[^6]: [Robin Hanson - Wikipedia](https://en.wikipedia.org/wiki/Robin_Hanson)
[^7]: [Robin Hanson speaker bio - CWG Speakers](https://cwgspeakers.com/speaker/robin-hanson/)
[^8]: [Robin Hanson - Machine Intelligence Research Institute](https://intelligence.org/2013/11/01/robin-hanson/)
[^9]: [Robin Hanson bio - hanson.gmu.edu](https://hanson.gmu.edu/bio)
[^10]: [Robin Hanson - Foresight Institute](https://foresight.org/people/robin-d-hanson/)
[^11]: [Robin Hanson - Wikipedia](https://en.wikipedia.org/wiki/Robin_Hanson)
[^12]: [Robin Hanson - ALLFED](https://allfed.info/about/team-and-board?view=article&id=36%3Arobin-d-hanson&catid=10)
[^13]: [Robin Hanson bio - hanson.gmu.edu](https://hanson.gmu.edu/bio)
[^14]: [Robin Hanson speaker bio - CWG Speakers](https://cwgspeakers.com/speaker/robin-hanson/)
[^15]: [Robin Hanson - Wikipedia](https://en.wikipedia.org/wiki/Robin_Hanson)
[^16]: [Robin Hanson - ALLFED](https://allfed.info/about/team-and-board?view=article&id=36%3Arobin-d-hanson&catid=10)
[^17]: [Robin Hanson bio - hanson.gmu.edu](https://hanson.gmu.edu/bio)
[^18]: [Robin Hanson speaker bio - CWG Speakers](https://cwgspeakers.com/speaker/robin-hanson/)
[^19]: [Robin Hanson - Closer To Truth](https://closertotruth.com/contributor/robin-hanson/)
[^20]: [Robin Hanson - Wikipedia](https://en.wikipedia.org/wiki/Robin_Hanson)
[^21]: [Robin Hanson bio - hanson.gmu.edu](https://hanson.gmu.edu/bio)
[^22]: [Robin Hanson - Fortune Magazine profile](https://commerce.net/fortune-magazine-profiles-robin-hanson/)
[^23]: [Robin Hanson bio - hanson.gmu.edu](https://hanson.gmu.edu/bio)
[^24]: [Robin Hanson - George Mason University Economics](https://economics.gmu.edu/people/rhanson)
[^25]: [Robin Hanson publications - IDEAS/RePEc](https://ideas.repec.org/f/pha1491.html)
[^26]: [Robin Hanson bio - hanson.gmu.edu](https://hanson.gmu.edu/bio)
[^27]: [Robin Hanson - George Mason University Economics](https://economics.gmu.edu/people/rhanson)
[^28]: [Robin Hanson bio - hanson.gmu.edu](https://hanson.gmu.edu/bio)
[^29]: [Robin Hanson - Wikipedia](https://en.wikipedia.org/wiki/Robin_Hanson)
[^30]: [Robin Hanson - Closer To Truth](https://closertotruth.com/contributor/robin-hanson/)
[^31]: [The Age of Em - Robin Hanson](https://ageofem.com)
[^32]: [The Age of Em - Robin Hanson](https://ageofem.com)
[^33]: [Robin Hanson on Lying to Ourselves - 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/robin-hanson-on-lying-to-ourselves/)
[^34]: [Robin Hanson on Lying to Ourselves - 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/robin-hanson-on-lying-to-ourselves/)
[^35]: [Robin Hanson on Lying to Ourselves - 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/robin-hanson-on-lying-to-ourselves/)
[^36]: [The Hanson-Yudkowsky AI-Foom Debate - LessWrong](https://www.lesswrong.com/w/robin-hanson?version=1.0.0)
[^37]: [AI Risk, Again - Overcoming Bias](https://www.overcomingbias.com/p/ai-risk-again)
[^38]: [Robin Hanson on AGI, Emulating Human Consciousness - Emerj Podcast](https://podcast.emerj.com/ai-futures-s2e2-dr-robin-hanson-on-agi-emulating-human-consciousness-and-the-future-of-work-and-life)
[^39]: [AI Risk, Again - Overcoming Bias](https://www.overcomingbias.com/p/ai-risk-again)
[^40]: [Linkpost: What Are Reasonable AI Fears? by Robin Hanson (2023) - EA Forum](https://forum.effectivealtruism.org/posts/XnnfPC2gsgRFZezkE/linkpost-what-are-reasonable-ai-fears-by-robin-hanson-2023)
[^41]: [AI Risk, Again comments - Overcoming Bias](https://www.overcomingbias.com/p/ai-risk-again/comments)
[^42]: [Robin Hanson AI X-Risk Debate Highlights and Analysis - LessWrong](https://www.lesswrong.com/posts/quj3RniAw7GGJtqeX/robin-hanson-ai-x-risk-debate-highlights-and-analysis)
[^43]: [Robin Hanson - Wikipedia](https://en.wikipedia.org/wiki/Robin_Hanson)
[^44]: [Robin Hanson - Fortune Magazine profile](https://commerce.net/fortune-magazine-profiles-robin-hanson/)
[^45]: [Some Thoughts on the EA Munich Robin Hanson Incident - EA Forum](https://forum.effectivealtruism.org/posts/K8pkTQrZjf2iSWd3E/some-thoughts-on-the-ea-munich-robin-hanson-incident)
[^46]: [Appeasing Hanson's Critics - EconLog](https://www.econlib.org/appeasing-hansons-critics/)
[^47]: [Where Do I Disagree with Robin Hanson? - Marginal Revolution](https://marginalrevolution.com/marginalrevolution/2007/08/where-do-i-disa.html)
[^48]: [Some Thoughts on the EA Munich Robin Hanson Incident - EA Forum](https://forum.effectivealtruism.org/posts/K8pkTQrZjf2iSWd3E/some-thoughts-on-the-ea-munich-robin-hanson-incident)
[^49]: [My New Grant - Overcoming Bias](https://www.overcomingbias.com/p/my-new-granthtml)
[^50]: [Coefficient Giving Grant to MIRI - Machine Intelligence Research Institute](https://intelligence.org/2016/09/06/grant-open-philanthropy/)
[^51]: [Robin Hanson - ALLFED](https://allfed.info/about/team-and-board?view=article&id=36%3Arobin-d-hanson&catid=10)
[^52]: [Robin Hanson - Foresight Institute](https://foresight.org/people/robin-d-hanson/)
[^53]: [Big Ideas with Robin Hanson - Ideas Untrapped](https://www.ideasuntrapped.com/p/big-ideas-with-robin-hanson)
[^54]: [Big Ideas with Robin Hanson - Ideas Untrapped](https://www.ideasuntrapped.com/p/big-ideas-with-robin-hanson)
[^55]: [AI Is GPT and GPTs Go Slow - Overcoming Bias](https://www.overcomingbias.com/p/ai-is-gpt-and-gpts-go-slow)
