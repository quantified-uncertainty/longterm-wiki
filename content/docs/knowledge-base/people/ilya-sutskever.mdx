---
title: Ilya Sutskever
description: Co-founder and CEO of Safe Superintelligence Inc., formerly Chief Scientist at OpenAI
sidebar:
  order: 11
entityType: person
subcategory: lab-leadership
quality: 26
readerImportance: 34
researchImportance: 36
lastEdited: "2026-02-22"
update_frequency: 21
llmSummary: Biographical overview of Ilya Sutskever's career trajectory from deep learning researcher (AlexNet, seq2seq, dropout) to co-founding Safe Superintelligence Inc. in 2024 after leaving OpenAI. Documents his role in the November 2023 OpenAI board incident, his co-leadership of the Superalignment team, SSI's funding history (over $3 billion raised at a $32 billion valuation as of April 2025), and his public statements on the limits of scaling and the path to safe superintelligence.
ratings:
  novelty: 2
  rigor: 3.5
  actionability: 1.5
  completeness: 5
clusters: ["ai-safety"]
---
import {DataInfoBox, DataExternalLinks, EntityLink} from '@components/wiki';

<DataExternalLinks pageId="ilya-sutskever" />

<DataInfoBox entityId="E163" />

## Quick Assessment

| Aspect | Assessment |
|--------|-----------|
| **Primary Role** | CEO and co-founder of Safe Superintelligence Inc. (SSI); formerly Chief Scientist at <EntityLink id="E218" name="openai">OpenAI</EntityLink> (2015–2024) |
| **Key Contributions** | Co-author of AlexNet (2012), sequence-to-sequence learning (2014), and the dropout paper (2014); led research at OpenAI on the GPT series, DALL-E, and RLHF |
| **Key Publications** | "ImageNet Classification with Deep Convolutional Neural Networks" (2012); "Sequence to Sequence Learning with Neural Networks" (2014); "Dropout: A Simple Way to Prevent Neural Networks from Overfitting" (2014) |
| **Institutional Affiliation** | Safe Superintelligence Inc. (SSI) |
| **Influence on AI Safety** | Co-led OpenAI's Superalignment team (2023–2024); founded SSI (2024) with the explicit mission of building safe superintelligence; has made public statements on the inadequacy of current AI generalization and the limits of scaling |

## Overview

Ilya Sutskever (born December 8, 1986) is an Israeli-Canadian computer scientist and a co-founder of Safe Superintelligence Inc. (SSI). He was previously a co-founder and Chief Scientist of <EntityLink id="E218" name="openai">OpenAI</EntityLink>, where he worked from 2015 to 2024. Prior to OpenAI, he conducted research at Google Brain and completed his PhD under <EntityLink id="E149" name="geoffrey-hinton">Geoffrey Hinton</EntityLink> at the University of Toronto. His research contributions include co-authorship of AlexNet (2012), which demonstrated large-scale convolutional neural network performance on image classification; the sequence-to-sequence learning framework (2014), which shaped modern natural language processing; and the foundational dropout regularization paper (2014).[^1][^2][^3]

Sutskever co-led OpenAI's Superalignment team from its founding in July 2023 until his departure in May 2024. He subsequently co-founded SSI in June 2024 with Daniel Gross and Daniel Levy, with the stated mission of building safe superintelligence as a singular technical goal, without concurrent product development or commercial obligations.[^4] SSI raised over \$3 billion across multiple funding rounds through April 2025, reaching a reported valuation of \$32 billion.[^5]

In a November 2025 interview, Sutskever argued that the current period represents a transition away from scaling-dominated progress toward a new phase requiring fundamental research breakthroughs, describing current large language model generalization as "jagged" and "dramatically worse than people."[^6]

## Background

### Early Life and Education

Sutskever was born in 1986 in Nizhny Novgorod (then Gorky), Russian SFSR, Soviet Union, into a Jewish family.[^7] In 1991, at approximately age five, he emigrated with his family to Israel, where he grew up in Jerusalem.[^7][^8] He attended the Open University of Israel from approximately 2000 to 2002, where he studied computer science and mathematics, and has credited the institution with providing him access to academic study at a young age.[^8][^9]

In 2002, at approximately age fifteen, he relocated with his family to Canada.[^7] He subsequently enrolled at the University of Toronto, where he earned a bachelor's degree in mathematics (2005), a master's degree in computer science (2007), and a PhD in computer science (2013) under the supervision of <EntityLink id="E149" name="geoffrey-hinton">Geoffrey Hinton</EntityLink>.[^7] He holds Israeli and Canadian nationality and speaks Russian, Hebrew, and English.[^7]

### Career

- **PhD, University of Toronto** (2013), supervised by Geoffrey Hinton
- **Research scientist, Google Brain** (2012–2013; internship and subsequent position)
- **Co-founder and Chief Scientist, <EntityLink id="E218" name="openai">OpenAI</EntityLink>** (2015–2024)
- **Co-lead, Superalignment team, OpenAI** (2023–2024), alongside <EntityLink id="E182" name="jan-leike">Jan Leike</EntityLink>
- **Co-founder and CEO, Safe Superintelligence Inc.** (2024–present)

## Major Technical Contributions

### AlexNet (2012)

With Alex Krizhevsky and <EntityLink id="E149" name="geoffrey-hinton">Geoffrey Hinton</EntityLink>, Sutskever co-authored "ImageNet Classification with Deep Convolutional Neural Networks," which demonstrated that deep convolutional neural networks trained on GPUs could achieve substantially lower error rates than prior methods on the ImageNet Large Scale Visual Recognition Challenge.[^1] The paper is among the most cited works in machine learning. Its approach to training deep networks on large image datasets influenced subsequent work across computer vision and other domains.

### Dropout (2014)

Sutskever is a co-author of the foundational dropout paper ("Dropout: A Simple Way to Prevent Neural Networks from Overfitting," Srivastava, Hinton, Krizhevsky, Sutskever, Salakhutdinov, 2014), which introduced the technique of randomly deactivating units during training to reduce overfitting. The paper demonstrated improvements across supervised learning tasks in vision, speech recognition, document classification, and computational biology.[^3]

### Sequence-to-Sequence Learning (2014)

Co-authored with Oriol Vinyals and Quoc Le at Google Brain, "Sequence to Sequence Learning with Neural Networks" (2014) introduced an encoder-decoder architecture using stacked LSTMs capable of mapping variable-length input sequences to variable-length output sequences.[^2] The paper provided a general framework for tasks including machine translation. The seq2seq paper received a Test of Time award at NeurIPS 2024 and had accumulated approximately 28,000 citations as of December 2024.[^10]

### LSTM Regularization

Sutskever co-authored "Recurrent Neural Network Regularization" (Zaremba, Sutskever, Vinyals), which introduced methods for applying dropout to LSTM networks to reduce overfitting in recurrent architectures.[^11]

### At OpenAI (2015–2024)

As Chief Scientist, Sutskever oversaw research contributing to the GPT series of language models, DALL-E (image generation), and foundational work on reinforcement learning from human feedback (<EntityLink id="E259" name="rlhf">RLHF</EntityLink>) and scaling laws for neural language models.[^11] His Google Scholar profile reflects over 760,000 citations across machine learning and deep learning.[^11]

## The Shift to Safety

### Timeline of Evolution

**Early OpenAI (2015–2019):**
Sutskever was primarily focused on capabilities research, leading development of large-scale generative models at OpenAI.

**Growing internal focus on safety (2020–2022):**
Sutskever increasingly engaged with alignment questions internally, alongside continued capabilities work.

**Superalignment (July 2023):**
Sutskever co-led the newly formed Superalignment team with <EntityLink id="E182" name="jan-leike">Jan Leike</EntityLink>. OpenAI announced a commitment to dedicate 20% of its compute resources to the team's alignment research.[^12] The team's stated goal was to develop technical solutions ensuring AI systems remain aligned with human objectives as capabilities scale toward superintelligence.[^12]

**OpenAI Departure and SSI (May–June 2024):**
Sutskever announced his departure from OpenAI in May 2024. He founded Safe Superintelligence Inc. in June 2024 with an explicit focus on safe superintelligence as a singular technical goal.[^4]

### The OpenAI Board Incident (November 2023)

On November 17, 2023, the OpenAI board voted to remove <EntityLink id="E269" name="sam-altman">Sam Altman</EntityLink> as CEO, stating he had not been "consistently candid in his communications with the board."[^13] The board at the time consisted of Sutskever, Adam D'Angelo (CEO of Quora), Tasha McCauley, and Helen Toner (strategy director at Georgetown's Center for Security and Emerging Technology).[^13]

According to reporting by Kara Swisher and The Wall Street Journal, Sutskever was instrumental in organizing the removal.[^13] In a sworn deposition later filed in the Musk v. Altman lawsuit, Sutskever stated that he had written a 52-page brief making the case for Altman's removal, and that he had been considering the action for at least a year, waiting for a moment when "the majority of the board is not obviously friendly with Sam." He stated in the deposition: "Sam exhibits a consistent pattern of lying, undermining his execs, and pitting his execs against one another."[^14]

Former board member Helen Toner later disclosed that the board had received accounts from two executives describing Altman's behavior as "psychological abuse," that the board had not been informed in advance about ChatGPT's November 2022 launch, and that Altman had not disclosed to the board that he personally owned the OpenAI Startup Fund.[^15]

By November 20, approximately 800 employees — nearly the entire staff — signed a letter demanding the board resign and Altman be reinstated. Sutskever was among the signatories.[^13] Altman returned as CEO within four days of his removal.[^13]

The incident generated significant public attention to internal governance tensions at OpenAI, though the precise relationship between Sutskever's safety concerns and his motivations for the board vote remains a matter of interpretation. OpenAI and Altman have not publicly characterized the episode in terms of a safety-versus-commercialization dispute.[^13][^14]

### Jan Leike's Departure and the Dissolution of Superalignment

In May 2024, Sutskever and <EntityLink id="E182" name="jan-leike">Jan Leike</EntityLink> announced their departures from OpenAI within hours of each other.[^12] Leike's public statement read: "I joined because I thought OpenAI would be the best place in the world to do this research. However, I have been disagreeing with OpenAI leadership about the company's core priorities for quite some time, until we finally reached a breaking point." He also wrote that OpenAI's "safety culture and processes have taken a backseat to shiny products."[^16]

OpenAI subsequently dissolved the Superalignment team, integrating its members into other research groups. Jakub Pachocki replaced Sutskever as Chief Scientist. The team had existed for less than one year since its July 2023 founding.[^12] Reporting from LessWrong and Axios noted that the Superalignment team had struggled to receive the compute resources originally pledged, despite OpenAI's public commitment of 20% of its compute to the team.[^17] Multiple other safety-focused researchers departed OpenAI around the same period, including Daniel Kokotajlo, Leopold Aschenbrenner, Cullen O'Keefe, Pavel Izmailov, and William Saunders.[^17]

## Safe Superintelligence Inc. (SSI)

### Founding and Mission

SSI was founded on June 19, 2024, when Sutskever announced it on X.[^18] Co-founders are Sutskever, Daniel Gross, and Daniel Levy. SSI's stated mission, per its website, is to be "the world's first straight-shot SSI lab" with one goal and one product: safe superintelligence. The organization states it approaches safety and capabilities "in tandem as a technical problem to be solved through revolutionary engineering and scientific breakthroughs."[^19]

The founding premise is that safety cannot be an afterthought, and that removing commercial product obligations and short-term revenue pressures enables the organization to focus on long-horizon technical research. SSI has no published AI model, technical demonstration, or research paper as of early 2026, and operates in what its leadership has described as deliberate stealth.[^18][^20]

### Co-Founders: Daniel Gross and Daniel Levy

**Daniel Gross** is an Israeli-American entrepreneur. He previously co-founded Cue, an enterprise search startup acquired by Apple, where he subsequently led artificial intelligence efforts. He served as a partner at Y Combinator and is a notable technology investor with positions including Uber, Instacart, Figma, GitHub, Airtable, Rippling, CoreWeave, Character.ai, and Perplexity AI.[^21] Gross served as CEO of SSI from its founding until June 29, 2025, when he departed to join Meta Superintelligence Labs. Following his departure, Sutskever assumed the CEO role and Daniel Levy became President.[^21]

**Daniel Levy** is a researcher who has served as a technical lead at SSI since its founding. His prior background is less extensively documented in public sources.

### Funding and Organizational Scale

SSI raised \$1 billion in a Series A round in September 2024 at an initial valuation of \$5 billion. Investors included Andreessen Horowitz, Sequoia Capital, DST Global, and SV Angel. All funding was provided in cash rather than cloud credits, primarily intended for compute acquisition and hiring. At the time of the raise, SSI had approximately 10 employees, split between offices in Palo Alto and Tel Aviv.[^22]

In March 2025, SSI reached a \$30 billion valuation in a round led by Greenoaks Capital — six times the September 2024 valuation.[^23] In April 2025, SSI raised an additional \$2 billion at a \$32 billion valuation, with investors including Alphabet, NVIDIA, Andreessen Horowitz, Lightspeed Venture Partners, and DST Global.[^5] Total funding raised through April 2025 exceeded \$3 billion.[^5]

Alphabet is both an investor and infrastructure partner: SSI announced at Google Cloud Next in April 2025 that it would use Google's TPU chips for its research, and has been described as Google Cloud's most significant external TPU customer.[^24]

As of March 2025, SSI employed approximately 20 people, with projections to reach approximately 50 by mid-2025.[^18] The organization generates no revenue and has released no products.[^18]

### Approach and Stated Philosophy

SSI's public position, as summarized on its website and in Sutskever's public statements:[^19]

1. Safety and capabilities must be advanced together, not sequentially
2. "Revolutionary engineering and scientific breakthroughs" are required, rather than incremental improvements to existing techniques
3. Commercial product obligations are treated as an organizational distraction to be avoided
4. A long time horizon is required; premature deployment is a risk to be managed
5. Compute and world-class research talent are the primary resource constraints

In a November 2025 interview with Dwarkesh Patel, Sutskever acknowledged that if timelines to superintelligence prove longer than anticipated, SSI might release a product, stating: "if it would be useful for the world to see powerful AI in action," early release could be considered.[^25]

### Criticisms and Open Questions

Critics and observers have raised several concerns about SSI's approach:

- **The founding paradox**: Some AI safety researchers question whether building superintelligence in order to make it safe is itself a risk-creating strategy, rather than a risk-mitigation strategy. SSI's response, implicit in its mission statement, is that the alternative — superintelligence developed without safety-primary organizations competing — is worse.
- **Commercial pressure over time**: Observers have noted that SSI may face the same organizational pressures as OpenAI as it grows and investors expect returns. SSI has not published a governance structure or legal mechanism equivalent to OpenAI's original nonprofit structure.
- **Stealth research**: Operating without publishing research or benchmarks makes independent evaluation of SSI's safety progress difficult. Whether this is a strategic choice to avoid competitive pressure or reflects the early stage of research is unclear.
- **Feasibility of the "tandem" approach**: The claim that safety and capabilities can be advanced together remains a hypothesis rather than a demonstrated result. It is contested by researchers who argue that capability advances create alignment problems faster than they can be solved.

## Views on AI Safety and Technical Approach

### Public Statements on Scaling and Research

At NeurIPS 2024 in December 2024, Sutskever accepted the Test of Time award for the seq2seq paper and delivered a talk titled "Sequence to Sequence Learning with Neural Networks: What a Decade." In the talk, he stated: "Pre-training as we know it will unquestionably end," and described future AI systems as "different, qualitatively" from current pattern-recognition-based models. He argued that reasoning capabilities beyond current approaches are necessary for the next stage of AI development.[^10]

In his November 2025 interview with Dwarkesh Patel, Sutskever described the period from 2012 to 2020 as "the age of research," 2020 to 2025 as "the age of scaling," and the current period as a return to research "just with big computers." He argued that scaling alone — including a hypothetical further 100x increase in compute — would not transform AI capabilities in the way scaling did from 2020 to 2025.[^6] He described current LLM generalization as "jagged": highly capable models that inexplicably fail at basic tasks. He stated: "These models somehow just generalize dramatically worse than people. It's super obvious."[^6] He estimated the timeline to AGI at roughly 5–20 years.[^6]

On superintelligence governance, Sutskever stated in the same interview that he would like the most powerful superintelligent AI to be "somehow capped," while acknowledging he does not know how to achieve that.[^26]

### Alignment Concerns

Based on Sutskever's public statements and SSI's founding rationale, his stated concerns include:

- **Generalization failures in current systems**: The gap between benchmark performance and real-world capability as evidence of fundamental limitations in current training approaches[^6]
- **Inadequate oversight at scale**: The challenge of maintaining meaningful human supervision over systems significantly more capable than humans
- **Deployment pressure**: Commercial incentives pushing toward release of systems before alignment is established — a dynamic he observed at OpenAI according to contemporaneous reporting[^16]
- **The end of pre-training as a sufficient paradigm**: The need for new training approaches beyond next-token prediction at scale[^10]

SSI has not published technical work describing specific alignment approaches it is pursuing as of early 2026.

### Core Stated Beliefs

From Sutskever's public statements and SSI's founding documents:

1. **Superintelligence is a near-term enough possibility to warrant immediate dedicated effort** — evidenced by his decision to found a company specifically aimed at it in 2024
2. **Safety cannot be retrofitted** — SSI's mission statement implies safety and capabilities must be developed in parallel from the outset
3. **Current technical approaches are insufficient** — Sutskever has publicly described generalization failures and the limits of scaling as fundamental problems requiring new research directions
4. **Commercial pressure poses organizational risks to safety-oriented research** — the SSI structure is explicitly designed to avoid this pressure
5. **Both capabilities and safety require continued technical work** — SSI does not advocate slowing AI development but aims to advance both in parallel

## Technical Perspective on Safety

### Background Relevant to His Assessments

Sutskever's technical background — including co-authorship of systems that exhibited emergent capabilities and led to current large-scale generative models — gives him direct familiarity with the trajectory of AI capability development. He has noted in interviews that his experience building these systems informs his assessment of what future systems may be capable of.[^6]

### Research Direction at SSI

SSI has not published research papers, demos, or technical benchmarks as of early 2026 and operates in deliberate stealth.[^20] Sutskever's public statements suggest areas of interest include:

- Synthetic data generation as a path beyond the limits of pre-training on human-generated text[^10]
- Inference-time compute as a mechanism for improving reasoning[^10]
- Improving AI generalization beyond current "jagged" benchmark performance[^6]
- Developing alignment approaches that scale to systems more capable than current LLMs

One of SSI's early Tel Aviv hires was Dr. Yair Carmon, a Stanford PhD and Tel Aviv University lecturer focused on robust and reliable machine learning algorithms.[^20]

## Public Communication

Sutskever is notably infrequent in public appearances. His major documented public appearances since founding SSI include:

- **NeurIPS 2024** (December 2024, Vancouver): Accepted the Test of Time award for seq2seq paper; delivered a talk on the decade since the paper and the limits of pre-training[^10]
- **Dwarkesh Patel podcast** (November 25, 2025): One of his few extended interviews since founding SSI; discussed scaling limits, generalization failures, AGI timelines, and SSI's strategy[^6]

SSI's website contains only a brief mission statement and an updates page; no blog or technical posts have been published.[^19]

## Organizational Context and Comparisons

### vs. <EntityLink id="E22" name="anthropic">Anthropic</EntityLink>
- **Similarities**: Both are safety-focused organizations founded by OpenAI alumni; both hold that safety and capabilities must be developed together
- **Differences**: <EntityLink id="E22" name="anthropic">Anthropic</EntityLink> generates revenue through commercial products (Claude API, Claude.ai consumer products); SSI has no products and no revenue. Anthropic publishes research including Constitutional AI and interpretability work; SSI has published no research. Anthropic employs hundreds; SSI employs tens.

### vs. <EntityLink id="E98" name="deepmind">Google DeepMind</EntityLink>
- **Similarities**: Both pursue large-scale technical AI research
- **Differences**: DeepMind is a subsidiary of Alphabet with broad research and product obligations; SSI is an independent company with a single stated technical goal

### vs. <EntityLink id="E202" name="miri">MIRI</EntityLink> and <EntityLink id="E25" name="arc">ARC</EntityLink>
- **Similarities**: Both SSI and organizations like MIRI and ARC prioritize long-term AI safety questions
- **Differences**: MIRI conducts theoretical alignment research and has published work on agent foundations and decision theory; ARC Evals develops evaluation methods for dangerous capabilities. SSI's stated approach involves building toward superintelligence rather than primarily theoretical or evaluative work. The three organizations hold different views on what technical work is most tractable.

## Influence and Impact

### Technical Legacy

Sutskever's co-authored papers — AlexNet, seq2seq, and dropout — are among the most cited works in modern machine learning.[^1][^2][^3] The seq2seq architecture in particular shaped the encoder-decoder structure that underlies many natural language processing systems.[^10]

### Organizational and Strategic Influence

Sutskever's departure from OpenAI in May 2024, alongside <EntityLink id="E182" name="jan-leike">Jan Leike</EntityLink>'s departure and public statement, drew widespread attention to internal tensions at the organization over the prioritization of safety versus capabilities and commercial timelines. The Superalignment team's dissolution shortly after both co-leaders departed was noted by AI safety observers as significant.[^16][^17]

SSI's funding trajectory — reaching a reported \$32 billion valuation within approximately ten months of founding — demonstrated that investor capital is available for organizations with an explicit safety-first framing, though whether SSI's approach will produce safety advances distinct from organizations like Anthropic or DeepMind remains to be evaluated.[^5]

### Field Building

Sutskever trained and worked alongside researchers at OpenAI who now hold senior positions across the field. SSI's hiring has drawn researchers to a safety-focused environment, though the organization's small size limits its current scale of influence relative to larger labs.

---

[^1]: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). "ImageNet Classification with Deep Convolutional Neural Networks." *Advances in Neural Information Processing Systems*.

[^2]: Sutskever, I., Vinyals, O., & Le, Q. V. (2014). "Sequence to Sequence Learning with Neural Networks." *Advances in Neural Information Processing Systems*. [PDF](https://arxiv.org/abs/1409.3215)

[^3]: Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). "Dropout: A Simple Way to Prevent Neural Networks from Overfitting." *Journal of Machine Learning Research*, 15, 1929–1958. [PDF](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)

[^4]: [Safe Superintelligence Inc. — Wikipedia](https://en.wikipedia.org/wiki/Safe_Superintelligence_Inc.) (retrieved 2025).

[^5]: ["\$2B Raise at \$32B Valuation: 5 Facts About OpenAI Co-Founder's Safe Superintelligence"](https://techfundingnews.com/inside-the-32b-ai-unicorn-backed-by-alphabet-nvidia-5-facts-about-openai-co-founders-safe-superintelligence/), *Tech Funding News*, April 2025.

[^6]: Patel, D. (2025, November 25). ["Ilya Sutskever — We're moving from the age of scaling to the age of research"](https://www.dwarkesh.com/p/ilya-sutskever-2). *The Dwarkesh Podcast*.

[^7]: ["Ilya Sutskever — Wikipedia"](https://en.wikipedia.org/wiki/Ilya_Sutskever) (retrieved 2025).

[^8]: ["Ilya Sutskever Biography — Life, Career & Facts"](https://www.biyografiler.com/en/biography/ilya-sutskever) (retrieved 2025).

[^9]: ["Interview with Dr. Ilya Sutskever, Co-Founder of OpenAI"](https://eightify.app/summary/artificial-intelligence/interview-with-dr-ilya-sutskever-co-founder-of-open-ai-open-university-studio-hebrew-translation), *Open University Studio* (Hebrew; English summary via Eightify).

[^10]: Sahota, H. (2024, December). ["NeurIPS Recap Week — Day 2 (Ilya Sutskever Keynote)"](https://harpreetsahota.substack.com/p/neurips-recap-week-day-2). *Substack*. See also: ["OpenAI co-founder Ilya Sutskever believes superintelligent AI will be 'unpredictable'"](https://techcrunch.com/2024/12/13/openai-co-founder-ilya-sutskever-believes-superintelligent-ai-will-be-unpredictable/), *TechCrunch*, December 13, 2024.

[^11]: ["Aman's AI Journal — Ilya Sutskever's Top 30 Papers"](https://aman.ai/primers/ai/top-30-papers/) (retrieved 2025). Citation count from Google Scholar profile as cited therein.

[^12]: ["OpenAI dissolves team focused on long-term AI risks"](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html), *CNBC*, May 18, 2024. See also: ["OpenAI's Long-Term Safety Team Has Disbanded"](https://www.axios.com/2024/05/17/openai-superalignment-risk-ilya-sutskever), *Axios*, May 17, 2024.

[^13]: ["Removal of Sam Altman from OpenAI — Wikipedia"](https://en.wikipedia.org/wiki/Removal_of_Sam_Altman_from_OpenAI) (retrieved 2025). See also: ["4 Days from Fired to Re-Hired: A Timeline of Sam Altman's Ouster"](https://abcnews.go.com/Business/sam-altman-reaches-deal-return-ceo-openai/story?id=105091534), *ABC News*, November 2023.

[^14]: ["Inside the Deposition That Showed How OpenAI Nearly Destroyed Itself"](https://decrypt.co/347349/inside-deposition-showed-openai-nearly-destroyed-itself), *Decrypt* (retrieved 2025). Sourced from Sutskever's sworn deposition in Musk v. Altman.

[^15]: ["Former OpenAI Board Member Explains Why Sam Altman Got Fired"](https://www.cnbc.com/2024/05/29/former-openai-board-member-explains-why-ceo-sam-altman-was-fired.html), *CNBC*, May 29, 2024. Interview with Helen Toner.

[^16]: ["Top OpenAI researcher resigns, saying company prioritized 'shiny products' over AI safety"](https://fortune.com/2024/05/17/openai-researcher-resigns-safety/), *Fortune*, May 17, 2024. Direct quotes from Jan Leike's public statement on X.

[^17]: ["Ilya Sutskever and Jan Leike Resign from OpenAI [Updated]"](https://www.lesswrong.com/posts/JSWF2ZLt6YahyAauE/ilya-sutskever-and-jan-leike-resign-from-openai-updated), *LessWrong*, May 2024.

[^18]: ["Safe Superintelligence Inc. — Wikipedia"](https://en.wikipedia.org/wiki/Safe_Superintelligence_Inc.) (retrieved 2025); ["Ilya Sutskever's SSI: Inside the \$32B Quest for Safe Superintelligence"](https://www.asapdrew.com/p/ilya-sutskever-ssi-safe-superintelligence), April 2025.

[^19]: [Safe Superintelligence Inc. official website](https://ssi.inc/) (retrieved 2025).

[^20]: ["Ilya Sutskever's SSI: Inside the \$32B Quest for Safe Superintelligence"](https://www.asapdrew.com/p/ilya-sutskever-ssi-safe-superintelligence), April 2025.

[^21]: ["Daniel Gross (businessman) — Wikipedia"](https://en.wikipedia.org/wiki/Daniel_Gross_(businessman)) (retrieved 2025).

[^22]: ["Safe Superintelligence (SSI) Raises \$1 Billion in Series A — September 4, 2024"](https://salestools.io/en/report/safe-superintelligence-ssi-raises-1-billion-series-a-september-2024), *SalesTools*, September 2024.

[^23]: ["Safe Superintelligence Inc. — Wikipedia"](https://en.wikipedia.org/wiki/Safe_Superintelligence_Inc.) (retrieved 2025); March 2025 Greenoaks Capital round.

[^24]: ["Safe Superintelligence to Use Google's TPU Chips for Research"](https://www.datacenterdynamics.com/en/news/ai-startup-safe-superintelligence-to-use-googles-tpu-chips-for-research/), *Data Center Dynamics*, April 2025.

[^25]: Sherry, B. (2025, November 26). ["This OpenAI Co-Founder Has Raised Billions. He Has No Product Plans Yet."](https://www.inc.com/ben-sherry/openai-co-founder-ilya-sutskever-safe-superintelligence-3-billion-no-product/91271937), *Inc. Magazine*.

[^26]: ["On Dwarkesh Patel's Second Interview With Ilya Sutskever"](https://www.lesswrong.com/posts/bMvCNtSH8DiGDTvXd/on-dwarkesh-patel-s-second-interview-with-ilya-sutskever), *LessWrong*, December 2025.

