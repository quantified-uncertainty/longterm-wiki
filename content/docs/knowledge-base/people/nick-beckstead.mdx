---
title: Nick Beckstead
description: American philosopher and longtermism researcher, known for his PhD
  dissertation on shaping the far future, former Program Officer at Open
  Philanthropy, and co-founder of the Secure AI Project.
importance: 50
lastEdited: "2026-02-20"
subcategory: ea-figures
sidebar:
  order: 50
ratings:
  novelty: 5
  rigor: 7
  actionability: 4
  completeness: 8
readerImportance: 58
tacticalValue: 62
quality: 60
llmSummary: Nick Beckstead is a philosopher and EA/longtermism figure whose 2013
  dissertation formalized longtermist ethics; this article covers his career arc
  from academic philosopher to Open Philanthropy grantmaker to FTX Future Fund
  CEO to Secure AI Project founder, including substantive coverage of
  controversies around FTX forewarnings, longtermist prioritization critiques,
  and alleged research suppression.
balanceFlags:
  - missing-source-incentives
  - unsourced-biographical-details
entityType: person
---
import {EntityLink, R, F, Calc, DataInfoBox, DataExternalLinks} from '@components/wiki';

## Quick Assessment

| Attribute | Detail |
|-----------|--------|
| **Full Name** | Nicholas Beckstead |
| **Born** | 1985 |
| **Nationality** | American |
| **Field** | Philosophy, AI safety, longtermism, effective altruism |
| **Current Role** | Co-founder and CEO, Secure AI Project |
| **Key Contribution** | PhD dissertation *On the Overwhelming Importance of Shaping the Far Future* (2013) |
| **Affiliated Organizations** | Secure AI Project, formerly <EntityLink id="E552">Open Philanthropy</EntityLink>, FTX Future Fund, <EntityLink id="E140">Future of Humanity Institute</EntityLink>, <EntityLink id="E517">Centre for Effective Altruism</EntityLink> |

## Key Links

| Source | Link |
|--------|------|
| Official Website | [nickbeckstead.com](https://www.nickbeckstead.com) |
| Wikipedia | [en.wikipedia.org](https://en.wikipedia.org/wiki/Longtermism) |
| EA Forum | [forum.effectivealtruism.org](https://forum.effectivealtruism.org/topics/nick-beckstead) |
| PhilPeople | [philpeople.org](https://philpeople.org/profiles/nick-beckstead) |
| Secure AI Project | [secureaiproject.org](https://secureaiproject.org/team/nick-beckstead/) |

## Overview

<EntityLink id="E861">Nick Beckstead</EntityLink> is an American philosopher whose work has been central to the development of longtermism as a philosophical and philanthropic framework. His 2013 Rutgers University PhD dissertation, *On the Overwhelming Importance of Shaping the Far Future*, is widely cited as an early systematic defense of the view that actions affecting the trajectory of civilization over millions of years deserve enormous moral weight—potentially outweighing the near-term benefits of conventional charitable interventions.[^1] Supervised by philosopher Larry Temkin, the dissertation addresses existential risk, population ethics, and decision theory under uncertainty, and continues to be a foundational reference in the effective altruism and AI safety communities.[^2]

Beyond academic philosophy, Beckstead has played a significant operational role in the effective altruism ecosystem. He was among the earliest employees at <EntityLink id="E552">Open Philanthropy</EntityLink>, serving as a Program Officer overseeing grantmaking in global catastrophic risk reduction, machine learning, science philanthropy, animal product alternatives, and mechanisms of aging.[^3] He subsequently served as CEO of the FTX Foundation and FTX Future Fund before that organization collapsed in November 2022, following which he transitioned to policy-focused AI safety work. He currently co-founded and leads the Secure AI Project, which focuses on developing pragmatic policies to reduce severe harms from advanced AI.[^4]

Beckstead also has deep roots in the effective altruism movement's organizational history. As a graduate student, he co-founded the first US chapter of Giving What We Can and was one of three founding trustees of the <EntityLink id="E517">Centre for Effective Altruism</EntityLink>.[^5] These early contributions earned him recognition from EA co-founders as a key figure in establishing the movement's institutional infrastructure in the United States.

## Education and Early Career

Beckstead completed a bachelor's degree in mathematics and philosophy at the University of Minnesota before pursuing a PhD in philosophy at Rutgers University, which he completed in 2013.[^1] During his graduate studies, he became involved with the effective altruism movement early in its formation—he joined Giving What We Can shortly after its 2009 launch and went on to co-found the organization's first US chapter at Rutgers, pledging to donate half of his post-tax income until retirement to cost-effective global poverty organizations.[^5]

His early community-building contributions extended beyond Giving What We Can. He helped organize a GWWC talk at Rutgers that attracted over 500 attendees, served as GWWC's director of research, and was one of the three founding trustees of what would become the Centre for Effective Altruism. According to an EA Forum post, William MacAskill described him as essentially a co-founder of GWWC and credited him as crucial to bridging GiveWell and Open Philanthropy with the broader EA movement.[^5]

Following his PhD, Beckstead was a Research Fellow at the <EntityLink id="E140">Future of Humanity Institute</EntityLink> at Oxford University, where he worked on long-term priorities in effective altruism and existential risk. He held this position from June 2013 until November 2014.[^6]

## Research and Publications

### PhD Dissertation

Beckstead's central academic contribution is his 2013 dissertation, *On the Overwhelming Importance of Shaping the Far Future*. The core argument holds that, from a global perspective, what matters most in expectation is doing what is best for the general trajectory along which humanity develops over millions of years or longer.[^7] This framing implies that existential risk reduction and trajectory changes—interventions that alter the long-run course of civilization—can dwarf proximate benefits such as disease treatment or poverty alleviation in terms of expected moral value.

The dissertation includes substantive chapters on empirical and normative defenses of longtermism, rebuttals to population ethics objections (including critiques of person-affecting views and diminishing marginal value arguments), and early discussion of decision theory under extreme uncertainty. Beckstead distinguishes between broad far-future shaping—general interventions robust across many scenarios—and more targeted interventions, arguing for a preference toward the former while acknowledging significant uncertainty.[^8]

### Other Academic Work

In collaboration with Toby Ord, Beckstead co-authored "Managing Existential Risk From Emerging Technologies," published in the UK Government Chief Scientific Adviser's annual report *Innovation: Managing Risk, Not Avoiding It* (2014).[^7] He also published "How much could refuges help us recover from a global catastrophe?" in the journal *Futures* in 2015, examining the potential role of isolated refuges in civilizational recovery from catastrophic events.[^7]

A more recent academic contribution, co-authored with philosopher Teruji Thomas, is "A Paradox for Tiny Probabilities and Enormous Values," which appeared as a Global Priorities Institute working paper in 2020 and was later published in the journal *Noûs* in 2023–2024.[^9] The paper argues that every theory of uncertain prospects faces at least one of three unpalatable properties: timidity, recklessness, or problematic trade-offs. This work draws on a chapter from Beckstead's 2013 dissertation and has implications for decision theory, axiology, and utilitarian ethics more broadly.[^9]

During his tenure at <EntityLink id="E552">Open Philanthropy</EntityLink>, Beckstead's output shifted primarily toward grantmaking and management rather than peer-reviewed publications, though he produced a number of web pages and reports on topics including animal product alternatives, potential risks from advanced AI, mechanisms of aging, and the long-term significance of reducing global catastrophic risks.[^7]

## Career at Open Philanthropy

Beckstead joined <EntityLink id="E552">Open Philanthropy</EntityLink> as one of its earliest employees at the end of 2014.[^3] As a Program Officer, he oversaw a broad portfolio of grantmaking spanning several cause areas. Notable grants associated with his oversight included support for Target Malaria (a gene drive project for malaria control), Impossible Foods (animal product alternatives), and Ed Boyden's laboratory at MIT.[^3] His portfolio also encompassed EA community support and existential risk reduction, reflecting his philosophical background.

In an EA Global 2017 talk, Beckstead described his approach to community grantmaking as oriented toward empowering talented individuals in career choices rather than scaling a mass movement, on the grounds that EA had succeeded in attracting funds and that the marginal need was for high-quality intellectual contributions and responsive support for emerging projects rather than additional fundraising infrastructure.[^10]

He remained at Open Philanthropy until approximately 2021, when he transitioned to lead the FTX Foundation and FTX Future Fund.[^1]

## FTX Future Fund and Subsequent Work

Beckstead joined the FTX Foundation as its CEO in November 2021 and served as the leader of the FTX Future Fund, which made grants focused on longtermist and EA-aligned causes.[^1] He resigned from both roles in November 2022 following the collapse of FTX.[^1]

After leaving FTX, Beckstead took on a role as Policy Lead at the <EntityLink id="E47">CAIS</EntityLink> (Center for AI Safety) and undertook various AI safety and governance consulting projects.[^4] He subsequently co-founded the Secure AI Project, where he serves as CEO. According to the organization's description, the Secure AI Project develops and advocates for pragmatic policies to reduce risks of severe harm from advanced AI.[^4] Commentator Zvi Mowshowitz has described the organization's work favorably, noting a private track record that includes improving safety practices at a major AI lab and characterizing the project as able to generate substantial impact relative to its funding.[^11]

Beckstead also stepped down from the boards of Effective Ventures UK and Effective Ventures US on August 23, 2023.[^12]

## Philosophical Views

Beckstead's philosophical commitments center on longtermism and the moral significance of future generations. In discussions documented on the EA Forum and in podcast appearances, he has advocated for weighting future generations roughly equivalently to the present, and for prioritizing causes that are neglected and where additional resources can significantly bend humanity's long-run trajectory.[^8]

On population ethics, Beckstead has engaged critically with strict person-affecting views—the position that only identifiable individuals' welfare matters morally—arguing these views generate counterintuitive implications when applied to scenarios involving potential future people. He favors approaches that aggregate across moral frameworks under uncertainty rather than committing to a single theory.[^8]

Beckstead has also discussed the practical implications of moral uncertainty for cause selection. He has expressed support for improving collective judgment through mechanisms like forecasting tournaments and prediction markets, citing Phil Tetlock's research as an example of empirically grounded progress on these questions.[^8]

On AI development strategy specifically, Beckstead has argued that racing to build AI capabilities without a viable alignment solution in place is strategically unsound, even if the developers have pro-safety intentions.[^13] He has also noted the challenge of gaining mainstream AI researcher buy-in for safety-focused approaches, attributing part of this friction to differences in research culture between machine learning (which emphasizes empirical, code-based demonstrations) and some AI safety methodologies.[^14]

## Criticisms and Controversies

### Association with FTX and Sam Bankman-Fried

The most significant controversy surrounding Beckstead concerns his role in leading the FTX Future Fund despite prior warnings about Sam Bankman-Fried's conduct. Reporting in *Time* magazine described how multiple EA leaders, including Beckstead, were warned as early as 2018–2019 about concerns regarding Bankman-Fried's trustworthiness, including allegations of lying and problematic business practices at Alameda Research.[^15] Despite these warnings, Beckstead joined the FTX Future Fund as its leader in 2021. Critics have argued that EA leadership, including Beckstead, prioritized FTX's philanthropic resources over adequate due diligence on Bankman-Fried's ethics.

Following the FTX collapse, Beckstead recused himself from Effective Ventures board matters related to FTX but remained on both EV UK and EV US boards for over nine months before departing in August 2023.[^12] Some observers described this prolonged tenure as problematic given the circumstances of his departure from FTX.

### Longtermism and Prioritization of Rich Countries

A passage from Beckstead's 2013 PhD dissertation attracted public criticism: he wrote that it seemed more plausible to him that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal, due to the greater potential "ripple effects" from wealthier, more connected individuals.[^15] Critics in outlets including *Jacobin* and *Current Affairs* cited this as evidence of elitism embedded in longtermist thinking, framing it as a form of trickle-down ideology that deprioritizes existing poor populations in favor of speculative future benefits.[^16]

Beckstead's broader longtermist framework has been critiqued by philosophers and journalists as potentially justifying neglect of present harms in pursuit of speculative long-run gains, with some critics characterizing longtermism as a dangerous ideological framework when operationalized at scale.[^17]

### Alleged Research Suppression

Philosopher Simon Knutsson has reported that managers associated with the Effective Altruism Foundation (EAF), in cooperation with Beckstead in his capacity as a grant investigator and CEA trustee, pressured him to modify his paper "The World Destruction Argument"—which critiqued views associated with Beckstead and others—in order to conform to EA communication guidelines, or risk losing funding. Knutsson's account includes claims that this involved outreach to his academic supervisor and that it reflected an attempt to suppress pessimistic ethics research.[^18] The nature and extent of Beckstead's direct involvement in these communications is not fully documented in publicly available sources.

## Key Uncertainties

- The full scope of Beckstead's decision-making authority and situational awareness regarding FTX prior to its collapse remains unclear from public sources.
- The degree to which the Secure AI Project's claimed policy impacts are independently verifiable is difficult to assess; accounts of its effectiveness are primarily from affiliated commentators.
- Beckstead's precise views on longtermist prioritization have evolved since the 2013 dissertation; public statements suggest he has moderated some positions, but the extent of revision is not fully documented.

## Sources

[^1]: [EA Forum – Nick Beckstead topic page](https://forum.effectivealtruism.org/topics/nick-beckstead)
[^2]: [Nick Beckstead – Research page](https://sites.google.com/site/nbeckstead/research)
[^3]: [80,000 Hours Podcast – Nick Beckstead on giving billions](https://80000hours.org/podcast/episodes/nick-beckstead-giving-billions/)
[^4]: [Nick Beckstead – Personal website](https://www.nickbeckstead.com)
[^5]: [EA Forum – Nick Beckstead is leaving the Effective Ventures boards](https://forum.effectivealtruism.org/posts/Defu3jkejb7pmLjeN/nick-beckstead-is-leaving-the-effective-ventures-boards)
[^6]: [Timeline of Future of Humanity Institute – Issa Rice](https://timelines.issarice.com/wiki/Timeline_of_Future_of_Humanity_Institute)
[^7]: [Nick Beckstead – Research publications list](https://sites.google.com/site/nbeckstead/research)
[^8]: [80,000 Hours Podcast – Nick Beckstead on giving billions](https://80000hours.org/podcast/episodes/nick-beckstead-giving-billions/)
[^9]: [Global Priorities Institute – A Paradox for Tiny Probabilities and Enormous Values (working paper)](https://www.globalprioritiesinstitute.org/wp-content/uploads/Nick-Beckstead-and-Teruji-Thomas_A-paradox-for-tiny-probabilities-and-enormous-values.pdf)
[^10]: [EA Forum – Nick Beckstead: EA Community Building (2017 talk)](https://forum.effectivealtruism.org/posts/xwDG64qAjGjQeXwfd/nick-beckstead-ea-community-building)
[^11]: [Zvi Mowshowitz – The Big Nonprofits Post 2025](https://thezvi.substack.com/p/the-big-nonprofits-post-2025)
[^12]: [EA Forum – Nick Beckstead is leaving the Effective Ventures boards](https://forum.effectivealtruism.org/posts/Defu3jkejb7pmLjeN/nick-beckstead-is-leaving-the-effective-ventures-boards)
[^13]: [AI Alignment Forum – Let's think about slowing down AI](https://www.alignmentforum.org/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai)
[^14]: [EA Global 2018 – Nick Beckstead fireside chat](https://www.effectivealtruism.org/articles/ea-global-2018-beckstead-fireside-chat)
[^15]: [Time Magazine – Sam Bankman-Fried, Effective Altruism, Alameda, FTX](https://time.com/6262810/sam-bankman-fried-effective-altruism-alameda-ftx/)
[^16]: [Jacobin – Effective Altruism, Longtermism, Nick Bostrom, racism](https://jacobin.com/2023/01/effective-altruism-longtermism-nick-bostrom-racism)
[^17]: [Aeon – Why longtermism is the world's most dangerous secular credo](https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo)
[^18]: [Simon Knutsson – Problems in Effective Altruism and Existential Risk](https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/)
