---
title: "Sam Altman"
description: "CEO of OpenAI since 2019, former Y Combinator president, and central figure in AI development. Co-founded OpenAI in 2015, survived November 2023 board crisis, and advocates for gradual AI deployment while acknowledging existential risks. Key player in debates over AI safety, commercialization, and governance."
sidebar:
  order: 1
quality: 40
llmSummary: "Comprehensive biographical profile of Sam Altman documenting his role as OpenAI CEO, timeline predictions (AGI within presidential term, superintelligence in \"few thousand days\"), and controversies including November 2023 board crisis and safety team departures. Includes detailed 'Statements & Track Record' section analyzing prediction accuracy—noting pattern of directional correctness on AI trajectory but consistent overoptimism on specific timelines, plus tension between safety rhetoric and deployment practices."
lastEdited: "2026-02-01"
importance: 22
update_frequency: 7
ratings:
  novelty: 3
  rigor: 5
  actionability: 2
  completeness: 7
clusters: ["ai-safety","governance"]
entityType: person
---
import {Mermaid, DataExternalLinks, EntityLink, F} from '@components/wiki';

<DataExternalLinks pageId="sam-altman" />

## Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| **Role** | CEO of <EntityLink id="openai">OpenAI</EntityLink> | Leading developer of GPT-4, ChatGPT, and frontier AI systems |
| **Influence Level** | Very High | Oversees company valued at <F e="openai" f="valuation-2025" />; ChatGPT reached 100M users faster than any product in history |
| **AI Safety Stance** | Moderate/Pragmatic | Signed extinction risk statement; advocates gradual deployment; criticized by safety researchers for prioritizing capabilities |
| **Timeline Views** | Near-term AGI | "AGI will probably get developed during this president's term" (2024); "superintelligence in a few thousand days" |
| **Regulatory Position** | Pro-regulation | Called for licensing agency in Senate testimony; supports "thoughtful" government oversight |
| **Key Controversy** | November 2023 Firing | Board cited lack of candor; reinstated after 95% of employees threatened to quit |
| **Net Worth** | ≈\$2.8 billion | From venture investments (Reddit, Stripe, Helion); holds no OpenAI equity |
| **Other Ventures** | Worldcoin, Helion, Oklo | Eye-scanning crypto project; nuclear fusion; nuclear fission |

## Key Links

| Source | Link |
|--------|------|
| Official Website | [openai.com](https://openai.com) |
| Wikipedia | [en.wikipedia.org](https://en.wikipedia.org/wiki/Sam_Altman) |
| Britannica | [britannica.com](https://www.britannica.com/money/Sam-Altman) |

## Personal Details

| Attribute | Details |
|-----------|---------|
| **Full Name** | Samuel Harris Altman |
| **Born** | April 22, 1985, Chicago, Illinois |
| **Education** | Stanford University (dropped out after 2 years); computer science |
| **Spouse** | Oliver Mulherin (married January 2024) |
| **Children** | One child (born February 2025) |
| **Residence** | San Francisco, California |
| **Net Worth** | ≈\$2.8 billion (primarily venture investments) |
| **OpenAI Salary** | \$76,001/year (holds no equity) |
| **Wikipedia** | [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) |

## Overview

Sam Altman is the CEO of <EntityLink id="openai">OpenAI</EntityLink>, the artificial intelligence company behind ChatGPT, GPT-4, and DALL-E. He has become one of the most influential figures in AI development, navigating the company through its transformation from a nonprofit research lab to a commercial powerhouse valued at over <F e="openai" f="valuation-2025" />. His leadership has been marked by both remarkable commercial success and significant controversy, including his brief firing and rapid reinstatement in November 2023.

Altman's career before OpenAI established him as a prominent Silicon Valley figure. He co-founded the location-based social network Loopt at age 19, became president of Y Combinator at 28, and helped fund hundreds of startups including Airbnb, Stripe, Reddit, and DoorDash. His transition to full-time OpenAI leadership in 2019 marked a pivot from startup investing to direct involvement in AI development.

His positions on AI risk occupy a complex middle ground. He has signed statements declaring AI an extinction-level threat alongside nuclear war, while simultaneously racing to deploy increasingly powerful systems. This tension between acknowledging catastrophic risks and accelerating capabilities development has made him a controversial figure in AI safety debates. Critics argue his warnings are performative while his actions prioritize commercial success over safety; supporters contend his gradual deployment philosophy represents the most realistic path to beneficial AI.

## Career Timeline

| Year | Event | Details |
|------|-------|---------|
| 1985 | Born | April 22, Chicago, Illinois; raised in St. Louis, Missouri |
| ≈1993 | First computer | Received at age 8; attended John Burroughs School |
| 2003 | Stanford | Enrolled to study computer science |
| 2005 | Loopt founded | Co-founded location-based social network at age 19; Y Combinator's first batch |
| 2005 | Stanford dropout | Left after 2 years to focus on Loopt |
| 2011 | Y Combinator | Became part-time partner at YC |
| 2012 | Loopt acquired | Sold to Green Dot Corporation for \$43 million |
| 2012 | Hydrazine Capital | Co-founded venture fund with brother Jack; \$21 million initial fund |
| 2014 | YC President | Became president of Y Combinator, succeeding Paul Graham |
| 2015 | OpenAI co-founded | Co-founded with Elon Musk, Greg Brockman, Ilya Sutskever, and others |
| 2015 | YC Continuity | Launched \$700 million equity fund for maturing YC companies |
| 2018 | Musk departure | Elon Musk resigned from OpenAI board |
| 2019 | OpenAI CEO | Left Y Combinator to become full-time OpenAI CEO |
| 2019 | Tools for Humanity | Co-founded Worldcoin parent company |
| 2022 | ChatGPT launch | November release; 100 million users in 2 months |
| 2023 | Senate testimony | May 16; called for AI licensing agency |
| 2023 | Board crisis | November 17-22; fired and reinstated within 5 days |
| 2024 | Marriage | January 24; married Oliver Mulherin in Hawaii |
| 2024 | Restructuring begins | September; plans announced to convert to for-profit |
| 2025 | Child born | February 2025; first child with husband |
| 2025 | OpenAI PBC | October; OpenAI restructured as public benefit corporation |

## Pre-OpenAI Career

### Loopt (2005-2012)

| Aspect | Details |
|--------|---------|
| **Role** | Co-founder, CEO |
| **Product** | Location-based social networking mobile app |
| **Funding** | Raised \$30+ million in venture capital |
| **Y Combinator** | One of first 8 companies in YC's inaugural batch (2005) |
| **Initial YC Investment** | \$6,000 per founder |
| **Partnerships** | Sprint, AT&T, other wireless carriers |
| **Outcome** | Failed to achieve user traction; acquired for \$43 million |
| **Acquirer** | Green Dot Corporation (March 2012) |

Loopt was Altman's first significant venture, founded when he was 19 and still a Stanford undergraduate. The app allowed users to share their location with friends, a concept that was early to the market but failed to gain widespread adoption. Despite partnerships with major carriers and significant venture funding, the company never achieved product-market fit.

### Y Combinator (2011-2019)

| Aspect | Details |
|--------|---------|
| **Role** | Partner (2011), President (2014-2019) |
| **Predecessor** | Paul Graham (co-founder) |
| **Companies Funded** | ≈1,900 during tenure |
| **Notable Companies** | Airbnb, Stripe, Reddit, DoorDash, Instacart, Twitch, Dropbox |
| **YC Continuity** | Founded \$700 million growth fund (2015) |
| **YC Research** | Founded nonprofit research lab; contributed \$10 million |
| **Goal** | Aimed to fund 1,000 companies per year |

Under Altman's leadership, Y Combinator expanded dramatically. He broadened the types of companies funded to include "hard technology" startups in areas like nuclear energy, biotechnology, and aerospace. By the time he departed in 2019, YC had become the most prestigious startup accelerator globally.

### Hydrazine Capital (2012-present)

| Aspect | Details |
|--------|---------|
| **Co-founder** | Jack Altman (brother) |
| **Initial Fund** | \$21 million |
| **Major Backer** | Peter Thiel (largest contributor) |
| **Portfolio** | 400+ companies |
| **Strategy** | 75% allocated to Y Combinator companies |
| **Notable Returns** | Reddit (9% stake pre-IPO, ≈\$1.4B value); Stripe (\$15K for 2% in 2009) |

Hydrazine Capital became a major source of Altman's personal wealth. His early bet on Stripe in 2009, paying \$15,000 for a 2% stake, grew to be worth hundreds of millions as Stripe's valuation reached \$65 billion.

## OpenAI Founding and Evolution

### The Founding (2015)

<Mermaid chart={`
flowchart TD
    subgraph 2015["2015: Founding"]
        CONCERN[Concern about Google/DeepMind dominance] --> MEETING[Altman-Musk dinner meetings]
        MEETING --> PROPOSAL[Altman proposes AI Manhattan Project]
        PROPOSAL --> FOUNDING[OpenAI founded as nonprofit]
        FOUNDING --> PLEDGE[\$1B pledge from founders]
    end

    subgraph 2018["2018: Musk Exit"]
        PLEDGE --> TENSION[Tensions over control]
        TENSION --> MUSKOUT[Musk resigns from board]
        MUSKOUT --> FORPROFIT[Capped-profit subsidiary created 2019]
    end

    subgraph 2022["2022-2023: Commercial Success"]
        FORPROFIT --> CHATGPT[ChatGPT launch Nov 2022]
        CHATGPT --> MSFT[\$13B Microsoft investment]
        MSFT --> CRISIS[Board crisis Nov 2023]
    end

    subgraph 2024["2024-2025: Restructuring"]
        CRISIS --> RETURN[Altman returns as CEO]
        RETURN --> RESTRUCTURE[For-profit conversion]
        RESTRUCTURE --> PBC[Public benefit corporation Oct 2025]
    end

    style CONCERN fill:#ffcccc
    style CRISIS fill:#ffcccc
    style CHATGPT fill:#ccffcc
    style PBC fill:#ccffcc
`} />

OpenAI emerged from Altman and Musk's shared concerns about the concentration of AI capabilities at Google following its 2014 acquisition of <EntityLink id="deepmind">DeepMind</EntityLink>. In March 2015, Altman emailed Musk with a proposal for a "Manhattan Project" for AI under Y Combinator's umbrella. The two co-chairs recruited a founding team including <EntityLink id="ilya-sutskever">Ilya Sutskever</EntityLink>, Greg Brockman, and others.

The organization was structured as a nonprofit with a stated mission to ensure artificial general intelligence benefits "all of humanity." Co-founders pledged \$1 billion, though actual donations fell far short; by 2019, only \$130 million had been collected.

### Structural Evolution

| Period | Structure | Key Changes |
|--------|-----------|-------------|
| 2015-2019 | Nonprofit | Pure research focus; mission-driven |
| 2019 | Capped-profit LP | Created to attract talent and capital; returns capped at 100x |
| 2019-2024 | Nonprofit-controlled | Nonprofit board retained ultimate control |
| October 2025 | Public benefit corporation | For-profit with charitable foundation; removes profit caps |

The 2019 creation of the capped-profit subsidiary was justified as necessary to compete for talent and compute resources. Altman later explained: "Wary of the incentives of investors influencing AGI, OpenAI's leadership team developed a 'capped profit' subsidiary, which could raise funds for investors but would be governed by a nonprofit board."

### Microsoft Partnership

| Milestone | Date | Amount | Terms |
|-----------|------|--------|-------|
| Initial investment | 2019 | \$1 billion | Exclusive cloud partnership |
| Extended partnership | January 2023 | \$10 billion | Largest single AI investment |
| Total committed | 2023 | ≈\$13 billion | Microsoft receives 49% of profits until recouped |
| Current stake | October 2025 | ≈27% | Post-restructuring; valued at ≈\$135 billion |

The Microsoft relationship transformed OpenAI from a research lab into a commercial powerhouse. The partnership provided both capital and cloud infrastructure, enabling the training runs that produced GPT-4 and subsequent models. However, the relationship has also drawn criticism for potentially compromising OpenAI's independence and mission focus.

## November 2023 Board Crisis

### Timeline of Events

| Date | Event | Details |
|------|-------|---------|
| November 17 | Firing announced | Board stated Altman "not consistently candid"; Mira Murati named interim CEO |
| November 17 | Brockman resigns | Co-founder learned of firing moments before announcement; resigned same day |
| November 18-19 | Negotiations begin | Investors and employees press for reversal |
| November 20 | Microsoft offer | Satya Nadella announces Altman will lead new Microsoft AI team |
| November 20 | Employee letter | 738 of 770 employees sign letter threatening to quit |
| November 20 | Sutskever regret | Chief scientist publicly expresses regret for role in firing |
| November 20 | New interim CEO | Twitch co-founder Emmett Shear named interim CEO |
| November 21 | Board negotiations | Agreement reached for new board composition |
| November 22 | Reinstatement | Altman returns as CEO; new board: Bret Taylor (Chair), Larry Summers, Adam D'Angelo |

### Board's Stated Reasons

Former board member <EntityLink id="helen-toner">Helen Toner</EntityLink> later provided detailed explanations for the board's decision in a [TED AI Show interview](https://www.cnbc.com/2024/05/29/former-openai-board-member-explains-why-ceo-sam-altman-was-fired.html):

> "For years, Sam had made it very difficult for the board to actually do that job by withholding information, misrepresenting things that were happening at the company, in some cases outright lying to the board."

| Issue | Allegation | Source |
|-------|------------|--------|
| **ChatGPT launch** | Board not informed before November 2022 release; learned on Twitter | <EntityLink id="helen-toner">Helen Toner</EntityLink> interviews |
| **Startup fund ownership** | Altman did not disclose he owned the OpenAI startup fund while "constantly claiming to be an independent board member with no financial interest" | Board members |
| **Safety processes** | Provided "inaccurate information about the small number of formal safety processes that the company did have in place" | <EntityLink id="helen-toner">Helen Toner</EntityLink> |
| **Executive complaints** | Two executives reported "psychological abuse" with documentation | October 2023 board conversations |
| **Information withholding** | Pattern of "misrepresenting things" and "in some cases outright lying" | <EntityLink id="helen-toner">Helen Toner</EntityLink> |

**The Toner paper incident (October 2023):**
Toner published a paper that Altman perceived as critical of OpenAI while praising Anthropic's approach. According to Toner and The New Yorker:
- Altman reprimanded her, claiming the paper "could cause problems" due to an FTC investigation
- He then approached other board members to push for her removal
- "He'd play them off against each other by lying about what other people thought. Things like that had been happening for years." ([The New Yorker](https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai))
- Toner: "The problem was that after the paper came out Sam started lying to other board members in order to try and push me off the board"

This incident was occurring while the board was "already talking pretty seriously about whether we needed to fire him."

### Resolution and New Governance

The crisis resolved when 95% of OpenAI employees signed an open letter threatening to leave if the board didn't reinstate Altman. Microsoft's simultaneous offer to hire Altman and the entire OpenAI team created leverage that forced the board's capitulation.

The new board replaced the mission-focused nonprofit directors with business-oriented members:
- **Bret Taylor** (Chair): Former Salesforce co-CEO, Twitter chairman
- **Larry Summers**: Former Treasury Secretary, Harvard president
- **Adam D'Angelo**: Quora CEO (only remaining original board member)

This governance change represented a significant shift away from the safety-focused oversight that had originally prompted the firing.

### Analysis of the Crisis

The November 2023 crisis revealed several structural tensions in AI governance:

| Tension | Manifestation | Outcome |
|---------|---------------|---------|
| **Mission vs. Commercial** | Nonprofit board vs. \$90B valuation | Commercial interests prevailed |
| **Safety vs. Speed** | Board concerns vs. deployment pressure | Speed prioritized |
| **Oversight vs. CEO Power** | Board authority vs. employee loyalty | CEO power consolidated |
| **Investor vs. Public Interest** | Microsoft's stake vs. nonprofit mission | Investor interests protected |

The crisis demonstrated that traditional nonprofit governance mechanisms may be insufficient to constrain AI companies with significant commercial value. The threat of mass employee departure, combined with investor pressure, effectively nullified the board's oversight function.

## Views on AI Safety and Timelines

### AGI Timeline Predictions

| Statement | Date | Context |
|-----------|------|---------|
| "AGI will probably get developed during this president's term" | 2024 | Bloomberg Businessweek interview |
| "We may see the first AI agents join the workforce" in 2025 | January 2025 | Blog post "Reflections" |
| "Superintelligence in a few thousand days" | 2024 | OpenAI blog |
| "I think AGI will probably hit sooner than most people think and it will matter much less" | 2024 | NYT Dealbook summit |
| "We are now confident we know how to build AGI as we have traditionally understood it" | 2025 | Blog post "Reflections" |

Altman's timeline predictions have become progressively more aggressive. In 2024, he stated OpenAI is "beginning to turn our aim beyond [AGI], to superintelligence in the true sense of the word."

### On Existential Risk

Altman has made numerous statements acknowledging AI's potential for catastrophic harm:

> "The development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity."

> "AI will probably, most likely, sort of lead to the end of the world, but in the meantime, there will be great companies built." (2015 tech conference)

> "If this technology goes wrong, it can go quite wrong." (Senate testimony, May 2023)

> "The bad case... is like lights out for all of us." (Lex Fridman podcast)

In May 2023, Altman signed the Center for AI Safety statement declaring: "Mitigating the risk of extinction from A.I. should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."

### Gradual Deployment Philosophy

Altman advocates for iterative release as a safety strategy:

> "The best way to make an AI system safe is by iteratively and gradually releasing it into the world, giving society time to adapt and co-evolve with the technology, learning from experience, and continuing to make the technology safer."

> "A slower takeoff gives us more time to figure out empirically how to solve the safety problem and how to adapt."

> "The world I think we're heading to and the safest world, the one I most hope for, is the short timeline slow takeoff."

This philosophy has been criticized by those who argue that commercial pressures make genuine caution impossible, and that "gradual deployment" has in practice meant racing to release capabilities as fast as possible.

### Regulatory Positions

In his May 2023 Senate testimony, Altman proposed:

| Proposal | Details |
|----------|---------|
| **Licensing agency** | New U.S. or global body to license powerful AI systems |
| **Safety testing** | Mandatory testing before deployment of dangerous models |
| **Independent audits** | Third-party evaluation of AI systems |
| **International coordination** | Suggested IAEA as model for global AI governance |
| **Capability thresholds** | Regulation above certain capability levels |

However, critics note that OpenAI has continued to deploy increasingly powerful systems without waiting for such regulatory frameworks to be established.

### Evolution of Safety Rhetoric

Altman's public statements on AI risk have shifted over time:

| Period | Stance | Representative Quote |
|--------|--------|---------------------|
| 2015 | Maximally alarmed | "AI will probably, most likely, sort of lead to the end of the world" |
| 2019-2022 | Cautiously concerned | Emphasized gradual deployment and safety research |
| 2023 | Publicly advocating regulation | "If this technology goes wrong, it can go quite wrong" |
| 2024-2025 | Confident in approach | "We are now confident we know how to build AGI" |

This evolution tracks with OpenAI's commercial success and may reflect either genuine confidence in safety progress or the influence of commercial pressures on public messaging.

## Statements & Track Record

For a detailed analysis of Altman's predictions and their accuracy, see the **<EntityLink id="sam-altman-predictions">full track record page</EntityLink>**.

**Summary**: Directionally correct on AI trajectory; consistently overoptimistic on specific timelines; rhetoric has shifted from "existential threat" to "will matter less than people think."

| Category | Examples |
|----------|----------|
| ✅ **Correct** | AI needing massive capital, cost declines, legal/medical AI capability |
| ❌ **Wrong** | Self-driving (2015), ChatGPT Pro profitability, GPT-5 launch execution |
| ⏳ **Pending** | AGI by 2025-2029, "superintelligence in a few thousand days" |

**Notable tension**: His safety rhetoric ("greatest threat to humanity" in 2015; signed extinction risk statement in 2023) contrasts with aggressive deployment practices and later claims that "AGI will matter much less than people think."

## Communication Patterns and Consistency

As analyzed in ["What's Going on With OpenAI's Messaging?"](https://www.lesswrong.com/posts/cy99dCEiLyxDrMHBi/what-s-going-on-with-openai-s-messaging), OpenAI appears to make conflicting promises tailored to different audiences:

| Audience | Message |
|----------|---------|
| **Talent** | "We will reach transformative AI" |
| **Public/PR** | "We care deeply about safety" |
| **Regulators** | "We're not making anything dangerous" |
| **Investors** | "Safety won't require significant resources or hurt competitiveness" |

The recommendation from that analysis: "ignore the noise" and "look at actual actions taken alone."

### AI Risk Rhetoric Evolution

| Date | Statement | Context | Source |
|------|-----------|---------|--------|
| **2015** | "AI will probably, most likely, sort of lead to the end of the world" | Conference (tech/startup audience) | [Tom's Guide](https://www.tomsguide.com/ai/i-think-ai-will-probably-most-likely-lead-to-the-end-of-the-world-everyone-is-sharing-sam-altmans-doomsday-quote-but-almost-no-one-notices-the-date) |
| **2023** | "Worst-case scenario is lights out for all of us" | Podcast (general public) | [Lex Fridman Podcast](https://lexfridman.com/sam-altman-2-transcript/) |
| **May 2023** | Testified AI poses "existential risk" requiring oversight | Senate hearing (regulators) | [Senate Judiciary](https://www.judiciary.senate.gov/) |
| **2024-2025** | "AGI will probably hit sooner than most people think and it will matter much less" | Bloomberg interview (investors/business) | [Bloomberg](https://www.bloomberg.com/features/2025-sam-altman-interview/) |

**Assessment**: Major rhetoric shift. Early statements emphasized existential danger; recent statements minimize significance. Risk-minimizing language appeared after OpenAI's commercial pivot accelerated and during fundraising.

### Regulation Stance Evolution

| Date | Statement | Context | Source |
|------|-----------|---------|--------|
| **May 2023** | Testified AI needs government oversight; proposed federal licensing agency as "#1 recommendation" | Senate hearing | [Senate Judiciary](https://www.judiciary.senate.gov/) |
| **May 2023** | Simultaneously lobbied behind the scenes to weaken the EU AI Act | Private lobbying | [TIME Exclusive](https://time.com/6288245/openai-eu-lobbying-ai-act/) |
| **Aug 2024** | OpenAI officially opposed California SB 1047 | Corporate position | [Newsweek](https://www.newsweek.com/openai-workers-push-california-ai-bill-against-sam-altman-1952033) |
| **May 2025** | Said requiring government approval before releases would be "disastrous" | Senate testimony | [Fortune](https://fortune.com/2025/05/08/sam-altman-openai-senate-hearing-testimony-china-ai-regulations/) |

Former OpenAI researchers William Saunders and Daniel Kokotajlo wrote: "When actual regulation is on the table, he opposes it." ([Windows Central](https://www.windowscentral.com/software-apps/ex-openai-researchers-claim-sam-altmans-public-support-for-ai-regulation-is-a-facade-when-actual-regulation-is-on-the-table-he-opposes-it))

### Open Source Stance

| Date | Position | Context | Source |
|------|----------|---------|--------|
| **2015-2018** | OpenAI published research openly (GPT, GPT-2 initial release) | Founding era | [OpenAI blog archives](https://openai.com/research) |
| **2019-2023** | Shifted to closed-source; GPT-4 technical details not released, citing safety | Commercial pivot | [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) |
| **Jan 2025** | "I personally think we have been on the wrong side of history here and need to figure out a different open source strategy" | Reddit AMA, days after DeepSeek | [VentureBeat](https://venturebeat.com/ai/sam-altman-admits-openai-was-on-the-wrong-side-of-history-in-open-source-debate) |

The timing of the "wrong side of history" admission—coming days after DeepSeek's open-source R1 model caused Nvidia's stock to lose \$600 billion—suggests reactive positioning.

### Hype Cycles and Twitter Patterns

Altman's Twitter/X activity shows a consistent pattern: cryptic teasers, bold claims, hype accumulation, then partial walkbacks while retaining attention benefits. ([Riley Brown analysis](https://x.com/rileybrown_ai/status/1886478538271871473))

| Phase | Tactic | Effect |
|-------|--------|--------|
| **Teaser** | Cryptic tweets, vague hints about upcoming releases | Builds anticipation |
| **Claim** | Bold statements about capabilities (AGI, benchmarks) | Generates headlines |
| **Speculation** | Let rumors run without correction | Amplifies attention |
| **Walkback** | Blame "Twitter hype," claim misunderstanding | Maintains credibility |

**Examples include:**
- The "her" tweet during GPT-4o launch (May 2024) referencing Scarlett Johansson
- The o3/AGI hype cycle (December 2024), claiming it "passed the ARC-AGI challenge" then walking back with "pls chill and cut your expectations 100x"
- Cryptic singularity tweets (January 2025): "near the singularity; unclear which side"

### Patterns from Karen Hao's *Empire of AI*

| Pattern | Description | Source |
|---------|-------------|--------|
| **"Paper cuts"** | "He sometimes lied about details so insignificant that it was hard to say why the dishonesty mattered at all. But over time, those tiny 'paper cuts'... led to an atmosphere of pervasive distrust" | [LessWrong summary](https://www.lesswrong.com/posts/gm7Jovt93tSaToCgJ/sam-altman-s-track-record-of-manipulation-some-quotes-from) |
| **Mirroring** | "He's so good at adjusting to what you say, and you really feel like you're making progress with him. And then you realize over time that you're actually just running in place" | Same |
| **Opposing views** | "Listened carefully to people to understand what they wanted, then promised to deliver on those wants, only to do the same for others with opposing views" | [idratherbewriting review](https://idratherbewriting.com/blog/book-review-empire-of-ai-karen-hao) |

**Key observation from Hao**: "No one could definitively say what Sam actually believes, since he seems to align with opposing views depending on who he talks to."

### The 20% Compute Promise

| Date | Commitment/Event | Source |
|------|------------------|--------|
| **July 2023** | OpenAI announced Superalignment team, promised 20% of compute for AI safety work | [OpenAI announcement](https://openai.com/index/introducing-superalignment/) |
| **May 2024** | Jan Leike resigned, stated team had been "struggling for compute" | [Wired](https://www.wired.com/story/openai-superalignment-team-ilya-sutskever/) |
| **May 2024** | OpenAI confirmed Superalignment team "is no more" | Same |

### Commitments vs. Reality: "Planning for AGI and Beyond"

OpenAI's February 2023 policy document made several commitments:

| Commitment | What Was Said | What Happened | Status |
|------------|---------------|---------------|--------|
| **Nonprofit control** | "A nonprofit that governs us and lets us operate for the good of humanity" | Nonprofit reduced from 100% control to 26% equity stake | ❌ Walked back |
| **Profit caps** | Profits capped at 100x ROI, decreasing over time | Caps eliminated entirely in 2024 | ❌ Eliminated |
| **Gradual deployment** | "A gradual transition gives people, policymakers, and institutions time" | Deployed ChatGPT without board knowledge | ⚠️ Questionable |

**Note**: The original blog post now carries a disclaimer: "Updated October 28, 2025: This post contains outdated information about our structure."

### The \$7 Trillion Example

In February 2024, while publicly discussing AI risks and the need for careful development, Altman was simultaneously seeking up to \$7 trillion for AI chip infrastructure—an amount larger than the GDP of Germany and 13x the entire global chip industry's 2023 revenues. ([WSJ via CNBC](https://www.cnbc.com/2024/02/09/openai-ceo-sam-altman-reportedly-seeking-trillions-of-dollars-for-ai-chip-project.html))

## Criticisms and Controversies

### Pre-OpenAI: Alleged Pattern

Helen Toner alleged that Altman's behavior at OpenAI followed a pattern from earlier roles:

| Role | Allegation | Source |
|------|------------|--------|
| **Loopt** (CEO, 2005-2012) | "The management team went to the board there twice and asked the board to fire him for what they called deceptive and chaotic behavior" | [Toner on TED AI Show](https://www.benzinga.com/news/24/05/39062331/former-openai-board-member-who-tried-to-oust-sam-altman-says-ceo-was-fired-from-y-combinator-and-ano) |
| **Y Combinator** (President, 2014-2019) | Toner claimed he "was actually fired from his previous job at Y Combinator, which was hushed up at the time" | Same |

**Disputed**: Paul Graham (YC co-founder) denied Altman was "fired," saying YC told him to choose between YC and OpenAI, and he chose OpenAI. The Washington Post reported he was "asked to leave" as "part of a pattern of clashes that some attribute to his self-serving approach." ([TechCrunch](https://techcrunch.com/2024/05/30/paul-graham-claims-altman-wasnt-fired-from-y-combinator/), [Washington Post](https://www.washingtonpost.com/technology/2023/11/22/sam-altman-fired-y-combinator-paul-graham/))

### Early OpenAI: Co-Founders' Trust Concerns (2017)

Leaked emails from the Musk lawsuit reveal OpenAI's own co-founders questioned Altman's motivations early on:

| Date | From | Quote | Source |
|------|------|-------|--------|
| **Sept 2017** | Sutskever & Brockman to Altman | "We haven't been able to fully trust your judgements throughout this process, because we don't understand your cost function... Is AGI truly your primary motivation?" | [Transformer News](https://www.transformernews.ai/p/openai-emails-altman-trust) |
| **Sept 2017** | Sutskever & Brockman | "You stated that you don't want to control the final AGI, but during this negotiation, you've shown to us that absolute control is extremely important to you" | Same |
| **Sept 2017** | Sutskever & Brockman | Expressed concern that Altman's proposed structure would allow him to become "a dictator in the company" | Same |

Altman "admitted that he lost a lot of trust with Greg and Ilya through this process."

**Other leaked email revelations:**
- Altman proposed an ICO (cryptocurrency offering) to fund OpenAI; Musk opposed it, warning it would "result in a massive loss of credibility" ([TechCrunch](https://techcrunch.com/2024/11/15/openais-tumultuous-early-years-revealed-in-emails-from-musk-altman-and-others/))
- Most of the announced \$1 billion in founding commitments never materialized; Musk provided nearly all early capital (\$38-44M by 2018)

### Founding Purpose vs. Current Direction

OpenAI was founded in 2015 explicitly as a "countervailing force" to prevent AI power concentration—specifically to counter Google/<EntityLink id="deepmind">DeepMind</EntityLink>:

| Source | Statement |
|--------|-----------|
| **Sutskever & Blackwell email** | Described OpenAI as a project to avoid an "AGI dictatorship" |
| **Daniel Kokotajlo** | "They wanted to create OpenAI to be this countervailing force... not concentrate power so much on Demis' hands" |
| **Musk's stated motivation** | Counter Google's "reckless pursuit of AI advancements" |

OpenAI is now itself one of the largest concentrations of AI capability, having lost most founding members (many citing safety concerns).

### Safety Team Departures (2024)

| Person | Role | Departure | Reason |
|--------|------|-----------|--------|
| Ilya Sutskever | Co-founder, Chief Scientist | May 2024 | Resigned after board crisis involvement |
| <EntityLink id="jan-leike">Jan Leike</EntityLink> | Superalignment co-lead | May 2024 | Cited safety concerns; said compute was deprioritized |
| Leopold Aschenbrenner | Safety researcher | 2024 | Allegedly fired for sharing safety document externally |
| Mira Murati | CTO | September 2024 | Announced departure after return to role post-crisis |

The departure of key safety personnel raised questions about OpenAI's commitment to alignment research. Jan Leike stated publicly that OpenAI had deprioritized safety work in favor of "shiny products."

### Equity Clawback Contradiction (May 2024)

| Date | Event | Source |
|------|-------|--------|
| **April 2023** | Altman signed documents authorizing equity clawback provisions that could strip departing employees of vested equity if they criticized OpenAI | [Vox](https://gizmodo.com/openai-sam-altman-chatgpt-employee-dissent-ndas-1851496211) |
| **May 17, 2024** | Vox reports on the existence of these provisions | Same |
| **May 18, 2024** | Altman tweets: "this is on me and one of the few times I've been genuinely embarrassed running OpenAI; **I did not know this was happening** and I should have" | [X/Twitter](https://x.com/sama/status/1791936857594581428) |
| **May 22, 2024** | Leaked documents emerge showing Altman's signature (along with Jason Kwon and Brad Lightcap) on documents explicitly authorizing the provisions | [The Deep Dive](https://thedeepdive.ca/sam-altman-claims-ignorance-on-equity-clawbacks-but-data-contradicts/) |

**The contradiction**: On May 18, Altman tweeted that he "did not know this was happening." Four days later, documents emerged showing he had personally signed the authorization in April 2023. Former employee Daniel Kokotajlo stated that refusing to sign the NDA cost him equity worth about 85% of his family's net worth. Altman later stated the provision would be removed.

### Scarlett Johansson Voice Controversy (May 2024)

| Date | Event | Source |
|------|-------|--------|
| **Sept 2023** | Altman contacted Johansson to voice ChatGPT; she declined | [Fortune](https://fortune.com/2024/05/20/scarlett-johansson-chatgpt-sky-voice-lawyer-sam-altman-openai-her/) |
| **May 11, 2024** | Two days before demo, Altman contacts Johansson's agent asking her to reconsider | Same |
| **May 13, 2024** | GPT-4o demo launches with "Sky" voice; Altman tweets single word: "her" | [CNBC](https://www.cnbc.com/2024/05/20/scarlett-johansson-says-openai-ripped-off-her-voice-.html) |
| **May 20, 2024** | Johansson: "shocked, angered and in disbelief"; says Altman "insinuated that the similarity was intentional" with his tweet | Same |
| **May 30, 2024** | Altman: "It's not her voice, it's not supposed to be. I'm sorry for the confusion" | [Hollywood Reporter](https://www.hollywoodreporter.com/business/business-news/sam-altman-comments-scarlett-johansson-ai-voice-controversy-1235911854/) |

Altman has publicly described *Her* as his favorite movie and said it "certainly more than a little bit inspired" OpenAI ([BGR](https://www.bgr.com/entertainment/sam-altmans-lawyers-probably-wish-this-video-clip-of-him-raving-about-her-didnt-exist/)). The timeline—contact for permission → denial → proceed anyway → coy tweet → walkback—led Johansson to say Altman "would make a good Marvel villain." ([Fortune](https://fortune.com/2024/07/15/scarlett-johansson-says-sam-altman-would-make-good-marvel-villain/))

### Worldcoin Privacy Concerns

Altman's Worldcoin project (now "World") has faced regulatory action in multiple jurisdictions:

| Jurisdiction | Action | Issue |
|--------------|--------|-------|
| Spain | Suspended operations | Data protection concerns |
| Argentina | Fines issued | Data terms violations |
| Kenya | Criminal investigation, halt | Biometric data collection |
| Hong Kong | Ordered to cease | "Excessive and unnecessary" data collection |

### 2025 Business Challenges

In late 2025, OpenAI faced significant headwinds that tested Altman's leadership:

| Challenge | Details | Response |
|-----------|---------|----------|
| **Market share decline** | ChatGPT visits fell below 6B monthly; second decline in 2025 | "Code red" memo issued |
| **Enterprise competition** | Market share dropped to 27%; Anthropic led at 40% | Refocused on enterprise features |
| **Cash burn** | ≈\$8 billion burned in 2025 | Plans to introduce advertising |
| **Revenue delays** | Agentic systems, e-commerce postponed | "Rough vibes" warning to employees |
| **Suicide lawsuit** | Family sued after teen's death involving ChatGPT | Altman expressed it weighs on him heavily |

Altman described advertising as OpenAI's "last resort" but acknowledged the company would pursue it given financial pressures.

### The OpenAI Files (June 2025)

["The OpenAI Files"](https://www.openaifiles.org/), compiled by nonprofit watchdog organizations (Midas Project and Tech Oversight Project), aggregates documentation of leadership and governance issues. Key findings:

- Dario and Daniela Amodei (former VP of Research and VP of Safety) described Altman's tactics as "gaslighting" and "psychological abuse" after leaving to found <EntityLink id="anthropic">Anthropic</EntityLink>, according to Karen Hao's book *Empire of AI* ([Fortune](https://fortune.com/2025/06/20/openai-files-sam-altman-leadership-concerns-safety-failures-ai-lab/))
- The WilmerHale law firm conducted an independent review after Altman's firing. Board member Larry Summers privately told people the investigation found "many instances of Altman saying different things to different people" — though the board decided this didn't preclude him from continuing as CEO ([OpenAI Files](https://www.openaifiles.org/ceo-integrity))
- **Sutskever's documentation**: Chief Scientist <EntityLink id="ilya-sutskever">Ilya Sutskever</EntityLink> provided the board with a self-destructing PDF containing Slack screenshots documenting "dozens of examples of lying or other toxic behavior." In his deposition, Sutskever revealed he wrote a 52-page brief stating: "Sam exhibits a consistent pattern of lying, undermining his execs, and pitting his execs against one another." When asked how long he'd considered firing Altman, Sutskever answered: "At least a year." ([Decrypt](https://decrypt.co/347349/inside-deposition-showed-openai-nearly-destroyed-itself))
- Former CTO Mira Murati told staffers she didn't feel "comfortable about Sam leading us to AGI." Sutskever said: "I don't think Sam is the guy who should have the finger on the button for AGI."

**Note**: The OpenAI Files states it "received no funding, editorial direction, assistance, or support of any kind from Elon Musk, xAI, Anthropic, Meta, Google, Microsoft, or any other OpenAI competitor."

### Relationship with Elon Musk

The Altman-Musk relationship has deteriorated from co-founding partnership to legal warfare:

| Period | Relationship Status | Key Events |
|--------|--------------------| ------------|
| 2015 | Close allies | Co-founded OpenAI after dinner meetings about AI risk |
| 2017 | Tensions emerge | Musk complained about nonprofit direction |
| 2017 | Control dispute | Musk requested majority equity, CEO position; rejected |
| 2018 | Departure | Musk resigned from board; told team "probability of success was zero" |
| 2023 | Open hostility | Musk mocked Altman firing as "OpenAI Telenovela" |
| February 2024 | First lawsuit | Musk sued alleging breach of founding agreement |
| August 2024 | Expanded lawsuit | Accused OpenAI of racketeering; claimed \$134.5B in damages |
| February 2025 | Buyout attempt | Musk consortium offered \$97.4B; rejected by board |
| April 2025 | OpenAI countersues | Accused Musk of harassment, acting for personal benefit |

The Musk-Altman conflict represents more than personal animosity; it reflects fundamental disagreements about AI governance, the role of profit in AI development, and who should control transformative technology. OpenAI has published internal emails showing Musk originally supported the for-profit transition, while Musk argues the current structure betrays the nonprofit mission he helped establish.

## Other Ventures

### Tools for Humanity / Worldcoin

| Aspect | Details |
|--------|---------|
| **Founded** | 2019 |
| **Role** | Chairman |
| **Product** | Iris-scanning cryptocurrency verification |
| **Technology** | "Orb" scans iris to create unique "IrisCode" |
| **Token** | WLD cryptocurrency |
| **Users** | 26 million on network; 12 million verified |
| **Funding** | ≈\$200 million from Blockchain Capital, Bain Capital Crypto, a16z |
| **US Launch** | April 30, 2025 (Austin, Atlanta, LA, Nashville, Miami, San Francisco) |
| **Goal** | Universal verification of humanity; potential UBI distribution |

Altman envisions Worldcoin as both proof-of-humanity infrastructure for an AI-saturated world and potentially a mechanism for universal basic income distribution.

### Energy Investments

| Company | Type | Investment | Role |
|---------|------|------------|------|
| Helion Energy | Nuclear fusion | \$375 million personal investment | Chairman |
| Oklo Inc. | Nuclear fission | Significant stake | Chairman |

Altman has been outspoken about AI's massive energy requirements, stating these investments aim to ensure sufficient clean energy for AI infrastructure.

### Other Investments

| Company | Sector | Details |
|---------|--------|---------|
| Reddit | Social media | 9% stake pre-IPO (≈\$1.4B value) |
| Stripe | Payments | \$15K for 2% in 2009 |
| Retro Biosciences | Longevity | \$180 million personal investment |
| Humane | AI hardware | Early investor |
| Boom Technology | Supersonic aviation | Investor |
| Cruise | Autonomous vehicles | Investor |

## 2024-2025 Corporate Restructuring

### Timeline

| Date | Development |
|------|-------------|
| September 2024 | Plans leaked: Altman to receive 7% equity; nonprofit control to end |
| December 2024 | Board announces public benefit corporation plan |
| May 2025 | Initial reversal: announced would remain nonprofit-controlled |
| October 2025 | Final restructuring completed as PBC |

### Final Structure

| Element | Details |
|---------|---------|
| **For-profit entity** | OpenAI Group PBC (public benefit corporation) |
| **Nonprofit entity** | OpenAI Foundation (oversight role) |
| **Foundation stake** | ≈26% of OpenAI Group (≈\$130B value) |
| **Microsoft stake** | ≈27% (≈\$135B value) |
| **Profit caps** | Removed; unlimited investor returns now possible |
| **Altman equity** | None (controversial decision not to grant equity) |
| **Foundation commitment** | \$25 billion for healthcare, disease research, AI resilience |
| **IPO plans** | Altman indicated "most likely path" but no timeline |

### AGI Definition Changes

Previously, the Microsoft partnership included a provision that Microsoft's access to OpenAI technology would terminate if OpenAI achieved AGI. Under the new terms, any AGI claims will be verified by an independent expert panel, preventing unilateral declarations.

## Public Assessment

### Supporters' View

| Argument | Evidence Cited |
|----------|----------------|
| Responsible leader | Called for regulation; signed extinction risk statement |
| Transparency advocate | Pushed for gradual deployment to build public familiarity |
| Mission-driven | Takes only \$76K salary; holds no equity |
| Effective executive | Built OpenAI from research lab to \$157B company |
| Realistic about safety | Acknowledges risks while arguing racing is unavoidable |

### Critics' View

| Argument | Evidence Cited |
|----------|----------------|
| Says safety, does capability | Safety team departures; compute deprioritized for products |
| Performative risk warnings | Warns of extinction while racing to deploy |
| Corporate capture | Transition from nonprofit to for-profit betrays founding mission |
| Governance failures | Board crisis revealed pattern of non-candor with oversight |
| Concentrating power | Restructuring removes safety-focused oversight |

### Center for AI Policy Assessment

The Center for AI Policy has been particularly critical:

> "A few years later, Musk left OpenAI, and Altman's interest in existential risk withered away. Once Altman had Musk's money, existential risk was no longer a top priority, and Altman could stop pretending to care about safety."

## Influence on AI Policy

Altman has become a significant voice in AI policy discussions globally:

### Congressional Engagement

| Date | Venue | Topic | Outcome |
|------|-------|-------|---------|
| May 2023 | Senate Judiciary Subcommittee | AI oversight | Called for licensing agency |
| 2023 | House dinner (60+ lawmakers) | ChatGPT demonstration | Built bipartisan relationships |
| 2024-2025 | Various committees | Ongoing testimony | Continued policy engagement |

### International Engagement

Altman has conducted world tours meeting with heads of state and regulators:

| Region | Key Engagements |
|--------|-----------------|
| **Europe** | Met with UK PM, French President; engaged with EU AI Act process |
| **Asia** | Japan, South Korea, Singapore government meetings |
| **Middle East** | UAE, Saudi Arabia discussions on AI investment |
| **Africa** | Kenya (related to Worldcoin operations) |

### Policy Positions Summary

| Issue | Altman's Position | Consistency |
|-------|-------------------|-------------|
| **Licensing for powerful AI** | Supports | Consistent since 2023 |
| **International coordination** | Supports IAEA-style body | Consistent |
| **Open-source frontier models** | Generally opposed | Shifted from early OpenAI stance |
| **Export controls** | Generally supports | Pragmatic alignment with US policy |
| **Compute governance** | Supports | Consistent |

## Key Uncertainties

| Uncertainty | Stakes | Current Trajectory |
|-------------|--------|-------------------|
| Does gradual deployment actually improve safety? | Whether commercial AI development can be made safe | Unclear; some evidence of adaptation, but capabilities accelerating |
| Will Altman's timeline predictions prove accurate? | Resource allocation, policy urgency | Becoming more aggressive; "few thousand days" to superintelligence |
| Can OpenAI maintain safety focus post-restructuring? | Whether commercial pressures overwhelm mission | Concerning; safety team departures, governance changes |
| Will regulatory frameworks emerge in time? | Government capacity to oversee AI | Slow progress despite Altman's calls for regulation |
| How will Musk litigation affect OpenAI? | Corporate stability, public trust | Ongoing legal battles; \$134.5B damages claimed |

## Sources and Citations

### Primary Sources

| Type | Source | Content |
|------|--------|---------|
| Testimony | [Senate Judiciary Committee (May 2023)](https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-rules-for-artificial-intelligence) | AI regulation proposals |
| Blog | [Sam Altman's Blog](https://blog.samaltman.com/) | "Reflections," "Three Observations" |
| Interviews | [Lex Fridman Podcast](https://www.lesswrong.com/posts/PTzsEQXkCfig9A6AS/transcript-of-sam-altman-s-interview-touching-on-ai-safety) | AI safety views transcript |
| Statement | [CAIS Extinction Risk Statement](https://www.safe.ai/statement-on-ai-risk) | Signed May 2023 |

### News Coverage

| Source | Coverage |
|--------|----------|
| [Wikipedia: Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) | Biography |
| [Wikipedia: Removal of Sam Altman](https://en.wikipedia.org/wiki/Removal_of_Sam_Altman_from_OpenAI) | November 2023 crisis |
| [TIME: OpenAI Timeline](https://time.com/7328674/openai-chatgpt-sam-altman-elon-musk-timeline/) | Corporate history |
| [CNN: AI Risk Taker](https://www.cnn.com/2023/10/31/tech/sam-altman-ai-risk-taker) | Risk acknowledgment while deploying |
| [Fortune: Altman Quotes](https://fortune.com/2023/06/08/sam-altman-openai-chatgpt-worries-15-quotes/) | Safety concerns statements |
| [CNBC: Board Explanation](https://www.cnbc.com/2024/05/29/former-openai-board-member-explains-why-ceo-sam-altman-was-fired.html) | Helen Toner interview |
| [TIME: Accusations Timeline](https://time.com/6986711/openai-sam-altman-accusations-controversies-timeline/) | Controversies overview |
| [TechCrunch: Worldcoin](https://techcrunch.com/2024/10/17/sam-altmans-worldcoin-becomes-world-and-shows-new-iris-scanning-orb-to-prove-your-humanity/) | World rebrand |
| [Bloomberg: Restructuring](https://www.bloomberg.com/news/articles/2025-10-29/openai-restructure-paves-way-for-ipo-and-ai-spending-spree) | Corporate changes |

### Analysis

| Source | Focus |
|--------|-------|
| [Center for AI Policy](https://www.centeraipolicy.org/work/sam-altmans-dangerous-and-unquenchable-craving-for-power) | Critical assessment |
| [Britannica Money](https://www.britannica.com/money/Sam-Altman) | Biography and facts |
| [OpenAI: Elon Musk](https://openai.com/index/openai-elon-musk/) | Musk relationship history |

## Related Entities

| Entity | Relationship |
|--------|--------------|
| <EntityLink id="openai">OpenAI</EntityLink> | CEO since 2019; co-founder 2015 |
| <EntityLink id="elon-musk">Elon Musk</EntityLink> | Former co-chair; now adversary |
| <EntityLink id="people/ilya-sutskever">Ilya Sutskever</EntityLink> | Co-founder; departed May 2024 |
| Greg Brockman | Co-founder; President |
| <EntityLink id="organizations/labs/microsoft">Microsoft</EntityLink> | Major investor (≈27% stake) |
| <EntityLink id="anthropic">Anthropic</EntityLink> | Competitor; founded by former OpenAI employees |

