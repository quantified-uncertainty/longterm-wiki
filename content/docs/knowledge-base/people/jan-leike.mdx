---
title: Jan Leike
description: Head of Alignment at Anthropic, formerly led OpenAI's superalignment team
sidebar:
  order: 4
quality: 27
llmSummary: Biography of Jan Leike covering his career from DeepMind through OpenAI's Superalignment team to his current role as Head of Alignment at Anthropic. Documents his research on RLHF and scalable oversight, his May 2024 departure from OpenAI citing safety prioritization concerns, and his current research priorities including weak-to-strong generalization and automated alignment techniques.
lastEdited: "2026-02-20"
readerImportance: 82
tacticalValue: 82
researchImportance: 39
update_frequency: 21
ratings:
  novelty: 2
  rigor: 3.5
  actionability: 2
  completeness: 5
clusters: ["ai-safety"]
entityType: person
---
import {DataInfoBox, DataExternalLinks, EntityLink} from '@components/wiki';

<DataExternalLinks pageId="jan-leike" />

<DataInfoBox entityId="E182" />

## Quick Assessment

| Aspect | Assessment |
|--------|-----------|
| **Primary Role** | Head of <EntityLink id="alignment">Alignment</EntityLink> at <EntityLink id="anthropic">Anthropic</EntityLink> (2024–present) |
| **Key Contributions** | Early development of <EntityLink id="rlhf">RLHF</EntityLink> techniques; research on <EntityLink id="scalable-oversight">scalable oversight</EntityLink> and <EntityLink id="weak-to-strong">weak-to-strong generalization</EntityLink>; co-led <EntityLink id="openai">OpenAI</EntityLink> Superalignment team |
| **Key Publications** | "Deep Reinforcement Learning from Human Preferences" (2017); "Scalable agent alignment via reward modeling" (2018); "Recursively Summarizing Books with Human Feedback" (2021) |
| **Career Trajectory** | PhD from Australian National University → <EntityLink id="deepmind">Google DeepMind</EntityLink> → OpenAI → Anthropic |
| **Notable Event** | Departed OpenAI in May 2024, posting publicly on X about safety prioritization concerns |

## Overview

Jan Leike is an AI alignment researcher who has held leadership positions at major AI laboratories. His work has focused on developing empirical methods for aligning AI systems with human values, particularly through reinforcement learning from human feedback. He completed a PhD at Australian National University, where he studied under Marcus Hutter, then worked on alignment problems at <EntityLink id="deepmind">Google DeepMind</EntityLink> before joining <EntityLink id="openai">OpenAI</EntityLink>. At OpenAI, he co-led the Superalignment team with <EntityLink id="ilya-sutskever">Ilya Sutskever</EntityLink> before departing in May 2024 and publicly stating concerns about safety prioritization. He currently serves as Head of <EntityLink id="alignment">Alignment</EntityLink> at <EntityLink id="anthropic">Anthropic</EntityLink>, where he continues research on alignment techniques for increasingly capable AI systems.

## Background

Jan Leike completed his PhD at Australian National University, where he studied AI safety under Marcus Hutter. His doctoral research addressed theoretical aspects of AI alignment and safe reinforcement learning.

His career has centered on developing practical, empirically-testable approaches to AI alignment:
- Research on safe exploration in reinforcement learning systems
- Development of methods for learning from human feedback
- Leadership of alignment research teams at multiple major AI laboratories
- Focus on techniques designed to work with contemporary machine learning paradigms

## Career Trajectory

### Google DeepMind

At <EntityLink id="deepmind">Google DeepMind</EntityLink>, Leike worked on early implementations of learning from human feedback, including:
- Methods for safe exploration in reinforcement learning environments
- <EntityLink id="reward-modeling">Reward modeling</EntityLink> techniques
- Approaches to scalable agent alignment

### OpenAI

Leike joined <EntityLink id="openai">OpenAI</EntityLink> to lead alignment research efforts. OpenAI subsequently announced the formation of the Superalignment team, which Leike co-led with <EntityLink id="ilya-sutskever">Ilya Sutskever</EntityLink>, with a stated mission of developing alignment techniques for superintelligent AI systems.

He departed from OpenAI in May 2024, following Sutskever's own departure from the organization.

### Anthropic (2024–present)

Leike joined <EntityLink id="anthropic">Anthropic</EntityLink> as Head of <EntityLink id="alignment">Alignment</EntityLink>, where he leads alignment research efforts. At Anthropic, he works alongside other researchers on alignment challenges related to the <EntityLink id="constitutional-ai">Claude</EntityLink> model family and future AI systems.

## Key Contributions

### RLHF Development

Leike was among the researchers who contributed to demonstrating that reinforcement learning from human feedback could be applied at scale:
- Co-authored research papers on reward learning from human preferences
- Developed techniques for training language models to exhibit helpful and harmless behaviors

The 2017 paper "Deep Reinforcement Learning from Human Preferences" demonstrated that agents could learn complex tasks from human feedback rather than requiring pre-specified reward functions. The 2018 paper "Scalable agent alignment via reward modeling" extended these techniques and provided a framework for applying <EntityLink id="reward-modeling">reward modeling</EntityLink> to more complex systems.

### Scalable Oversight Research

A significant portion of Leike's research has addressed the challenge of supervising AI systems that may be more capable than human evaluators:
- Recursive reward modeling approaches, where AI systems help humans evaluate other AI systems
- AI-assisted human evaluation techniques
- Comparisons between <EntityLink id="process-supervision">process supervision</EntityLink> (evaluating reasoning steps) and outcome supervision (evaluating final results)
- <EntityLink id="weak-to-strong">Weak-to-strong generalization</EntityLink> research examining whether less capable supervisors can effectively oversee more capable systems

### Superalignment Team

At <EntityLink id="openai">OpenAI</EntityLink>, Leike co-led the Superalignment team with <EntityLink id="ilya-sutskever">Ilya Sutskever</EntityLink>. The team's stated objectives included developing alignment techniques for <EntityLink id="superintelligence">superintelligent</EntityLink> AI systems and using AI systems to assist in aligning more capable AI systems.

## Research Publications

Selected publications include:

- **"Deep Reinforcement Learning from Human Preferences"** (2017) — Demonstrated methods for training reinforcement learning agents using human feedback on trajectory preferences
- **"Scalable agent alignment via reward modeling"** (2018) — Presented a framework for scaling <EntityLink id="reward-modeling">reward modeling</EntityLink> to more complex agent behaviors
- **"Recursively Summarizing Books with Human Feedback"** (2021) — Showed how <EntityLink id="rlhf">RLHF</EntityLink> techniques could be applied to summarization tasks on long documents

## Departure from OpenAI

In May 2024, Leike departed from <EntityLink id="openai">OpenAI</EntityLink> and posted on X (formerly Twitter) explaining his reasons for leaving. His stated concerns included:
- That building systems more intelligent than humans involves inherent risks
- That safety considerations had received insufficient priority relative to product development over the preceding period
- Issues regarding allocation of computational resources and organizational priorities for safety work

This departure occurred shortly after <EntityLink id="ilya-sutskever">Ilya Sutskever</EntityLink> also left OpenAI. The departures prompted discussion within the AI research community about safety research prioritization at AI development organizations. OpenAI's perspective on these events and any formal response to Leike's stated concerns has not been documented here.

## Research Focus at Anthropic

### Current Research Priorities

Leike has identified several research directions as priorities at <EntityLink id="anthropic">Anthropic</EntityLink>:

1. **<EntityLink id="weak-to-strong">Weak-to-strong generalization</EntityLink>**: Investigating methods by which less capable systems (including humans) can effectively supervise and evaluate more capable AI systems
2. **<EntityLink id="scalable-oversight">Scalable oversight</EntityLink> techniques**: Developing approaches to make human feedback mechanisms effective for systems that may exceed human capabilities in various domains
3. **AI system honesty**: Research on methods to ensure AI systems accurately represent their reasoning processes and limitations
4. **Automated alignment research**: Using AI systems to assist in solving alignment problems

### Technical Challenges

Research areas Leike has discussed include:

- **<EntityLink id="reward-hacking">Reward hacking</EntityLink>**: The problem of systems optimizing proxy measures rather than intended objectives
- **<EntityLink id="distributional-shift">Distributional shift</EntityLink>**: Maintaining alignment when systems encounter situations outside their training distribution
- **<EntityLink id="deceptive-alignment">Deceptive alignment</EntityLink>**: Risks that systems might appear aligned during training or evaluation while pursuing different objectives during deployment
- **Superalignment**: Developing alignment techniques that remain effective for systems more capable than humans

## Stated Views on AI Risk and Development

### Risk-Related Statements

Based on Leike's public statements and research priorities:

**Urgency**: Leike's May 2024 departure from <EntityLink id="openai">OpenAI</EntityLink> was accompanied by posts on X indicating concern about the pace of capability development relative to safety research. His public statements described building systems more capable than humans as an inherently dangerous endeavor and expressed concern about safety work being deprioritized relative to product development.

**Technical tractability**: Leike's continued focus on empirical research approaches—including <EntityLink id="weak-to-strong">weak-to-strong generalization</EntityLink>, <EntityLink id="rlhf">RLHF</EntityLink> improvements, and automated alignment research—reflects an assessment that alignment is technically approachable through empirical methods. He has indicated that current techniques require substantial improvements to remain effective for more capable systems.

### Stated Positions on AI Development

Based on public statements and career decisions:

- **Safety and capability research pacing**: His departure statement indicated concern when capability development substantially outpaces safety research
- **Resource allocation**: Advocated for dedicating significant computational resources to alignment research
- **Development incentives**: His departure statement referenced concerns about competitive pressures potentially leading to insufficient safety prioritization

### Research Methodology

Leike's approach to alignment research emphasizes:

- **Empirical testing**: Conducting experiments with existing AI systems rather than purely theoretical work
- **Learning from current systems**: Using contemporary models as testbeds for alignment techniques
- **Preparation for capability jumps**: Recognition that techniques effective for current systems may require substantial modification for more capable systems
- **Automation of alignment work**: Using AI tools to scale alignment research efforts

## Public Communication

Leike has communicated about alignment research through:
- Posts on X (formerly Twitter) discussing alignment challenges and safety concerns
- Technical blog posts explaining research directions
- The May 2024 departure statement that generated public discussion about safety prioritization at major AI laboratories

His departure from <EntityLink id="openai">OpenAI</EntityLink> attracted attention within the AI research community and generated discussion about the balance between capability development and safety research at major AI laboratories.

## Influence on the Field

### Research Impact

<EntityLink id="rlhf">RLHF</EntityLink> techniques that Leike contributed to developing have been applied across the AI industry in training large language models. The technique of using human feedback to train language models has been adopted by multiple organizations.

The <EntityLink id="scalable-oversight">scalable oversight</EntityLink> research program—including recursive reward modeling and weak-to-strong generalization—has contributed to ongoing work on how human oversight can remain effective as AI systems become more capable.

### Institutional Contributions

Leike has led alignment research teams at <EntityLink id="deepmind">Google DeepMind</EntityLink>, <EntityLink id="openai">OpenAI</EntityLink>, and <EntityLink id="anthropic">Anthropic</EntityLink>, contributing to the organizational capacity for alignment research at these institutions.

The Superalignment team at OpenAI represented a dedicated organizational unit focused on alignment for advanced AI systems. Following the departures of Leike and Sutskever, the subsequent structure and direction of that team has not been fully documented in public reporting.

## Current Work

At <EntityLink id="anthropic">Anthropic</EntityLink>, Leike continues research on alignment techniques applicable to increasingly capable AI systems. Current challenges include:

1. **Scaling existing techniques**: Determining which current alignment methods can be extended to more capable systems and which require fundamental innovations
2. **Evaluation methodology**: Developing methods to assess whether alignment techniques are effective
3. **Research automation**: Using AI systems to accelerate alignment research while managing the risks of doing so
4. **Knowledge sharing**: Determining which alignment insights should be shared across the research community
