---
title: Eli Lifland
description: "AI researcher, forecaster, and entrepreneur specializing in AGI
  timelines forecasting, scenario planning, and AI governance. Ranks #1 on the
  RAND Forecasting Initiative all-time leaderboard and co-authored the
  influential AI 2027 scenario forecast."
importance: 72
lastEdited: "2026-02-01"
update_frequency: 45
sidebar:
  order: 50
ratings:
  novelty: 4
  rigor: 6
  actionability: 5
  completeness: 8
quality: 58
llmSummary: Comprehensive biographical profile of Eli Lifland, a top-ranked
  forecaster and AI researcher who co-authored the influential AI 2027 scenario
  forecast predicting AGI by 2027-2028, though his timelines have since shifted
  to 2032-2035. The page provides detailed documentation of his forecasting
  track record, methodological approaches, and contributions to AI safety
  discourse, though it primarily serves as reference material rather than novel
  analysis.
metrics:
  wordCount: 3041
  citations: 73
  tables: 2
  diagrams: 0
clusters: ["ai-safety","epistemics"]
---
import {EntityLink, Backlinks, KeyPeople, KeyQuestions, Section} from '@components/wiki';

## Quick Assessment

| Attribute | Assessment |
|-----------|------------|
| **Primary Focus** | AGI forecasting, scenario planning, <EntityLink id="ai-governance">AI governance</EntityLink> |
| **Key Achievements** | #1 RAND Forecasting Initiative all-time leaderboard; co-authored AI 2027 scenario forecast; co-lead of <EntityLink id="samotsvety">Samotsvety</EntityLink> forecasting team |
| **Current Roles** | Researcher at <EntityLink id="ai-futures-project">AI Futures Project</EntityLink>; co-founder/advisor at Sage; guest fund manager at Long Term Future Fund |
| **Educational Background** | Computer science and economics degrees from University of Virginia |
| **Notable Contributions** | AI 2027 detailed scenario forecast; TextAttack framework for adversarial NLP attacks; top-ranked forecasting track record |
| **Community Standing** | Prominent figure in <EntityLink id="lesswrong">LessWrong</EntityLink> and Effective Altruism communities; known for openness to critique and technical rigor |


## Key Links

| Source | Link |
|--------|------|
| Official Website | [elilifland.com](https://www.elilifland.com) |
| Wikipedia | [en.wikipedia.org](https://en.wikipedia.org/wiki/AI_Futures_Project) |


## Overview

**Eli Lifland** is a prominent AI researcher, forecaster, and entrepreneur who has become one of the most influential voices in <EntityLink id="agi-timeline">AGI timeline</EntityLink> forecasting and AI safety planning. He ranks #1 on the RAND Forecasting Initiative all-time leaderboard and has consistently demonstrated exceptional forecasting accuracy across multiple platforms, including securing first place finishes for his <EntityLink id="samotsvety">Samotsvety</EntityLink> team in 2020, 2021, and 2022.[^1] His work combines technical expertise in AI systems with practical governance insights, making him a key bridge between technical AI research and policy planning.

Lifland is best known for co-authoring the **AI 2027** scenario forecast, a detailed exploration of potential <EntityLink id="agi-development">AGI development</EntityLink> trajectories that has sparked significant discussion in AI safety communities.[^2][^3] The project, developed alongside Daniel Kokotajlo, Thomas Larsen, and Scott Alexander, provides a concrete scenario for how superhuman AI capabilities might emerge by 2027-2028, including geopolitical tensions, technical breakthroughs, and alignment challenges. While his timelines have shifted—moving from a 2027 median to approximately 2032-2035 by late 2025—his work remains influential in shaping how researchers and policymakers think about near-term AGI risks.[^4][^5]

Currently, Lifland serves as a founding researcher at the <EntityLink id="ai-futures-project">AI Futures Project</EntityLink>, where he focuses on AGI capabilities forecasting and scenario planning.[^6] He also co-founded and advises Sage, an organization building interactive AI explainers and forecasting tools, and serves as a guest fund manager at the Long Term Future Fund.[^7] His previous technical contributions include working on <EntityLink id="elicit">Elicit</EntityLink> at Ought and co-creating TextAttack, a Python framework for adversarial attacks in natural language processing that has been cited 129 times.[^8]

## Background and Education

Eli Lifland holds degrees in computer science and economics from the University of Virginia.[^9] Before entering AI research and forecasting professionally, he demonstrated competitive excellence in multiple domains, including competitive programming (Battlecode, where he placed in the top 4 in three out of six years), mobile gaming (Clash Royale competitions), and speedcubing (solving Rubik's cubes, including one-handed and blindfolded variants, though he describes his performance as "decent but not world-class").[^10]

His early technical work focused on AI robustness and adversarial machine learning. While still in college or shortly after, he co-created **TextAttack**, a Python framework for adversarial attacks, data augmentation, and <EntityLink id="adversarial-training">adversarial training</EntityLink> in NLP.[^11] This work resulted in multiple well-cited academic papers, including "Reevaluating Adversarial Examples in Natural Language" (129 citations) and contributions to the RAFT few-shot classification benchmark (77 citations).[^12] These technical contributions established his credibility in AI safety-adjacent research before he pivoted more fully toward forecasting and governance work.

## Forecasting Career and Track Record

Lifland's forecasting career has been marked by exceptional and consistently documented success across multiple platforms and competitions. He ranks #1 on the RAND Forecasting Initiative (<EntityLink id="cset">CSET</EntityLink>-Foretell/INFER) all-time leaderboard and also held the #1 position for seasons one and two as of early 2024.[^13] On GJOpen, his Brier score of 0.23 significantly outperforms the median of 0.301 (ratio 0.76), and he secured 2nd place in the <EntityLink id="metaculus">Metaculus</EntityLink> Economist 2021 tournament and 1st in the Salk Tournament as of September 2022.[^14]

As co-lead of the **Samotsvety Forecasting team** (approximately 15 forecasters), Lifland helped guide the team to dominant performances across multiple years. In 2020, Samotsvety placed 1st with a relative score of -0.912 compared to -0.062 for 2nd place, with individual team members finishing 5th, 6th, and 7th.[^15] The team repeated this success in 2021, achieving 1st place with a relative score of -3.259 compared to -0.889 for 2nd place and -0.267 for Pro Forecasters, with individuals finishing 1st, 2nd, 4th, and 5th.[^16] Samotsvety holds positions 1, 2, 3, and 4 in INFER's all-time ranking, with some members achieving Superforecaster™ status.[^17]

The team has produced public forecasts on critical topics including AI existential risk and nuclear risk.[^18] Notably, Lifland worked with Samotsvety on AI existential risk forecasting for the Future Fund (now the <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> Worldview Prize), engaging in weekly discussions to decompose risks. In reviewing Joe Carlsmith's analysis, he personally estimated a **30% chance of existential risk from intent misalignment or <EntityLink id="power-seeking">power-seeking AI</EntityLink> by 2070**, updating from an initial 5% to greater than 10%.[^19]

## AI Futures Project and AI 2027

Lifland serves as a founding researcher at the **AI Futures Project**, a 501(c)(3) organization (EIN 99-4320292) focused on AGI forecasting, scenario planning, and policy engagement.[^20] The organization is comfortably funded through the medium term via private donations and a Secure Funding Foundation (SFF) grant, though it operates entirely on charitable contributions.[^21]

The project's flagship output is **AI 2027**, a detailed scenario forecast exploring how superintelligence might emerge between 2024 and 2027-2028.[^22] The scenario was co-authored with Daniel Kokotajlo (Executive Director of AI Futures Project and former <EntityLink id="openai">OpenAI</EntityLink> researcher), Thomas Larsen (who contributed full-time and has experience in both <EntityLink id="technical-ai-safety">technical AI safety</EntityLink> and AI policy), and Scott Alexander (who primarily assisted with rewriting).[^23] Romeo Dean contributed supplements on compute and security considerations.[^24]

The AI 2027 forecast presents a concrete narrative of AI development including:

- **Superhuman AI coders** emerging by 2027-2028 (median estimates), capable of automating significant portions of AI research and development[^25]
- **Superhuman AI researchers** following approximately one year after superhuman coders, improving data efficiency and accelerating progress[^26]
- **Geopolitical tensions**, particularly a US-China AI race, influencing safety decisions and deployment timelines[^27]
- **Alignment challenges**, including exploration of safer model series using chain-of-thought reasoning to address failures[^28]
- **Economic impacts**, including widespread job displacement and concerns about AI company revenues as indicators of capability progress[^29]

The project has sparked considerable discussion through podcasts, webinars (including a CEPR webinar on the AI 2027 forecast), and community engagement.[^30] Lifland has been featured discussing the implications for alignment research, safety, and international cooperation in venues including Lawfare Media and <EntityLink id="controlai">ControlAI</EntityLink>.[^31][^32]

## Timeline Updates and Methodology

Lifland's AGI timeline estimates have evolved significantly as new evidence emerges. His median forecast shifted from 2027 (when AI 2027 was initially published) to 2032 by December 2024, then to 2031 by April 2025, and ultimately to approximately 2035 by late 2025 and early 2026.[^33][^34] These updates reflect his assessment that "discrete capabilities progress appears slower in 2025 than in 2024," despite 2024's rapid advancement.[^35]

His forecasting methodology integrates multiple data streams:

- **Benchmark tracking**: Monitoring <EntityLink id="metr">METR</EntityLink>'s time horizon suite, RE-Bench, Cybench, OSWorld, SWE-Bench Verified, FrontierMath, and CBRN evaluations[^36]
- **Revenue extrapolations**: Tracking AGI company revenues as indicators of capability deployment, though he acknowledges these as weaker evidence than full models[^37]
- **Model architecture progress**: Assessing advances in distillation and reinforcement learning algorithms like PPO[^38]
- **Hardware considerations**: Accounting for compute availability and efficiency, including RL's relative inefficiency (citing <EntityLink id="toby-ord">Toby Ord</EntityLink>'s estimate that RL is approximately 1,000,000x less efficient than pre-training)[^39]

For his January 2026 forecast, Lifland's median for **Automated Coder** capabilities is approximately 2035 (1.5 years later than the AI 2027 model), while his median for **TED-AI** (a more general intelligence milestone) is 1.5 years earlier due to modeling faster takeoff dynamics.[^40] He has noted that some 2025 predictions fell below expectations (such as RE-Bench scores being higher than anticipated), while others like CBRN capabilities met or exceeded milestones (OpenAI achieved CBRN "High" and Cyber "Medium" ratings).[^41]

## Other Research and Technical Contributions

Beyond forecasting, Lifland has made technical contributions to AI safety and robustness research. His work at Ought involved contributions to Elicit, an AI-powered research assistant designed to help researchers more efficiently process academic literature.[^42] He also contributed to AI robustness research during this period, which aligns with his earlier work on adversarial examples.

His academic publications include:

- **TextAttack framework** and associated papers on adversarial NLP attacks and evaluation[^43]
- **RAFT Benchmark** for few-shot text classification (77 citations)[^44]
- **SaSTL (Spatial Aggregation Signal Temporal Logic)** for runtime monitoring in smart cities (36 citations), co-authored with collaborators at IEEE ICCPS 2020[^45]
- Papers on spatial-temporal specification-based monitoring systems (31 citations)[^46]

These contributions demonstrate breadth across AI safety, robustness, and monitoring systems. His Google Scholar profile shows an h-index of 7, with full publication details available.[^47]

Lifland has also contributed to forecasting methodology discussions within the Effective Altruism community. In a March 2024 podcast with Ozzie Gooen, he critiqued shallow crowd forecasting for high-importance questions, arguing that platforms may underweight questions requiring deeper research rather than quick probability estimates.[^48] He has advocated for "purer" <EntityLink id="alignment">AI alignment</EntityLink> approaches such as <EntityLink id="ai-assisted">AI-assisted alignment</EntityLink> and empirical iteration on failures, referencing discussions on aligning transformative AI if developed very soon.[^49]

## Sage and AI Digest

Lifland co-founded **Sage**, an organization focused on building interactive AI explainers and forecasting tools.[^50] One of Sage's key projects is **AI Digest**, which received \$550,000 from <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> for its work, with an additional \$550,000 for forecasting projects.[^51] The organization aims to make AI developments more accessible to broader audiences through interactive tools and clear explanations.

As co-founder and advisor, Lifland continues to guide Sage's strategic direction while primarily focusing his research efforts on the AI Futures Project.[^52] The organization operates within the effective altruism ecosystem and has benefited from networking through programs like the Constellation Astra Fellowship, which facilitated connections leading to Lifland's collaboration with Romeo Dean and Daniel Kokotajlo on AI 2027.[^53]

## Role in the AI Safety Community

Lifland plays an active role in the AI safety and alignment communities, particularly through <EntityLink id="lesswrong">LessWrong</EntityLink> and the Effective Altruism Forum. He is known for engaging openly with criticism and maintaining a technically rigorous approach to forecasting and scenario planning. Community members describe him as helpful, kind, and receptive to feedback, with a willingness to award bounties for identifying errors in his work.[^54]

He serves as a mentor in the MATS Program (focusing on Strategy & Forecasting, Policy & Governance streams), helping guide the next generation of AI safety researchers.[^55] His work has been featured in the documentary "Making God," which explores AGI risks and was released in 2025 or later.[^56] He has also contributed to discussions on navigating the AI alignment landscape, emphasizing the importance of upskilling for AI safety work while critiquing what he views as over-recruitment of junior researchers in the alignment community.[^57]

Lifland has taken the Giving What We Can Pledge, committing to donate 10% of his lifetime income to effective charities, reflecting his integration into effective altruism principles.[^58] His career focus on ensuring that "advanced AI goes well" aligns with long-term future priorities, and he has received support from the Long Term Future Fund (implied through his involvement with the EA community).[^59]

His collaborative approach extends to the Constellation Astra Fellowship, where over 80% of the first cohort were placed in AI safety roles at organizations including <EntityLink id="redwood-research">Redwood Research</EntityLink>, METR, <EntityLink id="anthropic">Anthropic</EntityLink>, <EntityLink id="openai">OpenAI</EntityLink>, and <EntityLink id="deepmind">DeepMind</EntityLink>.[^60] Through mentorship relationships (such as with Buck Shlegeris, who connected a fellow to a team leadership role at Redwood Research), Lifland has helped facilitate career development in AI safety.[^61]

## Criticisms and Controversies

Lifland's work, particularly the AI 2027 timelines model, has faced methodological criticism from community members. In a detailed critique posted to <EntityLink id="lesswrong">LessWrong</EntityLink>, the EA Forum, and Substack in June 2024, forecaster "titotal" described the model's fundamental structure as "highly questionable," with little empirical validation, misrepresented code, and poor justification for parameters like superexponential time horizon growth curves.[^62] Titotal, identifying as a physicist, argued that models need strong conceptual and empirical justifications before influencing major decisions, characterizing AI 2027 as resembling a "shoddy toy model stapled to a sci-fi short story" disguised as rigorous research.[^63]

Critics have also raised concerns about philosophical overconfidence, warning that popularizing flawed models could lead people to make significant life decisions (such as whether to attend law school) based on shaky forecasts.[^64] However, others counter that inaction on short timelines could be costlier if the forecasts prove accurate, and that models inevitably inform real-world decisions regardless of their limitations.[^65]

Lifland responded to these criticisms with notable openness, acknowledging errors and reviewing titotal's critique for factual accuracy. He agreed to changes in the model write-up and paid \$500 bounties to both titotal and another critic, Peter Johnson, for identifying issues.[^66][^67] He released an updated model addressing some concerns, including adding more weight on superexponential growth and extending overall timelines. While defending the core plausibility of superhuman coders emerging by 2027, Lifland emphasized that the model represents his team's best current guess and challenged critics to develop better alternatives.[^68]

Other criticisms include:

- **Lack of skeptic engagement**: Some community members felt AI 2027 did not sufficiently address skeptical frameworks or justify its models against competing views, focusing more on detailed scenario planning than on persuading dissenters[^69]
- **Unverifiable predictions**: Concerns that predictions like "METR tasks taking approximately 16 hours may not scale to model improvement complexity" are difficult to validate empirically[^70]
- **Forecasting depth debates**: Disagreements with other forecasters like Ozzie Gooen about whether platforms adequately weight high-importance questions, with Lifland arguing they underweight questions requiring deep research[^71]

Lifland has been forthright about forecast misses, noting in 2025 that some predictions fell below expectations (such as discrete capabilities progress appearing slower than in 2024) while others exceeded them.[^72] He maintains that despite imperfections, models like AI 2027 represent state-of-the-art thinking and provide valuable frameworks for navigating uncertainty about AGI development.

No major personal controversies or ethical issues have been documented beyond these methodological debates, and Lifland's willingness to engage with criticism has generally been well-received in the community.[^73]

## Key Uncertainties

Several major uncertainties surround Lifland's forecasts and their implications:

1. **Timeline accuracy**: While Lifland's median forecast has shifted to approximately 2035 for key AGI milestones, substantial uncertainty remains about whether superhuman AI capabilities will emerge on this timeline, sooner, or significantly later. His models acknowledge this uncertainty, but the true trajectory depends on factors including compute availability, algorithmic breakthroughs, and <EntityLink id="alignment-progress">alignment progress</EntityLink>.

2. **Model validity**: The methodological critiques of the AI 2027 timelines model raise questions about whether the underlying technical approach (including superexponential growth assumptions and parameter choices) accurately captures AI development dynamics. Even with updates, the model's predictive power remains untested by time.

3. **Alignment solutions**: Lifland's scenarios explore alignment challenges, but whether proposed solutions like chain-of-thought reasoning or AI-assisted alignment research will prove sufficient for safe superhuman AI remains deeply uncertain.

4. **Geopolitical dynamics**: The AI 2027 scenario includes significant US-China race dynamics, but how international cooperation or competition will actually unfold—and whether it will prioritize safety over capability advancement—is unpredictable.

5. **Economic and social impacts**: While the scenarios explore job displacement and revenue growth as indicators, the actual economic and social consequences of rapid AI progress remain highly uncertain, including questions about wealth distribution, governance structures, and societal adaptation.

6. **Forecasting generalization**: While Lifland has an exceptional track record on specific forecasting platforms, the degree to which success on shorter-term, more constrained questions predicts accuracy on unprecedented, long-term AGI development remains unclear.

## Sources

[^1]: [Samotsvety Track Record](https://samotsvety.org/track-record/)
[^2]: [AI 2027 About Page](https://ai-2027.com/about)
[^3]: [Lawfare Media - Daniel Kokotajlo and Eli Lifland on AI 2027](https://www.lawfaremedia.org/article/lawfare-daily--daniel-kokotajlo-and-eli-lifland-on-their-ai-2027-report)
[^4]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^5]: [Marketing AI Institute - Moving Back AGI Timeline](https://www.marketingaiinstitute.com/blog/moving-back-agi-timeline)
[^6]: [AI Futures Project About Page](https://ai-futures.org/about/)
[^7]: [Eli Lifland Personal Website](https://www.elilifland.com)
[^8]: [Eli Lifland Google Scholar Profile](https://scholar.google.com/citations?user=Q33DXbEAAAAJ&hl=en)
[^9]: [Eli Lifland Personal Website](https://www.elilifland.com)
[^10]: [Eli Lifland Personal Website](https://www.elilifland.com)
[^11]: [Eli Lifland Google Scholar Profile](https://scholar.google.com/citations?user=Q33DXbEAAAAJ&hl=en)
[^12]: [Eli Lifland Google Scholar Profile](https://scholar.google.com/citations?user=Q33DXbEAAAAJ&hl=en)
[^13]: [Samotsvety Track Record](https://samotsvety.org/track-record/)
[^14]: [Samotsvety Track Record](https://samotsvety.org/track-record/)
[^15]: [Samotsvety Track Record](https://samotsvety.org/track-record/)
[^16]: [Samotsvety Track Record](https://samotsvety.org/track-record/)
[^17]: [Samotsvety Track Record](https://samotsvety.org/track-record/)
[^18]: [Quantified Uncertainty - Eli Lifland Podcast](https://quri.substack.com/p/eli-lifland-on-navigating-the-ai)
[^19]: [Quantified Uncertainty - Eli Lifland on AI Alignment](https://quri.substack.com/p/eli-lifland-on-navigating-the-ai)
[^20]: [AI Futures Project](https://ai-futures.org)
[^21]: [Zvi Substack - Big Nonprofits Post 2025](https://thezvi.substack.com/p/the-big-nonprofits-post-2025)
[^22]: [AI 2027 About Page](https://ai-2027.com/about)
[^23]: [Lawfare Media - Daniel Kokotajlo and Eli Lifland on AI 2027](https://www.lawfaremedia.org/article/lawfare-daily--daniel-kokotajlo-and-eli-lifland-on-their-ai-2027-report)
[^24]: [AI 2027 About Page](https://ai-2027.com/about)
[^25]: [AI 2027 Website](https://ai-2027.com)
[^26]: [AI 2027 Website](https://ai-2027.com)
[^27]: [ControlAI Newsletter - Future of AI Special Edition](https://controlai.news/p/special-edition-the-future-of-ai)
[^28]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^29]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^30]: [CEPR Webinar - AI 2027 Scenario Forecast](https://cepr.org/multimedia/cepr-webinar-series-economics-artificial-intelligence-ai-2027-scenario-forecast)
[^31]: [Lawfare Media - Daniel Kokotajlo and Eli Lifland on AI 2027](https://www.lawfaremedia.org/article/lawfare-daily--daniel-kokotajlo-and-eli-lifland-on-their-ai-2027-report)
[^32]: [ControlAI Newsletter - Future of AI Special Edition](https://controlai.news/p/special-edition-the-future-of-ai)
[^33]: [Eli Lifland Personal Website](https://www.elilifland.com)
[^34]: [Marketing AI Institute - Moving Back AGI Timeline](https://www.marketingaiinstitute.com/blog/moving-back-agi-timeline)
[^35]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^36]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^37]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^38]: [AI 2027 Website](https://ai-2027.com)
[^39]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^40]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^41]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^42]: [Eli Lifland Personal Website](https://www.elilifland.com)
[^43]: [Eli Lifland Google Scholar Profile](https://scholar.google.com/citations?user=Q33DXbEAAAAJ&hl=en)
[^44]: [Eli Lifland Google Scholar Profile](https://scholar.google.com/citations?user=Q33DXbEAAAAJ&hl=en)
[^45]: [Eli Lifland Google Scholar Profile](https://scholar.google.com/citations?user=Q33DXbEAAAAJ&hl=en)
[^46]: [Eli Lifland Google Scholar Profile](https://scholar.google.com/citations?user=Q33DXbEAAAAJ&hl=en)
[^47]: [Eli Lifland Google Scholar Profile](https://scholar.google.com/citations?user=Q33DXbEAAAAJ&hl=en)
[^48]: [EA Forum - Is Forecasting a Promising EA Cause Area](https://ea.greaterwrong.com/posts/fsnMDpLHr78XgfWE8/podcast-is-forecasting-a-promising-ea-cause-area)
[^49]: [EA Forum - Eli Lifland on Navigating AI Alignment](https://forum.effectivealtruism.org/posts/QeLE22fefLqKfYTW6/eli-lifland-on-navigating-the-ai-alignment-landscape)
[^50]: [Eli Lifland Personal Website](https://www.elilifland.com)
[^51]: [Manifund - AI Digest Project](https://manifund.org/projects/ai-digest)
[^52]: [Eli Lifland Personal Website](https://www.elilifland.com)
[^53]: [Constellation Astra Fellowship](https://www.constellation.org/programs/astra-fellowship)
[^54]: [EA Forum - Eli Lifland User Profile](https://forum.effectivealtruism.org/users/elifland)
[^55]: [MATS Program - Eli Lifland Mentor Profile](https://www.matsprogram.org/mentor/lifland)
[^56]: [EA Forum - Making God Documentary](https://forum.effectivealtruism.org/posts/gsKQknEikbERo4Hih/creating-making-god-a-feature-documentary-on-risks-from-agi)
[^57]: [Quantified Uncertainty - Eli Lifland Podcast](https://quantifieduncertainty.org/posts/eli-lifland-on-navigating-the-ai-722/)
[^58]: [Eli Lifland Personal Website](https://www.elilifland.com)
[^59]: [Eli Lifland Personal Website](https://www.elilifland.com)
[^60]: [Constellation Astra Fellowship](https://www.constellation.org/programs/astra-fellowship)
[^61]: [Constellation Astra Fellowship](https://www.constellation.org/programs/astra-fellowship)
[^62]: [LessWrong - Deep Critique of AI 2027 Timeline Models](https://www.lesswrong.com/posts/PAYfmG2aRbdb74mEp/a-deep-critique-of-ai-2027-s-bad-timeline-models)
[^63]: [LessWrong - Deep Critique of AI 2027 Timeline Models](https://www.lesswrong.com/posts/PAYfmG2aRbdb74mEp/a-deep-critique-of-ai-2027-s-bad-timeline-models)
[^64]: [EA Forum - Practical Value of Flawed Models](https://forum.effectivealtruism.org/posts/fKx6DkWfzJXoycWhE/the-practical-value-of-flawed-models-a-response-to-titotal-s)
[^65]: [EA Forum - Practical Value of Flawed Models](https://forum.effectivealtruism.org/posts/fKx6DkWfzJXoycWhE/the-practical-value-of-flawed-models-a-response-to-titotal-s)
[^66]: [AI Futures Notes Substack - Response to Titotal Critique](https://aifuturesnotes.substack.com/p/response-to-titotals-critique-of)
[^67]: [EA Forum - Practical Value of Flawed Models](https://forum.effectivealtruism.org/posts/fKx6DkWfzJXoycWhE/the-practical-value-of-flawed-models-a-response-to-titotal-s)
[^68]: [AI Futures Notes Substack - Response to Titotal Critique](https://aifuturesnotes.substack.com/p/response-to-titotals-critique-of)
[^69]: [ControlAI Newsletter - Future of AI Special Edition](https://controlai.news/p/special-edition-the-future-of-ai)
[^70]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^71]: [EA Forum - Is Forecasting a Promising EA Cause Area](https://ea.greaterwrong.com/posts/fsnMDpLHr78XgfWE8/podcast-is-forecasting-a-promising-ea-cause-area)
[^72]: [Eli Lifland LessWrong Profile](https://www.lesswrong.com/users/elifland)
[^73]: [EA Forum - Eli Lifland User Profile](https://forum.effectivealtruism.org/users/elifland)

<Backlinks />
