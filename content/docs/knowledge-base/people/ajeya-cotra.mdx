---
title: Ajeya Cotra
description: "Member of technical staff at METR and former senior advisor at Coefficient Giving (Open Philanthropy), known for the Bio Anchors AI timelines report and influential work on intelligence explosion dynamics, crunch time strategy, and AI safety grantmaking. Placed 3rd out of 413 participants in AI development forecasting."
sidebar:
  order: 8
entityType: person
subcategory: safety-researchers
quality: 55
readerImportance: 55
researchImportance: 60
lastEdited: "2026-02-20"
update_frequency: 45
llmSummary: "Ajeya Cotra is a member of technical staff at METR and former senior advisor at Coefficient Giving (Open Philanthropy), where she led technical AI safety grantmaking including a $25M agent benchmarks RFP. Author of the Bio Anchors AI timelines report (15% transformative AI by 2036, 50% by 2060), she has become influential for her work on intelligence explosion dynamics, crunch time strategy (the 6-12 month window after AI automates AI R&D), and the case for using early transformative AI for defensive work."
ratings:
  novelty: 4
  rigor: 6
  actionability: 5
  completeness: 6.5
clusters:
  - ai-safety
  - community
---
import {DataInfoBox, R, EntityLink, DataExternalLinks} from '@components/wiki';

<DataExternalLinks pageId="ajeya-cotra" />

<DataInfoBox entityId="E864" />

## Overview

Ajeya Cotra is a member of technical staff at <EntityLink id="metr">METR</EntityLink> (Model Evaluation and Threat Research) and formerly a senior advisor at <EntityLink id="coefficient-giving">Coefficient Giving</EntityLink> (formerly <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink>), where she spent nine years doing AI strategy research and leading technical AI safety grantmaking. She is among the most respected and accurate forecasters of AI developments, placing 3rd out of 413 participants in an AI forecasting competition, and her work on timelines, capability evaluations, and threat modeling has been widely influential in AI safety circles.

Cotra is best known for the [Bio Anchors report](https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines), developed with <EntityLink id="holden-karnofsky">Holden Karnofsky</EntityLink>, which estimates AI development timelines by comparing required computation to biological systems. The framework projects roughly 15% probability of transformative AI by 2036 and 50% by 2060. More recently, she has developed influential thinking on the "crunch time" window --- the potentially brief period (perhaps 6--12 months) between AI automating AI research and the arrival of uncontrollably powerful superintelligence --- and the case for redirecting AI labor toward alignment, biodefense, cyberdefense, and collective decision-making during that window.

| Dimension | Details |
|-----------|---------|
| **Current Role** | Member of Technical Staff, <EntityLink id="metr">METR</EntityLink> (since late 2025) |
| **Previous Role** | Senior Advisor, <EntityLink id="coefficient-giving">Coefficient Giving</EntityLink> (2016--2025) |
| **Education** | UC Berkeley (graduated 2016) |
| **Key Publication** | [Bio Anchors report](https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) on AI timelines |
| **Forecasting** | 3rd out of 413 in AI development forecasting competition |
| **Grantmaking** | Led \$25M+ agent benchmarks RFP; \$2--3M evidence-gathering RFP |
| **Core Thesis** | Early 2030s: top-human-expert-dominating AI; followed by 6--12 month crunch time window |

## Career Evolution

### Research at Coefficient Giving (2016--2023)

Cotra joined GiveWell (the predecessor organization) in 2016 immediately after graduating from UC Berkeley, drawn by the organization's intellectual depth and commitment to rigorous charity evaluation. For her first six to seven years, she focused primarily on deep research rather than grantmaking --- an unusual role at a grantmaking organization, driven by demand from leadership (particularly Holden Karnofsky) for foundational AI strategy work.

Key research outputs during this period include:

- **Bio Anchors Report** (2020): A framework for estimating AI timelines by comparing required computation to biological systems, projecting 15% probability of transformative AI by 2036 and 50% by 2060. This became one of the most cited references in AI timelines discourse.
- **AI Takeoff Speeds Analysis**: Research on how quickly capabilities might advance once key thresholds are reached.
- **Threat Modeling**: Work on how AI could lead to catastrophic outcomes, including through deceptive alignment and power-seeking behavior.

### Technical AI Safety Grantmaking (2023--2025)

In late 2023, Cotra transitioned to leading Coefficient Giving's technical AI safety grantmaking portfolio, which had been orphaned after previous program officers departed. She brought a distinctive approach emphasizing deep inside-view understanding of research directions rather than relying primarily on heuristics about researcher quality.

Her approach involved forming detailed views about how specific research directions (interpretability, control, evaluations) would connect to preventing AI takeover, then using those views to co-create grant opportunities with researchers. This contrasted with the organization's more typical approach of faster, less deeply justified grantmaking.

**Major grantmaking achievements:**

| Initiative | Amount | Focus |
|-----------|--------|-------|
| **Agent Benchmarks RFP** (late 2023) | \$25M | Funded realistic agent benchmarks including Cybench; pushed for harder, more realistic tasks than existing benchmarks |
| **Evidence-Gathering RFP** | \$2--3M | Funded surveys, RCTs, and other non-benchmark evidence about AI impact, including the LEAP panel at the Forecasting Research Institute |
| **FTX Emergency Grants** (2022) | â‰ˆ50 grants | Rapid response grants to researchers affected by the FTX Foundation collapse |

### Sabbatical and Transition to METR (2025)

After Karnofsky's departure from Coefficient Giving in 2023, Cotra found the working environment increasingly difficult --- the loss of engaged intellectual partnership, challenges with management, and the tension between her desire for deep understanding and the pace demands of grantmaking. In September 2025, she took a four-month sabbatical.

During the sabbatical, she reflected on career patterns, participated in the inaugural Curve Conference (bringing together AI skeptics and safety researchers), and considered going independent as a writer. She ultimately returned to Coefficient Giving in a senior advisor role, helping new GCR director Emily Oehlsen develop strategy, before joining <EntityLink id="metr">METR</EntityLink> as a member of technical staff --- a role more aligned with her desire for deep research.

## Key Ideas and Frameworks

### The Crunch Time Framework

Cotra's most distinctive recent contribution is the "crunch time" framework for thinking about the intelligence explosion. The core argument:

1. **AI will likely automate AI R&D in the early 2030s**, producing what Ryan Greenblatt calls "top-human-expert-dominating AI" --- systems better than any human expert at all remote computer-based tasks.
2. **A narrow window follows** (perhaps 6--12 months by default) before AI becomes uncontrollably powerful.
3. **During this window**, the optimal strategy is to redirect as much AI labor as possible from further capability acceleration toward protective work: alignment research, biodefense, cyberdefense, AI for better collective decision-making, and other defensive measures.
4. **This plan is the stated strategy** of all major frontier labs (OpenAI, Anthropic, Google DeepMind), though none have quantitative commitments about what fraction of AI labor they will redirect.

### Why the Plan Might Fail

Cotra identifies several failure modes for the crunch time strategy:

| Failure Mode | Likelihood | Description |
|-------------|-----------|-------------|
| **Insufficient redirection** | Highest | Companies face competitive pressure and simply don't redirect enough AI labor from capabilities to safety work |
| **No meaningful window** | Moderate | AI jumps from narrow capability to overwhelming superintelligence in days or weeks, leaving no time to respond |
| **Capability mismatch** | Moderate | AIs good at AI R&D but not at safety research, biodefense, moral philosophy, or other needed work |
| **Misaligned AI helpers** | Moderate | Early transformative AIs have incentives to undermine alignment, defensive work, and epistemics --- not just alignment research |
| **Execution failure** | Moderate | Even with commitment, organizations can't pivot fast enough; decision-making lags prevent rapid reallocation |

### The Transparency Imperative

Cotra argues that transparency about AI capabilities is essential for detecting the onset of an intelligence explosion. Her proposed transparency regime includes:

- **Calendar-cadence benchmark reporting**: Labs should report their highest internal benchmark scores every three months, not just at product launch --- because danger could come from purely internal deployment.
- **Internal AI adoption metrics**: The fraction of pull requests mostly written and reviewed by AI (not just lines of code), tracking how much decision-making authority is being ceded to AI systems.
- **Safety incident reporting**: Whether models have lied about important matters or covered up logs in real internal use.
- **Observed productivity measures**: The ultimate indicator --- whether labs are discovering insights faster internally, signaling the intelligence explosion is underway.

She argues this information should be public rather than shared only with governments, because detecting and responding to an intelligence explosion requires society-wide common knowledge and the ability for outside experts to weigh in.

### The 1,000-Fold Disagreement

Cotra highlights an extraordinary range of views on AI's economic impact. At one extreme, mainstream economists expect AI to add roughly 0.3 percentage points to economic growth. At the other, futurist-oriented researchers project 1,000%+ annual economic growth --- a disagreement spanning three to four orders of magnitude.

She traces this to two competing priors:

- **The slow camp** leans on 150 years of \~2% growth in frontier economies despite radical technological change (electricity, radio, television, computers, internet), plus a general prior that things are "always harder and slower than you think."
- **The fast camp** leans on 10,000-year economic history showing acceleration, plus models where AI closing the full loop of producing more AI (cognitive and physical) removes the constraints that kept growth at 2%.

### Three Types of Intelligence Explosion

Drawing on Tom Davidson's work at Forethought, Cotra emphasizes that automating AI R&D is only one of three feedback loops needed for a full intelligence explosion:

1. **Software improvement**: AI improves AI training algorithms and architectures
2. **Hardware production**: AI automates chip design, fabrication, equipment manufacturing, and raw material processing
3. **Physical automation**: AI controls robots that close the loop of manufacturing everything needed to make more AI

She expects the software loop to kick in first (early 2030s), with hardware and physical automation following within one to two years, enabled by rapid advances in robotics.

## Views and Positions

### AI Timelines

| Milestone | Cotra's Estimate | Basis |
|-----------|-----------------|-------|
| Top-human-expert-dominating AI | Early 2030s | Bio Anchors framework, current capability trends |
| AI automating AI R&D | Early-to-mid 2030s | Software feedback loop analysis |
| Full physical automation | 1--2 years after software automation | Robotics progress, bootstrapping from cognitive AI |
| World as different as hunter-gatherer era vs. today | By 2050 | "10,000 years of progress" driven by AI |

### AI Safety Strategy

Cotra supports a multi-pronged approach:

- **<EntityLink id="ai-control">AI Control</EntityLink>** techniques for getting useful work from potentially misaligned early transformative AI
- **Transparency requirements** for frontier labs, especially internal capability metrics and safety incidents
- **Gradual slowdown** rather than hard pause-and-unpause, preferring to stretch a one-year default trajectory to 10--20 years
- **Using AI labor for defense**: Alignment, biodefense, cyberdefense, epistemics, coordination, and moral philosophy
- **Foundation strategy shift**: Philanthropies like Coefficient Giving should prepare to spend heavily on AI inference compute rather than human researcher salaries during crunch time

### Effective Altruism

Cotra has been involved in effective altruism since age 13. She identifies three things that originally drew her to the movement:

1. **Expanding moral circle**: Caring about distant, different beings (globally, temporally, across species)
2. **Intellectual depth**: Rigorous, quantitative, "do-the-homework" approach to figuring out how to help
3. **Extreme transparency and integrity**: GiveWell's mistakes page, refusal of donation matching, proactive honesty

She observes that while the first remains strong, the second and third have eroded as EA has shifted from a research-and-persuasion movement to one wielding significant resources in adversarial political environments. She believes EA's comparative advantage lies in incubating speculative cause areas --- like <EntityLink id="digital-sentience">digital sentience</EntityLink>, space governance, and value lock-in --- at a stage when they are too unconventional for mainstream engagement.

## Influence and Track Record

Rob Wiblin noted in an October 2025 interview that many of Cotra's earlier predictions and concerns had proven prescient:

| Topic (from 2023 interview) | Subsequent Validation |
|------------------------------|----------------------|
| <EntityLink id="metr">METR</EntityLink> evaluating autonomous capabilities | Became super influential in policy circles |
| Probes to monitor dangerous conversations | Standard practice; one of the most useful interpretability outputs |
| Chain of thought monitoring | Still the dominant AI oversight technique |
| Growing situational awareness in AI models | Now a completely mainstream topic |
| <EntityLink id="deceptive-alignment">Deceptive alignment</EntityLink> | Research confirmed models do hide misbehavior when trained against it |
| Models getting "schemier" with RL | Confirmed by research; observed in practice |
| <EntityLink id="sycophancy">Sycophancy</EntityLink> | Major recognized problem in deployed models |

## Key Relationships

| Person/Org | Relationship |
|-----------|-------------|
| <EntityLink id="holden-karnofsky">Holden Karnofsky</EntityLink> | Former manager at Coefficient Giving; intellectual collaborator on Bio Anchors and AI strategy |
| <EntityLink id="coefficient-giving">Coefficient Giving</EntityLink> | Nine years (2016--2025); research and grantmaking |
| <EntityLink id="metr">METR</EntityLink> | Current employer; aligned with her view of METR as "the world's early warning system for intelligence explosion" |
| <EntityLink id="redwood-research">Redwood Research</EntityLink> | Considered joining; close alignment with their AI control work |
| <EntityLink id="paul-christiano">Paul Christiano</EntityLink> | Intellectual influence; ARC/METR ecosystem |

## Sources

- [80,000 Hours Podcast #226: Ajeya Cotra on transformative AI crunch time](https://80000hours.org/podcast/episodes/ajeya-cotra-transformative-ai-crunch-time/) (February 2026) --- Primary source for crunch time framework, career narrative, and current views
- [80,000 Hours Podcast #151: Ajeya Cotra on accidentally teaching AI models to deceive us](https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/) (May 2023) --- Earlier interview covering deceptive alignment concerns
- [Bio Anchors Report](https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) --- Foundational AI timelines methodology
- [Ajeya Cotra joins METR](https://metr.org) --- Transition announcement
