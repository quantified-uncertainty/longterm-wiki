---
title: Chris Olah
description: Co-founder of Anthropic and researcher in neural network interpretability
sidebar:
  order: 12
entityType: person
subcategory: safety-researchers
quality: 27
readerImportance: 79
researchImportance: 39
tacticalValue: 80
lastEdited: "2026-02-20"
update_frequency: 45
llmSummary: Biographical overview of Chris Olah's career trajectory from Google Brain to co-founding Anthropic, focusing on his work in mechanistic interpretability including feature visualization, circuit analysis, and sparse autoencoder research (Scaling Monosemanticity 2024). Documents his contributions to science communication through Distill journal and influential blog posts.
ratings:
  novelty: 2
  rigor: 3.5
  actionability: 2
  completeness: 5
clusters: ["ai-safety"]
---
import {DataInfoBox, DataExternalLinks, EntityLink} from '@components/wiki';

<DataExternalLinks pageId="chris-olah" />

<DataInfoBox entityId="E59" />

## Quick Assessment

| Dimension | Assessment |
|-----------|------------|
| **Primary Role** | Co-founder and interpretability research lead at <EntityLink id="anthropic">Anthropic</EntityLink> |
| **Key Contributions** | Feature visualization techniques, circuit analysis methodology, sparse autoencoder applications for interpretability, co-founding Distill journal |
| **Key Publications** | "Toy Models of Superposition" (2022), "Scaling Monosemanticity" (2024), "Feature Visualization" (2017), "The Building Blocks of Interpretability" (2018) |
| **Institutional Affiliation** | <EntityLink id="anthropic">Anthropic</EntityLink> (2021–present); previously Google Brain (2015–2021) |
| **Influence on AI Safety** | Contributed to establishing <EntityLink id="mech-interp">mechanistic interpretability</EntityLink> as a research direction within AI safety; applied transparency and verification approaches to <EntityLink id="large-language-models">large language models</EntityLink> |

## Overview

Chris Olah is a researcher specializing in neural network <EntityLink id="interpretability">interpretability</EntityLink> and a co-founder of <EntityLink id="anthropic">Anthropic</EntityLink>. His work focuses on understanding the internal computations of neural networks through feature visualization, circuit analysis, and sparse autoencoder techniques. He maintained a technical blog at [colah.github.io](https://colah.github.io) that covered topics including LSTM networks, neural network representations, and attention mechanisms. At <EntityLink id="anthropic">Anthropic</EntityLink>, he leads interpretability research aimed at making AI systems more transparent and verifiable.

Olah co-founded the Distill journal in 2016, which emphasized interactive visualizations and web-native presentation of machine learning research. His blog posts, particularly a 2015 explanation of LSTM networks, are widely referenced as accessible introductions to complex neural network architectures.

## Background

Olah joined Google Brain in 2015, where he conducted research on neural network interpretability. In 2021, he co-founded <EntityLink id="anthropic">Anthropic</EntityLink> along with <EntityLink id="dario-amodei">Dario Amodei</EntityLink> and other former <EntityLink id="openai">OpenAI</EntityLink> researchers.

His technical blog, hosted at [colah.github.io](https://colah.github.io), includes posts on LSTM networks, visualization of neural network representations, and attention mechanisms in recurrent neural networks. These posts combine technical explanations with interactive visualizations.

## Mechanistic Interpretability Research

Olah's research program aims to understand neural networks by reverse-engineering their internal algorithms and representations. This approach, termed <EntityLink id="mech-interp">mechanistic interpretability</EntityLink>, treats neural networks as systems that can be understood at the level of individual features and circuits rather than solely through input-output behavior.

### Feature Visualization

Feature visualization techniques synthesize inputs that maximally activate specific neurons or layers in a neural network. Olah's 2017 work on feature visualization established methods for generating these visualizations and interpreting what features neural networks learn. The approach involves optimizing input images to maximize activation of target neurons, revealing the visual patterns those neurons respond to.

The **"Feature Visualization"** (2017) paper introduced optimization-based activation maximization and methods for visualizing intermediate layers to understand hierarchical feature learning. This work involved collaboration with researchers at Google Brain including Alexander Mordvintsev and Ludwig Schubert.

### Circuit Analysis

Circuit analysis extends feature visualization by tracing how features connect and process information. The 2018 paper **"The Building Blocks of Interpretability"** demonstrated that individual features can be identified and visualized, that connections between features form interpretable circuits, and that these circuits implement specific algorithms or computations. Co-authors included Shan Carter, Ludwig Schubert, and other Google Brain researchers.

The 2020 paper **"Zoom In: An Introduction to Circuits"** further developed this framework, arguing that neural networks can be understood through the lens of individual computational units and their connections, with co-authors including Nick Cammarata and Gabriel Goh.

### Superposition and Sparse Autoencoders

**"Toy Models of Superposition"** (2022) provided a mathematical framework for understanding why interpretability is difficult. The paper demonstrated that neural networks can represent more features than they have dimensions by storing features in superposition — allowing multiple features to interfere in the same neurons. Key findings included that networks learn to represent sparse features in superposition, that the number of representable features scales with sparsity, and that this explains polysemanticity (neurons responding to multiple unrelated concepts). Co-authors included Anthropic researchers Nelson Elhage, Tom Henighan, and others.

**"Scaling Monosemanticity"** (2024) applied sparse autoencoders to extract interpretable features from Claude 3 Sonnet. The paper reported training sparse autoencoders with millions of features and demonstrating that these features correspond to interpretable concepts, can be causally intervened upon to change model behavior, exist at multiple levels of abstraction, and scale to production-sized <EntityLink id="large-language-models">large language models</EntityLink>. This work involved a large team at <EntityLink id="anthropic">Anthropic</EntityLink> and represented an application of interpretability techniques to a frontier AI system.

## Distill Journal

In 2016, Olah co-founded Distill with Shan Carter and others. Distill operated as an academic journal with peer review but emphasized interactive visualizations, clear explanations, and web-native presentation of machine learning research. Articles underwent review for both correctness and clarity.

The journal published research on topics including neural network interpretability and visualization, attention mechanisms, optimization dynamics, and feature learning. During its period of operation, interactive visualizations became more common in machine learning research presentations and papers. Distill is no longer actively publishing new work.

## Work at Anthropic

At <EntityLink id="anthropic">Anthropic</EntityLink>, Olah leads interpretability research with a focus on understanding frontier AI systems. The research program aims to:

1. **Scale interpretability to production models**: Develop techniques that work on models the size of Claude rather than only small research models
2. **Connect interpretability to safety**: Use understanding of model internals to detect potentially dangerous capabilities or behaviors
3. **Automate interpretability**: Use AI systems to help interpret other AI systems, enabling analysis at scale
4. **Develop verification methods**: Create techniques that can verify properties of AI systems through understanding their internals

### Interpretability for AI Safety

The interpretability program at <EntityLink id="anthropic">Anthropic</EntityLink> aims to support safety through several approaches:

**Capability detection**: Identifying when models possess specific capabilities by examining internal representations and features, potentially enabling detection of dangerous capabilities before they manifest in behavior.

**Behavior verification**: Understanding the mechanisms behind model outputs to assess whether models are reporting their actual internal states, relevant to concerns about <EntityLink id="deceptive-alignment">deceptive alignment</EntityLink>.

**Debugging**: Using mechanistic understanding to identify and potentially modify problematic model behaviors or learned heuristics.

**Monitoring**: Developing methods to detect anomalous internal activations that might indicate <EntityLink id="scheming">scheming</EntityLink> or other concerning behaviors.

## Research Philosophy and Communication

Olah's research approach emphasizes several recurring themes:

**Visual communication**: Using diagrams, interactive visualizations, and carefully designed figures to convey technical concepts. His blog posts and papers typically include extensive visualizations.

**Accessibility without simplification**: Explaining complex topics clearly while maintaining technical precision. His LSTM post from 2015 is frequently referenced as an introduction to LSTM architectures.

**Infrastructure investment**: Building tools and frameworks for interpretability research, including visualization libraries and analysis frameworks.

**Long-term research**: Pursuing research directions over multiple years, with superposition research spanning from initial theoretical work in 2022 to scaled demonstrations in 2024.

## Key Publications

**Blog Posts** ([colah.github.io](https://colah.github.io)):
- "Understanding LSTM Networks" (2015)
- "Visualizing Representations: Deep Learning and Human Beings" (2015)
- "Attention and Augmented Recurrent Neural Networks" (2016, with Shan Carter)

**Research Papers**:
- "Feature Visualization" (2017, with Alexander Mordvintsev, Ludwig Schubert, and others)
- "The Building Blocks of Interpretability" (2018, with Shan Carter, Ludwig Schubert, and others)
- "Zoom In: An Introduction to Circuits" (2020, with Nick Cammarata, Gabriel Goh, and others)
- "Toy Models of Superposition" (2022, with Nelson Elhage, Tristan Hume, Tom Henighan, and others)
- "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning" (2023)
- "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet" (2024)

## Views on AI Safety

Olah has written and spoken about interpretability research as one component of AI safety rather than a complete solution. Positions he has articulated include:

**Necessity of understanding**: Deployment of powerful AI systems requires understanding their internal operations, not just observing input-output behavior.

**Tractability**: Neural networks can be understood mechanistically through sustained research effort, contrary to the view that they are inherently inscrutable.

**Complementarity**: Interpretability works alongside other safety approaches including <EntityLink id="rlhf">RLHF</EntityLink>, <EntityLink id="scalable-oversight">scalable oversight</EntityLink>, and <EntityLink id="constitutional-ai">constitutional AI</EntityLink>.

**Automation necessity**: Fully understanding large models requires using AI to assist in interpretation, as human analysis alone cannot scale to billions of parameters.

**Access requirements**: Interpretability research on frontier models requires working with those models, a consideration that influenced the decision to conduct research at <EntityLink id="anthropic">Anthropic</EntityLink> rather than academia.

## Challenges and Limitations

Several challenges to interpretability research have been identified within the field:

**Scaling limitations**: While sparse autoencoder approaches have been applied to Claude 3 Sonnet, it remains an open question whether interpretability techniques can keep pace with capability improvements in future systems.

**Verification gaps**: Understanding model internals does not automatically provide verification that models lack dangerous capabilities, as understanding is necessarily incomplete and features may be missed.

**Deceptive models**: Models exhibiting <EntityLink id="deceptive-alignment">deceptive alignment</EntityLink> might develop internal representations specifically designed to appear benign under interpretability analysis.

**Resource requirements**: Interpretability research on frontier models requires substantial computational resources and access to those models.

**Sufficiency questions**: Researchers have raised questions about whether interpretability, even if successful, provides sufficient safety guarantees for advanced AI systems, or whether it primarily offers insights rather than robust verification.

The research program at <EntityLink id="anthropic">Anthropic</EntityLink> frames interpretability as improving on treating models as black boxes, while acknowledging that current techniques do not provide complete safety guarantees.

## Influence on the Field

Interpretability research has grown as a subfield within AI safety and machine learning since the mid-2010s:

**Research groups**: Multiple organizations now have dedicated interpretability teams, including <EntityLink id="anthropic">Anthropic</EntityLink>, <EntityLink id="openai">OpenAI</EntityLink>, <EntityLink id="deepmind">Google DeepMind</EntityLink>, and others.

**Methods adoption**: Feature visualization and circuit analysis techniques are used by researchers studying neural networks across domains.

**Communication practices**: Interactive visualizations and clear explanations have become more common in machine learning research communication, a trend Distill contributed to during its period of operation.

**Safety integration**: Interpretability is included in AI safety frameworks and discussions of verification and monitoring approaches.

The extent to which current interpretability techniques will scale to future AI systems remains an active research question.
