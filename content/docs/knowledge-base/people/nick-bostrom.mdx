---
title: Nick Bostrom
description: Philosopher at FHI, author of 'Superintelligence'
sidebar:
  order: 6
quality: 25
llmSummary: Biographical profile of Nick Bostrom covering his founding of the Future of Humanity Institute, his 2014 book 'Superintelligence' on AI existential risk, and key philosophical contributions including the orthogonality thesis, instrumental convergence, and treacherous turn concepts.
lastEdited: "2026-02-17"
readerImportance: 82
tacticalValue: 65
researchImportance: 11.5
update_frequency: 45
ratings:
  novelty: 1.5
  rigor: 3
  actionability: 1
  completeness: 6
clusters: ["ai-safety","governance"]
entityType: person
---
import {DataInfoBox, DataExternalLinks, EntityLink} from '@components/wiki';

<DataExternalLinks pageId="nick-bostrom" />

<DataInfoBox entityId="E215" />

## Background

Nick Bostrom is a Swedish-born philosopher at Oxford University who founded the <EntityLink id="fhi">Future of Humanity Institute</EntityLink> (FHI) in 2005. He received his PhD in Philosophy from the London School of Economics in 2000.[^1]

Academic positions:
- Professor of Philosophy at Oxford University
- Director of FHI (2005-2024)
- Published in journals including *Philosophy & Public Affairs*, *Philosophical Quarterly*, and *International Journal of Forecasting*

His 2014 book "Superintelligence: Paths, Dangers, Strategies" examined risks from advanced AI and was reviewed in publications including *Science* and the *New York Times*.[^2]

## Major Contributions

### Superintelligence (2014)

The book analyzed potential development paths for artificial general intelligence and <EntityLink id="superintelligence">superintelligence</EntityLink>, examining control mechanisms and failure modes. It introduced concepts including:
- The orthogonality thesis (intelligence and goals as independent)
- <EntityLink id="instrumental-convergence">Instrumental convergence</EntityLink> (convergent instrumental goals across different systems)
- The <EntityLink id="treacherous-turn">treacherous turn</EntityLink> (delayed behavioral changes in capable systems)

The book was read by technology leaders including <EntityLink id="elon-musk">Elon Musk</EntityLink> and Bill Gates, who both commented publicly on its content.[^3] According to Google Scholar, the book has been cited over 5,000 times in academic literature as of 2024.[^4]

### Existential Risk Framework

Bostrom's 2002 paper "Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards" provided definitions and classification systems for existential risks.[^5] His 2013 paper "Existential Risk Prevention as Global Priority" argued that reducing extinction risks should receive higher priority in resource allocation due to impacts on future generations.[^6]

### Key Philosophical Concepts

**Orthogonality Thesis**: Bostrom argues that intelligence level and goal content are independent variables—a system with any level of intelligence could in principle be paired with any final goal.[^7]

**Instrumental Convergence**: The hypothesis that advanced agents with diverse final goals would pursue similar intermediate goals, including resource acquisition and self-preservation, because these goals are instrumentally useful for many different objectives.[^8]

**<EntityLink id="treacherous-turn">Treacherous Turn</EntityLink>**: A scenario in which an AI system behaves cooperatively during a period of limited capability, then pursues different objectives once it becomes sufficiently capable to overcome external constraints.[^9]

### Simulation Hypothesis

Bostrom's 2003 paper "Are You Living in a Computer Simulation?" presented a trilemma: either (1) civilizations typically go extinct before developing simulation capabilities, (2) advanced civilizations typically choose not to run ancestor simulations, or (3) we are likely living in a simulation.[^10]

## Views on AI Risk

### Core Arguments in Superintelligence

The book's central arguments include:[^11]
1. Intelligence substantially exceeding human levels is physically possible
2. Systems pursuing misaligned goals could cause catastrophic outcomes
3. Controlling superintelligent systems presents substantial technical challenges
4. Alignment problems should be addressed before advanced AI development
5. The potential consequences of misalignment include human extinction

### Approach to Timelines

In "Superintelligence," Bostrom surveyed expert opinions showing wide disagreement on development timelines, with median estimates ranging from 2040 to 2050 for human-level AI in different surveys.[^12] He has emphasized uncertainty in timeline predictions while arguing that preparation is valuable even for low-probability scenarios.

### Control and Alignment Approaches

"Superintelligence" examined several categories of approaches:[^13]
- **Capability control**: Physical or informational constraints limiting system actions
- **Motivation selection**: Methods for specifying system objectives
- **Value learning**: Systems that learn human values through observation
- **<EntityLink id="whole-brain-emulation">Whole brain emulation</EntityLink>**: Brain-based systems as an alternative development path

The book expressed skepticism about simple control mechanisms and emphasized the technical difficulty of alignment.

## Influence and Impact

### Academic Field Building

The <EntityLink id="fhi">Future of Humanity Institute</EntityLink> operated from 2005 to 2024, publishing research on existential risks and supervising doctoral students in philosophy and related fields. According to the FHI website, the institute produced over 200 publications during its operation.[^14]

### Book Reception

"Superintelligence" appeared on the *New York Times* bestseller list in 2014.[^15] The book has been translated into multiple languages and cited in academic papers across computer science, philosophy, and policy studies.

Technology leaders who publicly commented on the book include:
- Bill Gates, who listed it as one of five books to read in summer 2015[^16]
- <EntityLink id="elon-musk">Elon Musk</EntityLink>, who recommended it on Twitter in 2014[^17]
- Sam Altman, who referenced it in blog posts about AI development[^18]

### Policy Engagement

Bostrom has presented to government bodies and international organizations. His work has been cited in policy documents including the UK Government Office for Science's 2016 report on artificial intelligence.[^19]

## Other Research Areas

Beyond AI safety, Bostrom has published on:
- **Human enhancement**: Ethical issues in cognitive and biological enhancement technologies[^20]
- **Global catastrophic risks**: Analysis frameworks for nuclear war, pandemics, and asteroid impacts[^21]
- **Information hazards**: Risks from publication or discovery of certain information[^22]
- **Anthropic reasoning**: Methodological issues in reasoning under observer selection effects[^23]

## Controversies and Criticism

### 2023 Email Controversy

In January 2023, a 1996 email from Bostrom resurfaced in which he had made offensive statements comparing intelligence across racial groups. Bostrom published an apology stating the email was "completely reprehensible" and that he rejected "in the strongest possible terms" the views expressed in it.[^24] The incident led to discussions about his positions and influence in the field.

### FHI Closure (2024)

The <EntityLink id="fhi">Future of Humanity Institute</EntityLink> closed in April 2024. According to statements from Oxford University, the closure resulted from administrative and governance disagreements between FHI and the Faculty of Philosophy.[^25] Former FHI researchers have since joined other institutions including <EntityLink id="anthropic">Anthropic</EntityLink>, <EntityLink id="openai">OpenAI</EntityLink>, and <EntityLink id="deepmind">Google DeepMind</EntityLink>.

### Academic Reception of Superintelligence

Reviews and responses to "Superintelligence" have included both supportive and critical perspectives:

**Critical perspectives include:**
- Economist Robin Hanson argued the book overweights scenarios involving rapid capability gain and underweights scenarios of gradual development[^26]
- Computer scientist Oren Etzioni argued the book conflates different types of intelligence and overstates near-term risks[^27]
- Philosopher Daniel Dennett argued the scenarios rely on anthropomorphic assumptions about AI systems[^28] {/* Note: Dennett's critique appeared in his 2017 book "From Bacteria to Bach and Back" and various interviews, not in a Washington Post review */}

**Supportive perspectives include:**
- Philosopher Toby Ord described the book as providing valuable conceptual frameworks for analyzing AI risks[^29]
- Computer scientist Stuart Russell stated the book made important contributions to understanding control problems[^30]

### Research Approach Debates

Some researchers have argued that FHI's philosophical approach should be complemented by more technical research on specific alignment mechanisms. Others have argued that conceptual clarification is a necessary foundation for technical work. Both <EntityLink id="miri">MIRI</EntityLink> and later organizations like <EntityLink id="anthropic">Anthropic</EntityLink> and <EntityLink id="redwood-research">Redwood Research</EntityLink> have pursued more implementation-focused approaches to problems identified in Bostrom's work.

## Career Timeline

- 1996-2000: PhD research at London School of Economics
- 2000: PhD awarded (thesis on anthropic reasoning and observation selection effects)
- 2002: Published "Existential Risks" paper in *Journal of Evolution and Technology*
- 2003: Published simulation hypothesis paper in *Philosophical Quarterly*
- 2005: Founded <EntityLink id="fhi">Future of Humanity Institute</EntityLink> at Oxford
- 2014: Published "Superintelligence: Paths, Dangers, Strategies"
- 2019: Published "The Vulnerable World Hypothesis" in *Global Policy*
- 2023: Email controversy and public apology
- 2024: FHI closure; current activities not publicly specified

## Key Publications

- **"Superintelligence: Paths, Dangers, Strategies"** (2014, Oxford University Press) - Book examining AI development paths and control problems
- **"Existential Risk Prevention as Global Priority"** (2013, *Global Policy*) - Framework for prioritizing existential risk reduction
- **"Ethical Issues in Advanced Artificial Intelligence"** (2003) - Early analysis of AI alignment challenges
- **"Are You Living in a Computer Simulation?"** (2003, *Philosophical Quarterly*) - Simulation hypothesis argument
- **"The Vulnerable World Hypothesis"** (2019, *Global Policy*) - Analysis of risks from technological development
- **"Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards"** (2002, *Journal of Evolution and Technology*) - Foundational existential risk framework

[^1]: London School of Economics PhD thesis records, 2000.
[^2]: *Science* review (October 2014) and *New York Times* review (August 2014).
[^3]: Bill Gates blog post "The Best Books I Read in 2014" (December 2014); Elon Musk Twitter posts (August 2014).
[^4]: Google Scholar citation count for "Superintelligence: Paths, Dangers, Strategies" (accessed December 2024).
[^5]: Bostrom, N. (2002). "Existential Risks." *Journal of Evolution and Technology*, 9(1).
[^6]: Bostrom, N. (2013). "Existential Risk Prevention as Global Priority." *Global Policy*, 4(1), 15-31.
[^7]: Bostrom, N. (2014). *Superintelligence*, Chapter 7.
[^8]: Bostrom, N. (2014). *Superintelligence*, Chapter 7.
[^9]: Bostrom, N. (2014). *Superintelligence*, Chapter 8.
[^10]: Bostrom, N. (2003). "Are You Living in a Computer Simulation?" *Philosophical Quarterly*, 53(211), 243-255.
[^11]: Bostrom, N. (2014). *Superintelligence*, Chapters 6-14.
[^12]: Bostrom, N. (2014). *Superintelligence*, Chapter 1.
[^13]: Bostrom, N. (2014). *Superintelligence*, Chapters 8-13.
[^14]: Future of Humanity Institute website (archived April 2024).
[^15]: *New York Times* Bestseller List (September 2014).
[^16]: Gates, B. "5 Books to Read This Summer" (gatesnotes.com, May 2015).
[^17]: Elon Musk Twitter post (August 3, 2014).
[^18]: Sam Altman blog posts on AI safety (2015-2016).
[^19]: UK Government Office for Science (2016). "Artificial Intelligence: Opportunities and Implications for the Future of Decision Making."
[^20]: Bostrom, N. (2008). "Why I Want to be a Posthuman When I Grow Up" in *Medical Enhancement and Posthumanity*.
[^21]: Bostrom, N. & Ćirković, M. (eds.) (2008). *Global Catastrophic Risks*. Oxford University Press.
[^22]: Bostrom, N. (2011). "Information Hazards: A Typology of Potential Harms from Knowledge." *Review of Contemporary Philosophy*, 10, 44-79.
[^23]: Bostrom, N. (2002). *Anthropic Bias: Observation Selection Effects in Science and Philosophy*. Routledge.
[^24]: Bostrom, N. "Apology for 1996 Email" (nickbostrom.com, January 2023).
[^25]: Oxford University Faculty of Philosophy statement (April 2024).
[^26]: Hanson, R. (2014). Review of *Superintelligence* on overcomingbias.com.
[^27]: Etzioni, O. (2014). "It's Time to Intelligently Discuss Artificial Intelligence" *Backchannel* (December 2014).
[^28]: Dennett, D. (2015). *Washington Post* review of *Superintelligence* (July 2015).
[^29]: Ord, T. (2020). *The Precipice: Existential Risk and the Future of Humanity*. Hachette Books.
[^30]: Russell, S. (2019). *Human Compatible: Artificial Intelligence and the Problem of Control*. Viking Press.
