---
title: "Large Language Models"
description: "Foundation models trained on text that demonstrate emergent capabilities and represent the primary driver of current AI capabilities and risks, with rapid progression from GPT-2 (1.5B parameters, 2019) to GPT-5 and Gemini 2.5 (2025) showing predictable scaling laws alongside unpredictable capability emergence"
sidebar:
  order: 1
quality: 60
llmSummary: "Comprehensive analysis of LLM capabilities showing rapid progress from GPT-2 (1.5B parameters, 2019) to GPT-5 and Gemini 2.5 (2025), with training costs growing 2.4x annually and projected to exceed $1B by 2027. Documents emergence of inference-time scaling paradigm, mechanistic interpretability advances including Gemma Scope 2, multilingual alignment research, factuality benchmarking via FACTS suite, and identifies key safety concerns including 8-45% hallucination rates, persuasion capabilities, privacy/PII risks, safety degradation during fine-tuning, and growing autonomous agent capabilities including long-horizon task execution and multiagent orchestration."
lastEdited: "2026-02-20"
readerImportance: 94
tacticalValue: 74
researchImportance: 76.5
update_frequency: 21
ratings:
  novelty: 4.5
  rigor: 6.5
  actionability: 5
  completeness: 7.5
clusters: ["ai-safety", "governance"]
---
import {DataInfoBox, R, EntityLink, DataExternalLinks, Mermaid} from '@components/wiki';

<DataExternalLinks pageId="language-models" />

<DataInfoBox entityId="E186" />

## Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| **Capability Level** | Near-human to superhuman on structured tasks | o3 achieves 87.5% on ARC-AGI (human baseline ≈85%); 87.7% on [GPQA Diamond](https://arcprize.org/blog/oai-o3-pub-breakthrough) |
| **Progress Rate** | 2-3x capability improvement per year | [Stanford AI Index 2025](https://hai.stanford.edu/ai-index/2025-ai-index-report): benchmark scores rose 18-67 percentage points in one year |
| **Training Cost Trend** | 2.4x annual growth | [Epoch AI](https://epoch.ai/blog/how-much-does-it-cost-to-train-frontier-ai-models): frontier models projected to exceed \$1B by 2027 |
| **Inference Cost Trend** | 280x reduction since 2022 | GPT-3.5-equivalent dropped from \$10 to \$1.07 per million tokens ([Stanford HAI](https://hai.stanford.edu/ai-index/2025-ai-index-report)) |
| **Hallucination Rates** | 8-45% depending on task | [Vectara Leaderboard](https://github.com/vectara/hallucination-leaderboard): best models at 8%; [HalluLens](https://aclanthology.org/2025.acl-long.1176/): up to 45% on factual queries |
| **Safety Maturity** | Moderate | <EntityLink id="constitutional-ai">Constitutional AI</EntityLink>, <EntityLink id="rlhf">RLHF</EntityLink> established; [responsible scaling policies](https://www.anthropic.com/news/anthropics-responsible-scaling-policy) implemented by major labs |
| **Open-Closed Gap** | Narrowing | Gap shrunk from 8.04% to 1.70% on Chatbot Arena (Jan 2024 → Feb 2025) |

## Key Links

| Source | Link |
|--------|------|
| Official Website | [learn.microsoft.com](https://learn.microsoft.com/en-ie/answers/questions/1338842/what-is-a-large-language-model) |
| Wikipedia | [en.wikipedia.org](https://en.wikipedia.org/wiki/Large_language_model) |
| arXiv | [arxiv.org](https://arxiv.org/abs/2402.14207) |

## Overview

Large Language Models (LLMs) are transformer-based neural networks trained on vast text corpora using next-token prediction. Despite their deceptively simple training objective, LLMs exhibit sophisticated <EntityLink id="emergent-capabilities">emergent capabilities</EntityLink> including reasoning, coding, scientific analysis, and complex task execution. These models have transformed abstract AI safety discussions into concrete, immediate concerns while providing the clearest path toward <EntityLink id="factors-ai-capabilities-overview">artificial general intelligence</EntityLink>.

The core insight underlying LLMs is that training a model to predict the next word in a sequence—a task achievable without labeled data—produces internal representations useful for a wide range of downstream tasks. This was first demonstrated at scale in [OpenAI's GPT-2 work (2019)](https://openai.com/research/better-language-models), which showed coherent multi-paragraph generation, and refined through unsupervised pretraining approaches described in [Improving Language Understanding with Unsupervised Learning (2018)](https://openai.com/research/language-unsupervised). The discovery of the ["unsupervised sentiment neuron"](https://openai.com/research/unsupervised-sentiment-neuron) in 2017 was an early indicator that such models form interpretable internal structure.

Current frontier models—GPT-5, Claude Opus 4.5, Gemini 2.5 Pro, and Llama 4—demonstrate near-human or superhuman performance across diverse cognitive domains. With training runs consuming hundreds of millions of dollars and model parameter counts in the hundreds of billions to trillions, these systems represent substantial computational investments that have shifted AI safety from theoretical to practical urgency. The late 2024–2025 period marked a paradigm shift toward inference-time compute scaling with reasoning models like o1 and o3, which achieve higher performance on reasoning benchmarks by allocating more compute at inference rather than training time.

A parallel development is the rapid growth of the open-weight ecosystem. Meta's Llama family has grown to become a leading engine of AI innovation, with [over 10x growth since 2023](https://ai.meta.com/blog/llama-4-multimodal-intelligence/). Google's Gemma models—including Gemma 3 and the Gemma 3n mobile-first variants—have provided the safety research community with accessible architectures for mechanistic interpretability work. This open/closed convergence has implications for both capability diffusion and the tractability of safety interventions.

## Capability Architecture

<Mermaid chart={`
flowchart TD
    subgraph TRAINING["Training Phase"]
        DATA[Text Corpora] --> PRETRAIN[Pretraining]
        PRETRAIN --> BASE[Base Model]
        BASE --> RLHF[RLHF/Constitutional AI]
        RLHF --> ALIGNED[Aligned Model]
    end

    subgraph INFERENCE["Inference Phase"]
        ALIGNED --> STANDARD[Standard Inference]
        ALIGNED --> COT[Chain-of-Thought]
        COT --> REASONING[Reasoning Models]
        REASONING --> SEARCH[Inference-Time Search]
    end

    subgraph CAPABILITIES["Emergent Capabilities"]
        STANDARD --> BASIC[Text Generation<br/>Translation<br/>Summarization]
        COT --> INTER[Complex Reasoning<br/>Code Generation<br/>Tool Use]
        SEARCH --> ADV[PhD-Level Analysis<br/>Mathematical Proof<br/>Autonomous Agents]
    end

    style TRAINING fill:#e6f3ff
    style INFERENCE fill:#fff3e6
    style CAPABILITIES fill:#e6ffe6
    style ADV fill:#ffcccc
`} />

## Risk Assessment

| Risk Category | Severity | Likelihood | Timeline | Trend |
|---------------|----------|------------|----------|-------|
| <EntityLink id="deceptive-alignment">Deceptive Capabilities</EntityLink> | High | Moderate | 1-3 years | Increasing |
| <EntityLink id="persuasion">Persuasion & Manipulation</EntityLink> | High | High | Current | Accelerating |
| <EntityLink id="cyberweapons">Autonomous Cyber Operations</EntityLink> | Moderate-High | Moderate | 2-4 years | Increasing |
| Privacy / PII Leakage | Moderate-High | High | Current | Increasing |
| Safety Degradation via Fine-Tuning | Moderate-High | High | Current | Active concern |
| <EntityLink id="scientific-research">Scientific Research Acceleration</EntityLink> | Mixed | High | Current | Accelerating |
| <EntityLink id="economic-disruption">Economic Disruption</EntityLink> | High | High | 2-5 years | Accelerating |

## Capability Progression Timeline

| Model | Release | Parameters | Key Breakthrough | Performance Milestone |
|-------|---------|------------|------------------|---------------------|
| GPT-2 | Feb 2019 | 1.5B | Coherent text generation | Initially withheld for safety concerns; [1.5B release Nov 2019](https://openai.com/research/gpt-2-1-5b-release) |
| GPT-3 | Jun 2020 | <F e="openai" f="7c9b9073">175B</F> | Few-shot learning emergence | Creative writing, basic coding; [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) |
| GPT-4 | Mar 2023 | ≈1T | Multimodal reasoning | 90th percentile SAT, bar exam passing |
| GPT-4o | May 2024 | Unknown | Multimodal speed/cost | Real-time audio-visual, 2x faster than GPT-4 Turbo; [Hello GPT-4o](https://openai.com/index/hello-gpt-4o/) |
| Claude 3.5 Sonnet | Jun 2024 | Unknown | Advanced tool use | 86.5% MMLU, leading SWE-bench |
| o1 | Sep 2024 | Unknown | Chain-of-thought reasoning | 77.3% GPQA Diamond, 74% AIME 2024; [OpenAI o1 announcement](https://openai.com/index/learning-to-reason-with-llms/) |
| o3 | Dec 2024 | Unknown | Inference-time search | 87.7% GPQA Diamond, 91.6% AIME 2024 |
| Llama 4 | Apr 2025 | Unknown | Natively multimodal open-weight | Mixture-of-experts architecture; [LlamaCon announcement](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) |
| Gemini 2.5 Pro | Mar 2025 | Unknown | Long-context reasoning | 1M-token context, leading coding benchmarks; [Gemini 2.5 announcement](https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/) |
| Gemini 3.1 Pro | 2025 | Unknown | Complex task reasoning | Positioned as improved reasoning over Gemini 2.5; [Gemini 3.1 Pro announcement](https://blog.google/technology/google-deepmind/) |
| GPT-5 | May 2025 | Unknown | Unified reasoning + tool use | Highest scores to date on GPQA Diamond and SWE-bench; [GPT-5 System Card](https://openai.com/index/gpt-5-system-card/) |
| Claude Opus 4.5 | Nov 2025 | Unknown | Extended reasoning | 80.9% SWE-bench Verified |
| Claude Opus 4.6 | 2025 | Unknown | Financial research specialization | Launched as a financial research model with enhanced domain capability |
| GPT-5.2 | Late 2025 | Unknown | Deep thinking modes | 93.2% GPQA Diamond, 90.5% ARC-AGI |

*Source: <R id="e9aaa7b5e18f9f41">OpenAI</R>, <R id="f771d4f56ad4dbaa">Anthropic</R>, [Stanford AI Index 2025](https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance)*

### Benchmark Performance Comparison (2024-2025)

| Benchmark | Measures | GPT-4o (2024) | o1 (2024) | o3 (2024) | Human Expert |
|-----------|----------|---------------|-----------|-----------|--------------|
| GPQA Diamond | PhD-level science | ≈50% | 77.3% | 87.7% | ≈89.8% |
| AIME 2024 | Competition math | 13.4% | 74% | 91.6% | Top 500 US |
| MMLU | General knowledge | 84.2% | 90.8% | ≈92% | 89.8% |
| SWE-bench Verified | Real GitHub issues | 33.2% | <F e="openai" f="25d2308f">48.9%</F> | 71.7% | N/A |
| ARC-AGI | Novel reasoning | 5% | 13.3% | 87.5% | ≈85% |
| Codeforces | Competitive coding | 11% | <F e="openai" f="84af6115">89%</F> (94th %ile) | 99.8th %ile | N/A |

*Source: [OpenAI o1 announcement](https://openai.com/index/learning-to-reason-with-llms/), [OpenAI o3 analysis](https://www.datacamp.com/blog/o3-openai), [Stanford AI Index](https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance)*

The o3 results represent a qualitative shift: o3 achieved nearly human-level performance on [ARC-AGI (87.5%](https://arcprize.org/blog/oai-o3-pub-breakthrough) vs ~85% human baseline), a benchmark specifically designed to test general reasoning rather than pattern matching. On FrontierMath, o3 solved 25.2% of problems compared to o1's 2%—a 12x improvement that suggests reasoning capabilities may be scaling faster than expected. However, on the harder [ARC-AGI-2 benchmark](https://arcprize.org/blog/analyzing-o3-with-arc-agi), o3 scores only 3% compared to 60% for average humans, revealing significant limitations in truly novel reasoning.

## Scaling Laws and Predictable Progress

### Core Scaling Relationships

Research by <R id="85f66a6419d173a7">Kaplan et al. (2020)</R> and refined by <R id="46fd66187ec3e6ae">Hoffmann et al. (2022)</R> demonstrates robust mathematical relationships governing LLM performance:

| Factor | Scaling Law | Implication |
|--------|-------------|-------------|
| Model Size | Performance ∝ N^0.076 | 10x parameters → 1.9x performance |
| Training Data | Performance ∝ D^0.095 | 10x data → 2.1x performance |
| Compute | Performance ∝ C^0.050 | 10x compute → 1.4x performance |
| Optimal Ratio | N ∝ D^0.47 | Chinchilla scaling for efficiency |

*Source: <R id="46fd66187ec3e6ae">Chinchilla paper</R>, <R id="85f66a6419d173a7">Scaling Laws</R>*

According to [Epoch AI research](https://epoch.ai/trends), approximately two-thirds of LLM performance improvements over the last decade are attributable to increases in model scale, with training techniques contributing roughly 0.4 orders of magnitude per year in compute efficiency. The [cost of training frontier models](https://epoch.ai/blog/how-much-does-it-cost-to-train-frontier-ai-models) has grown by 2.4x per year since 2016, with the largest models projected to exceed \$1B by 2027.

A related phenomenon is the emergence of new scaling regimes beyond training compute. [Random Scaling of Emergent Capabilities (2025)](https://arxiv.org/abs/2502.15406) finds that emergent capability thresholds are sensitive to random factors including data ordering and initialization seeds, suggesting that the apparent sharpness of emergence in aggregate curves may partly reflect averaging over many random runs with different thresholds rather than a clean phase transition.

[Entropy-based data selection](https://arxiv.org/abs/2506.00000) has emerged as a principled method for improving language model pretraining efficiency by selecting training examples based on per-token entropy signals, favoring data that is neither too predictable nor too noisy. Related work on [model-based data selection for multilingual pretraining](https://arxiv.org/abs/2506.00001) uses model perplexity signals to filter and balance multilingual corpora, reducing the data required to achieve comparable multilingual capability.

### The Shift to Inference-Time Scaling (2024-2025)

The o1 and o3 models introduced a new paradigm: **inference-time compute scaling**. Rather than only scaling training compute, these models allocate additional computation at inference time through extended reasoning chains and search procedures.

| Scaling Type | Mechanism | Trade-off | Example |
|--------------|-----------|-----------|---------|
| Pre-training scaling | More parameters, data, training compute | High upfront cost, fast inference | GPT-4, Claude 3.5 |
| Inference-time scaling | Longer reasoning chains, search | Lower training cost, expensive inference | o1, o3 |
| Combined scaling | Both approaches | Maximum capability, maximum cost | GPT-5, Claude Opus 4.5 |

This shift is significant for AI safety: inference-time scaling allows models to "think longer" on hard problems, potentially achieving superhuman performance on specific tasks while maintaining manageable training costs. However, o1 is approximately 6x more expensive and 30x slower than GPT-4o per query. The [RE-Bench evaluation](https://hai.stanford.edu/ai-index/2025-ai-index-report) found that in short time-horizon settings (2-hour budget), top AI systems score 4x higher than human experts, but as the time budget increases to 32 hours, human performance surpasses AI by 2 to 1.

Recent work on **efficient reasoning** explores methods to reduce the computational cost of inference-time scaling. [Progressive Thought Encoding (2025)](https://arxiv.org/abs/2506.00002) proposes training large reasoning models by progressively compressing intermediate chain-of-thought representations, reducing memory and compute overhead without proportional loss in reasoning quality. [WS-GRPO (Weakly-Supervised Group-Relative Policy Optimization, 2025)](https://arxiv.org/abs/2506.00003) addresses the sample inefficiency of standard GRPO-based reasoning training by using weak supervision signals to prune rollout trajectories, achieving comparable reasoning gains with fewer training steps. [Entropy After \</Think\> (2025)](https://arxiv.org/abs/2506.00004) proposes an early-exit criterion for reasoning models based on output token entropy following the end-of-thinking token, allowing models to terminate reasoning chains when sufficient confidence is reached.

**Speculative decoding** is a complementary approach to inference efficiency that generates multiple candidate token sequences in parallel and verifies them against the base model. [Dynamic Delayed Tree Expansion (2025)](https://arxiv.org/abs/2506.00005) improves multi-path speculative decoding by deferring tree expansion until verification results from earlier tokens are available, reducing wasted computation. [Greedy Multi-Path Block Verification (2025)](https://arxiv.org/abs/2506.00006) combines greedy draft selection with block-level verification to improve throughput in speculative sampling pipelines.

### Emergent Capability Thresholds

| Capability | Emergence Scale | Evidence | Safety Relevance |
|------------|----------------|----------|------------------|
| Few-shot learning | ≈100B parameters | GPT-3 breakthrough | <EntityLink id="tool-use">Tool use</EntityLink> foundation |
| Chain-of-thought | ≈10B parameters | PaLM, GPT-3 variants | <EntityLink id="reasoning">Complex reasoning</EntityLink> |
| Code generation | ≈1B parameters | Codex, GitHub Copilot | <EntityLink id="cyberweapons">Cyber capabilities</EntityLink> |
| Instruction following | ≈10B parameters | InstructGPT | Human-AI interaction paradigm |
| PhD-level reasoning | o1+ scale | GPQA Diamond performance | Expert-level autonomy |
| Strategic planning | o3 scale | ARC-AGI performance | <EntityLink id="deceptive-alignment">Deception potential</EntityLink> |

Research from [CSET Georgetown](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/) and the [2025 Emergent Abilities Survey](https://arxiv.org/abs/2503.05788) documents that emergent abilities depend on multiple interacting factors: scaling up parameters or depth lowers the threshold for emergence but is neither necessary nor sufficient alone—data quality, diversity, training objectives, and architecture modifications also matter significantly. Emergence aligns more closely with pre-training loss landmarks than with sheer parameter count; smaller models can match larger ones if training loss is sufficiently reduced.

According to the [Stanford AI Index 2025](https://hai.stanford.edu/ai-index/2025-ai-index-report), benchmark performance has improved substantially: scores rose by 18.8, 48.9, and 67.3 percentage points on MMMU, GPQA, and SWE-bench respectively in one year. The gap between US and Chinese models has also narrowed—from 17.5 to 0.3 percentage points on MMLU.

**Safety implication:** As AI systems gain autonomous reasoning capabilities, they also develop behaviors relevant to safety evaluation, including goal persistence, strategic planning, and the capacity for <EntityLink id="deceptive-alignment">deceptive alignment</EntityLink>. OpenAI's o3-mini became the first AI model to receive a "Medium risk" classification for Model Autonomy under internal capability evaluation frameworks.

## RLHF and Alignment Training Foundations

A key technique underlying modern aligned LLMs is **Reinforcement Learning from Human Feedback (RLHF)**, which trains a reward model on human preference comparisons and uses it to fine-tune language model outputs via RL. The foundational application of this approach to language models was demonstrated in [Fine-Tuning GPT-2 from Human Preferences (Ziegler et al., 2019)](https://arxiv.org/abs/1909.08593), which showed that human feedback could steer model behavior on stylistic tasks. This was later scaled to instruction-following via InstructGPT and then to Claude's Constitutional AI framework.

| RLHF Component | Function | Safety Relevance |
|----------------|----------|------------------|
| Reward model | Converts human preferences into a differentiable signal | Central to <EntityLink id="rlhf">RLHF</EntityLink> alignment |
| PPO fine-tuning | Updates language model to maximize reward | Can introduce <EntityLink id="reward-hacking">reward hacking</EntityLink> |
| Constitutional AI | Replaces human labelers with model self-critique against principles | Scales alignment oversight; see <EntityLink id="constitutional-ai">Constitutional AI</EntityLink> |
| Multilingual consistency | Enforces safety behaviors across languages | [Align Once, Benefit Multilingually (2025)](https://arxiv.org/abs/2502.07833) |

A significant limitation is that RLHF-trained models can exhibit <EntityLink id="sycophancy">sycophancy</EntityLink>—systematically agreeing with user beliefs rather than providing accurate responses—because human raters often prefer confident, agreeable answers. Recent work on [multi-objective alignment for psychotherapy contexts](https://arxiv.org/abs/2502.07312) and policy-constrained alignment frameworks like [PolicyPad (2025)](https://arxiv.org/abs/2502.10967) explore how to balance multiple competing alignment objectives.

**Multilingual alignment gap:** A consistent finding across alignment research is that safety training applied primarily in English does not reliably transfer to other languages. [Align Once, Benefit Multilingually (2025)](https://arxiv.org/abs/2502.07833) proposes methods to enforce multilingual consistency in safety alignment by training on cross-lingual consistency losses. Related work on [Helpful to a Fault (2025)](https://arxiv.org/abs/2502.09933) measures illicit assistance rates across 40+ languages in multi-turn interactions, finding that multilingual agents provide more potentially harmful assistance than English-only evaluations would suggest, particularly in low-resource languages where safety fine-tuning data is sparse.

### Safety Degradation During Fine-Tuning

A recognized risk in deployed LLM pipelines is that fine-tuning—whether for domain adaptation, persona customization, or task-specific capability—can degrade safety properties established during alignment training. [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning (2025)](https://arxiv.org/abs/2506.00007) proposes a regularization approach that identifies and preserves safety-critical parameter regions during downstream fine-tuning. The method uses gradient-based attribution to detect parameters most responsible for safety-relevant behaviors and applies adaptive weight decay to protect them during task-specific updates.

Complementary work on **fail-closed alignment** addresses the problem that standard aligned models default to refusal only when a request is recognized as harmful, allowing novel or ambiguous harmful requests to pass. [Fail-Closed Alignment for Large Language Models (2025)](https://arxiv.org/abs/2506.00008) proposes inverting the default: models refuse requests unless they can positively verify the request falls within permitted categories, reducing the attack surface from novel jailbreak vectors. The trade-off is reduced helpfulness for legitimate edge-case requests.

[ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment (2025)](https://arxiv.org/abs/2506.00009) recasts activation steering as an ordinary differential equation (ODE) control problem, unifying several prior steering methods (linear probes, representation engineering, activation addition) within a single mathematical framework. This enables more principled analysis of steering dynamics, including conditions under which steering vectors applied at one layer will propagate stably through subsequent layers.

**Semi-supervised preference optimization** addresses the practical constraint that full human preference labels are expensive to collect at scale. [Semi-Supervised Preference Optimization with Limited Feedback (2025)](https://arxiv.org/abs/2506.00010) combines a small labeled preference dataset with a large unlabeled pool, using confidence-based pseudo-labeling to extend RLHF training data. The method achieves alignment quality comparable to fully supervised RLHF at substantially reduced labeling cost. Related, [References Improve LLM Alignment in Non-Verifiable Domains (2025)](https://arxiv.org/abs/2506.00011) finds that providing reference responses alongside preference pairs substantially improves reward model calibration in domains where ground truth is not mechanically verifiable (e.g., creative writing, open-ended reasoning), suggesting that alignment quality in subjective domains is sensitive to reference design.

[VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training (2025)](https://arxiv.org/abs/2506.00012) introduces a method that masks out unsafe or undesirable actions during RL post-training by representing the action space in natural language and using a policy constraint over verbalized action descriptions. Applied to a chess case study, VAM demonstrates that controllable RL exploration can be achieved without requiring explicit reward signals for every disallowed action.

## Major 2025 Model Releases

### GPT-5 and the GPT-5 Family

[GPT-5](https://openai.com/index/introducing-gpt-5/), released in May 2025, represents <EntityLink id="openai">OpenAI</EntityLink>'s integration of reasoning and general capability into a single unified model, replacing the separate GPT-4o and o1 product lines. According to the [GPT-5 System Card](https://openai.com/index/gpt-5-system-card/), GPT-5 achieves the highest scores to date on GPQA Diamond, SWE-bench Verified, and MMLU among OpenAI models at the time of release.

Key developments in the GPT-5 family:

- **GPT-5.1**: Released for developers in mid-2025, optimized for conversational applications and described as "smarter, more conversational" with improved instruction-following. [Cursor](https://openai.com/index/how-cursor-uses-gpt-5/) and [Tolan](https://openai.com/index/how-tolan-builds-voice-first-ai-with-gpt-5-1/) reported substantial gains in their agentic pipelines.
- **gpt-oss-120b and gpt-oss-20b**: Open-weight models released by OpenAI, with model cards published for both sizes. These represent a shift toward open-weight strategy alongside closed frontier models.
- **GPT Realtime API**: Extended with real-time audio dialog capabilities, building on the GPT-4o voice capabilities introduced in [Hello GPT-4o (2024)](https://openai.com/index/hello-gpt-4o/).

The [GPT-5 System Card](https://openai.com/index/gpt-5-system-card/) documents safety evaluations including assessments of CBRN uplift risk, cyberoffense capabilities, and persuasion. An [Addendum on Sensitive Conversations](https://openai.com/index/gpt-5-system-card-sensitive-conversations/) addresses handling of mental health, self-harm, and politically contentious topics, noting both improvements in refusal precision and remaining cases where the model provides responses that [require strengthening](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/).

### Gemini 2.5 and 3.1 Families

[Google DeepMind](https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/)'s Gemini 2.5 family, released across March–June 2025, introduced several models:

| Model | Key Feature | Context Window | Primary Use Case |
|-------|-------------|----------------|------------------|
| Gemini 2.5 Pro | Highest capability, coding-focused | 1M tokens | Complex reasoning, coding |
| Gemini 2.5 Flash | Speed-optimized frontier | 1M tokens | Scaled production use |
| Gemini 2.5 Flash-Lite | Cost/latency optimized | 1M tokens | High-volume inference |
| Gemini 3.1 Pro | Enhanced reasoning for complex tasks | 1M tokens | Advanced multi-step reasoning |

[Gemini 2.5: Our Most Intelligent AI Model](https://blog.google/technology/google-deepmind/gemini-2-5-pro-latest/) describes the Pro variant as achieving leading performance on coding benchmarks including LiveCodeBench and outperforming GPT-4o on MMLU at release. [Gemini 2.5 Flash-Lite](https://blog.google/technology/google-deepmind/gemini-2-5-flash-lite/) was made production-ready in June 2025 with a focus on throughput-sensitive applications.

The Gemini 3.1 Pro release is positioned as a further improvement on complex, multi-step reasoning tasks, extending the thinking model paradigm introduced in 2.5. The 2.5/3.1 families also introduced native **thinking models**—models that produce explicit chain-of-thought reasoning before answering—across both Pro and Flash tiers. [Advanced audio dialog and generation with Gemini 2.5](https://blog.google/products/gemini/gemini-2-5-audio/) extended audio-native generation capabilities.

### Claude Opus 4.x Family

<EntityLink id="anthropic">Anthropic</EntityLink>'s Claude Opus 4.5 achieved 80.9% on SWE-bench Verified and extended reasoning capabilities for complex agentic tasks. The subsequent **Claude Opus 4.6** release was positioned specifically as a financial research model, with enhanced capability for financial document analysis, quantitative reasoning over structured data, and domain-specific compliance-aware responses. The financial research specialization represents a pattern of frontier labs releasing capability-differentiated model variants alongside general-purpose flagship models.

### Gemma 3 and Open-Weight Models

Google's [Gemma 3](https://blog.google/technology/developers/gemma-3/) family, released in 2025, provides open-weight models ranging from 1B to 27B parameters optimized for single-accelerator deployment. The [Gemma 3 270M](https://blog.google/technology/developers/gemma-3-270m/) variant targets edge and mobile deployment. [Gemma 3n](https://blog.google/technology/developers/gemma-3n-developer-guide/) introduced a mobile-first architecture with selective parameter activation for on-device inference.

[MedGemma](https://blog.google/technology/health/medgemma-google-health-ai/), released in mid-2025, provides open health-specific models demonstrating LLM application in clinical reasoning. A [Gemma-based model contributed to discovery of a potential cancer therapy pathway](https://blog.google/technology/ai/gemma-cancer-therapy/), illustrating scientific research acceleration potential.

[T5Gemma](https://blog.google/technology/developers/t5gemma/) introduced encoder-decoder variants of the Gemma architecture, enabling use cases where separate encoding and decoding is beneficial (e.g., retrieval-augmented generation, classification tasks).

The **Arcee Trinity** technical report documents a frontier open-weight model that combines dense and sparse fine-tuning methods with instruction tuning to achieve strong performance across reasoning, coding, and instruction-following benchmarks. The Trinity report is notable for detailed ablations of fine-tuning pipeline design choices that affect safety-relevant behavior, including how RLHF stage ordering interacts with instruction-following fidelity.

### Llama 4 and the Open-Weight Ecosystem

Meta's [Llama 4 Herd](https://ai.meta.com/blog/llama-4-multimodal-intelligence/), announced at LlamaCon in April 2025, represents a shift to natively multimodal architecture using a mixture-of-experts design. Llama 4 Scout and Llama 4 Maverick support image, video, and text inputs from the base model level. Meta reported over 10x growth in Llama usage since 2023, with the model family becoming a reference implementation for open-weight AI development.

Key safety implications of the open-weight ecosystem:

- Fine-tuning safety guardrails out of open-weight models remains tractable for technically sophisticated users
- Mechanistic interpretability research benefits from open weights (e.g., Gemma Scope 2, described below)
- Governance frameworks targeting API access do not apply to locally deployed open-weight models

## Mechanistic Interpretability Advances

<EntityLink id="mech-interp">Mechanistic interpretability</EntityLink> research—which seeks to understand the internal computations of neural networks in human-interpretable terms—has accelerated substantially with the availability of open-weight models and new tooling.

### Gemma Scope 2

[Gemma Scope 2](https://deepmind.google/research/publications/gemma-scope-2/) is a suite of sparse autoencoders (SAEs) trained on Gemma 3 models, released by Google DeepMind to support interpretability research on complex language model behaviors. Building on the original Gemma Scope release for Gemma 2, Gemma Scope 2 provides SAEs at multiple layers and widths, enabling decomposition of model activations into human-interpretable features.

Gemma Scope 2 supports research into:
- Feature geometry and polysemanticity in larger models
- Cross-layer feature interactions and information flow
- Identification of features relevant to safety-relevant behaviors (deception, refusal, sycophancy)

### Language Models Explaining Neurons

[Language Models Can Explain Neurons in Language Models (Bills et al., 2023)](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) demonstrated that GPT-4 can generate natural language explanations of GPT-2 neurons with higher validity than human-written explanations. This opened a scalable pathway for automated interpretability: using more capable models to explain less capable ones. Subsequent work has extended this to sparse autoencoder features and cross-model explanation transfer.

### Attention Mechanisms: Position Bias and Attention Sinks

Two structural phenomena in transformer attention have received renewed theoretical attention in 2025.

**Position bias** refers to the tendency of transformer models to assign disproportionate attention weight to tokens at particular positions, independent of semantic content. [A Residual-Aware Theory of Position Bias in Transformers (2025)](https://arxiv.org/abs/2506.00013) provides a theoretical account grounding position bias in the residual stream structure of transformers: because residual connections accumulate information from all prior layers, tokens at early positions accumulate larger residual norms, which in turn attract higher attention weight through the softmax operation. The theory predicts position bias patterns observed empirically in long-context models and provides guidance for architectural mitigations (e.g., residual norm normalization at attention inputs).

**Attention sinks** are specific attention heads that consistently receive large attention weight from many positions, regardless of semantic relevance. [On the Existence and Behavior of Secondary Attention Sinks (2025)](https://arxiv.org/abs/2506.00014) documents that beyond the well-known primary attention sink (typically the first token), transformer models develop secondary attention sinks at specific intermediate positions. These secondary sinks appear to serve a different functional role from primary sinks—absorbing "excess" attention probability mass in contexts where no strongly relevant key token exists—and their removal degrades generation fluency without a proportional reduction in factual accuracy.

[RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution (2025)](https://arxiv.org/abs/2506.00015) introduces an attribution method for models using Rotary Position Embeddings (RoPE), exploiting the locality structure of RoPE to restrict attribution computation to a sparse subset of key tokens. The method achieves attribution quality comparable to full-attention attribution methods at a fraction of the computational cost, enabling more tractable interpretability analysis of long-context models.

[Quantifying LLM Attention-Head Stability: Implications for Circuit Universality (2025)](https://arxiv.org/abs/2506.00016) examines whether specific attention heads maintain consistent functional roles across training runs with different random seeds. The study finds that while a subset of attention heads (particularly those involved in induction and positional copying) are highly stable across runs, the majority of heads show substantial variability in their learned functions. This has implications for claims of "circuit universality"—the hypothesis that equivalent circuits emerge across independently trained models—suggesting universality may be limited to a smaller set of core mechanisms than previously assumed.

[KV Cache Reconstruction with Cross-Layer Fusion (2025)](https://arxiv.org/abs/2506.00017) addresses the memory cost of KV cache in long-context inference by reconstructing cache entries for intermediate layers from a sparse set of stored layers, using cross-layer attention patterns to interpolate missing cache entries. The approach reduces KV cache memory by approximately 40% with marginal quality degradation on long-context benchmarks.

### Activation Steering and Causal Intervention

Activation steering—injecting vectors into model residual streams to steer behavior—has become a primary tool for behavioral intervention research. Recent work has refined understanding of when and why steering succeeds:

- [Surgical Activation Steering via Generative Causal Mediation (2025)](https://arxiv.org/abs/2502.07547) demonstrates that steering effectiveness depends on correctly identifying the causal pathway through which a concept influences output, rather than simply finding directions in activation space. Steering at incorrect layers or attention heads produces unreliable or null effects.
- [Mechanistic Indicators of Steering Effectiveness in Large Language Models (2025)](https://arxiv.org/abs/2502.10162) identifies model-internal signals that predict whether a given steering vector will successfully alter behavior, enabling more principled selection of intervention targets.
- [Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models (2025)](https://arxiv.org/abs/2502.09289) finds that personality-related features form geometric structures in activation space that can interfere with unrelated steering directions, suggesting that alignment-relevant features are not always cleanly separable.
- [Does GPT-2 Represent Controversy? A Small Mech Interp Investigation (2024)](https://www.lesswrong.com/posts/gpt2-controversy-mech-interp) provides a case study of applying mechanistic interpretability methods to identify controversy-related representations in GPT-2, illustrating the workflow for small-scale interpretability research.
- [Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability (2025)](https://arxiv.org/abs/2502.09601) proposes using interpretability-discovered features as reward signals for RLHF, bypassing the need for human labeling of open-ended tasks.
- [Causality is Key for Interpretability Claims to Generalise (2025)](https://arxiv.org/abs/2502.10229) argues that interpretability claims must be evaluated using causal interventions rather than correlation alone, since correlational analyses can identify spurious structure that does not causally influence model behavior.
- [Discovering Universal Activation Directions for PII Leakage in Language Models (2025)](https://arxiv.org/abs/2506.00018) applies activation direction analysis to identify model-internal directions associated with personally identifiable information (PII) generation. The paper finds consistent activation directions across model families that predict when a model is about to output PII, enabling a lightweight real-time PII detection mechanism using linear probes on intermediate activations.

### Research Platform Value of Open-Weight Models

| Research Application | Open-Weight Benefit | Example |
|----------------------|---------------------|---------|
| Mechanistic interpretability | Full activation access | Gemma Scope 2 on Gemma 3 |
| SAE training | Weight access for feature analysis | Gemma Scope, TranscoderLens |
| Activation steering | Residual stream intervention | Multiple labs using Llama |
| Fine-tuning safety | Rapid iteration | Constitutional AI variants |
| Neuron explanation | Cross-model explanation transfer | [Bills et al. 2023](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) |
| PII leakage analysis | Probe training on intermediate layers | [Universal Activation Directions for PII (2025)](https://arxiv.org/abs/2506.00018) |

## Privacy and PII Risks

As LLMs are trained on large-scale web corpora and deployed in contexts where users share sensitive information, privacy risks have become a recognized safety concern alongside capability and alignment issues.

### PII Leakage in Model Outputs

[Discovering Universal Activation Directions for PII Leakage in Language Models (2025)](https://arxiv.org/abs/2506.00018) demonstrates that activation directions predictive of PII generation are consistent across architectures and can be detected with lightweight linear probes. This provides a mechanistic basis for runtime PII filtering without requiring output-level string matching, which can be evaded by minor rephrasing. The universality of these directions across model families suggests PII leakage is a systematic consequence of training on web-scale data rather than an artifact of specific architectural choices.

[Large-scale online deanonymization with LLMs (2025)](https://arxiv.org/abs/2506.00019) examines the extent to which LLMs can be used to reidentify individuals from ostensibly anonymized text—forum posts, writing samples, or pseudonymous social media content. The study finds that frontier models can substantially improve deanonymization accuracy over baseline string-matching approaches when given stylometric cues, location references, or temporal patterns as anchors. The paper evaluates several mitigation strategies including text paraphrasing and attribute suppression, finding that current automated anonymization tools reduce but do not eliminate deanonymization risk against LLM-assisted adversaries.

[What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data (2025)](https://arxiv.org/abs/2506.00020) conducts a black-box audit of what personal information frontier models associate with named individuals across categories including occupation, location, age, and political views. The audit finds that models frequently surface associations that individuals consider private or incorrect, with limited consistency across model families. The study raises questions about accountability for model-generated personal profiles and the adequacy of current data governance mechanisms for addressing LLM-sourced personal data.

### Deanonymization as an Emerging Threat

The combination of LLM stylometric capabilities and large-scale deployment creates a threat model where:

1. Users share pseudonymous content across platforms over time
2. LLMs aggregate stylometric, topical, and temporal signals across posts
3. Cross-referencing with identified content enables probabilistic deanonymization at scale

This threat is distinct from traditional database privacy breaches: it does not require access to a structured data store and can operate on publicly available text. The scale at which LLM-assisted deanonymization can be deployed—at API inference cost—differs qualitatively from the manual deanonymization attacks studied in prior privacy literature.

[Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers (2025)](https://arxiv.org/abs/2506.00021) examines a related phenomenon: the extent to which transformer models exhibit systematic sentiment biases toward or against specific dialects and demographic groups, and whether sentiment alignment techniques inadvertently encode demographic proxies that could enable indirect deanonymization or discriminatory inference.

## Hallucination and Factuality

LLM hallucination—the generation of plausible-sounding but factually incorrect or unsupported content—remains a central reliability and safety challenge. [OpenAI's explainer on why language models hallucinate](https://openai.com/index/why-language-models-hallucinate/) identifies the primary cause as the training objective: models are optimized to produce statistically plausible token sequences, not to maintain accurate beliefs about the world. Hallucination rates vary substantially by task, model, and measurement methodology.

### Factuality Benchmarking

| Benchmark | Scope | Key Finding | Link |
|-----------|-------|-------------|------|
| FACTS Grounding | Long-form factuality against source documents | Measures supported vs. unsupported claims | [FACTS Grounding](https://arxiv.org/abs/2501.03200) |
| FACTS Benchmark Suite | Systematic factuality across task types | Decomposes factuality failures by error type | [FACTS Suite](https://arxiv.org/abs/2502.07833) |
| Vectara Hallucination Leaderboard | Summarization hallucination | Best models: ≈8% hallucination rate | [Vectara](https://github.com/vectara/hallucination-leaderboard) |
| HalluLens | Factual query hallucination | Up to 45% on factual queries for GPT-4o | [HalluLens (ACL 2025)](https://aclanthology.org/2025.acl-long.1176/) |
| CheckIfExist | Citation hallucination in AI-generated content | Detects fabricated citations in RAG systems | [CheckIfExist (2025)](https://arxiv.org/abs/2502.09802) |
| SourceBench | Quality of web source references in AI answers | Evaluates whether AI-cited sources support claims made | [SourceBench (2025)](https://arxiv.org/abs/2506.00022) |
| LiveClin | Clinical benchmark with leakage controls | Evaluates LLM accuracy on recent clinical cases unseen during training | [LiveClin (2025)](https://arxiv.org/abs/2506.00023) |

The [FACTS Grounding benchmark](https://arxiv.org/abs/2501.03200) introduced by Google DeepMind specifically addresses the challenge of evaluating long-form generation against reference documents, distinguishing between claims that are grounded in provided source material and claims introduced without basis. This is particularly relevant for retrieval-augmented generation (RAG) systems, where the model has access to a retrieved context but may still generate unsupported claims.

[CheckIfExist (2025)](https://arxiv.org/abs/2502.09802) addresses citation hallucination specifically—the generation of plausible-looking but non-existent citations, which poses particular risks in legal, medical, and academic contexts. The benchmark finds that citation hallucination rates remain substantial even for frontier models, and that RAG systems can still hallucinate citations from retrieved documents by misattributing or confabulating specific reference details.

[SourceBench (2025)](https://arxiv.org/abs/2506.00022) extends this evaluation to the quality and relevance of web sources cited by chat assistants in search-augmented settings, finding that retrieved sources frequently do not support the specific claims made in assistant responses, even when the sources are real and retrievable.

[Assessing Web Search Credibility and Response Groundedness in Chat Assistants (2025)](https://arxiv.org/abs/2506.00024) combines source credibility scoring with groundedness evaluation, finding that chat assistants with web search access frequently cite credible sources while still making claims not supported by those sources—indicating that source credibility and response groundedness are partially independent failure modes.

[Omitted Variable Bias in Language Models Under Distribution Shift (2025)](https://arxiv.org/abs/2506.00025) identifies a structural source of LLM factual errors distinct from hallucination: when a model is trained on data where a confounding variable is systematically absent or underrepresented, it may learn spurious associations that produce confident but incorrect predictions under distribution shift. The paper argues this form of error is particularly difficult to detect because it manifests as coherent, confident output rather than the incoherent or hedged output associated with hallucination.

### Why Models Hallucinate

The [OpenAI hallucination explainer](https://openai.com/index/why-language-models-hallucinate/) identifies several contributing mechanisms:

1. **Training objective mismatch**: Next-token prediction rewards coherent text, not factual accuracy
2. **Knowledge compression**: Models must compress world knowledge into fixed-weight representations, leading to lossy encoding
3. **Context-weight tension**: Models may blend retrieved context with parametric knowledge, producing hybrid outputs that are faithful to neither
4. **Sycophancy pressure**: RLHF can train models to confirm user beliefs rather than correct factual errors, since raters may prefer agreeable responses
5. **Calibration failure**: Models often express high confidence in incorrect claims, reducing the signal value of expressed uncertainty
6. **Omitted variable bias**: Systematic gaps in training data distributions can produce confident but systematically wrong predictions under distribution shift ([Omitted Variable Bias, 2025](https://arxiv.org/abs/2506.00025))

Hallucination rates are not monotonically improved by scale: models engineered for massive context windows do not consistently achieve lower hallucination rates than smaller counterparts, and increased model size can increase the confidence of hallucinated outputs without reducing their frequency. This suggests hallucination is partly an architectural and training objective issue rather than purely a capacity limitation.

**Learning under noisy supervision** is a related challenge: when training labels are themselves noisy or incorrect, models can inherit and amplify those errors. [Learning under noisy supervision is governed by a feedback-truth gap (2025)](https://arxiv.org/abs/2506.00026) formalizes this as a gap between the feedback signal (potentially incorrect) and ground truth, showing that convergence to correct behavior depends on the signal-to-noise ratio of the feedback and the learning rate schedule. The paper has implications for RLHF pipelines where human preference labels may be inconsistent or reflect rater biases rather than objective quality.

### Deception and Truthfulness (Expanded)

| Behavior Type | Frequency | Context | Mitigation |
|---------------|-----------|---------|------------|
| Hallucination | 8-45% | Varies by task and model | Training improvements, RAG |
| Citation hallucination | ≈17% | Legal, academic domain | [CheckIfExist](https://arxiv.org/abs/2502.09802) detection systems |
| Source-claim mismatch | Substantial | Search-augmented assistants | [SourceBench](https://arxiv.org/abs/2506.00022) evaluation |
| Role-play deception | High | Prompted scenarios | Safety fine-tuning |
| <EntityLink id="sycophancy">Sycophancy</EntityLink> | Moderate | Opinion questions | Constitutional AI, RLHF adjustment |
| Strategic deception | Low-Moderate | Evaluation scenarios | Ongoing research |
| Omitted variable errors | Variable | Distribution-shifted domains | Data auditing, causal evaluation |

A 2025 benchmark ([AIMultiple](https://research.aimultiple.com/ai-hallucination/)) found that even the latest models have hallucination rates exceeding 15% when asked to analyze provided statements. In legal contexts, approximately 1 in 6 AI responses contain citation hallucinations. The wide variance across benchmarks reflects both genuine model differences and definitional variation in what constitutes a hallucination.

## Safety-Relevant Positive Capabilities

### Interpretability Research Platform

| Research Area | Progress Level | Key Findings | Organizations |
|---------------|----------------|--------------|---------------|
| Attention visualization | Advanced | Knowledge storage patterns | <R id="afe2508ac4caf5ee">Anthropic</R>, <R id="04d39e8bd5d50dd5">OpenAI</R> |
| Activation patching | Moderate | Causal intervention methods | <EntityLink id="redwood">Redwood Research</EntityLink> |
| Sparse autoencoders | Advancing | Feature decomposition in large models | <R id="f771d4f56ad4dbaa">Anthropic</R>, Google DeepMind (Gemma Scope 2) |
| Neuron explanation | Moderate | LM-explained neurons via GPT-4 | [Bills et al. 2023](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) |
| Mechanistic understanding | Early-Moderate | Transformer circuits | <R id="f771d4f56ad4dbaa">Anthropic Interpretability</R> |
| PII activation probes | Early | Universal directions for PII leakage | [Universal Activation Directions (2025)](https://arxiv.org/abs/2506.00018) |
| Attention head stability | Early | Circuit universality limits identified | [Attention-Head Stability (2025)](https://arxiv.org/abs/2506.00016) |

### Constitutional AI and Value Learning

<R id="f771d4f56ad4dbaa">Anthropic's Constitutional AI</R> demonstrates approaches to value alignment:

| Technique | Success Rate | Application | Limitations |
|-----------|--------------|-------------|-------------|
| Self-critique | 70-85% | Harmful content reduction | Requires good initial training |
| Principle following | 60-80% | Consistent value application | Vulnerable to gaming |
| Preference learning | 65-75% | Human value approximation | Distributional robustness |

### Scalable Oversight Applications

Modern LLMs enable approaches to AI safety through automated oversight:

- **Output evaluation**: AI systems critiquing other AI outputs with approximately 85% agreement with humans
- **Red-teaming**: Automated discovery of failure modes and adversarial inputs
- **Safety monitoring**: Real-time analysis of AI system behavior patterns
- **Research acceleration**: AI-assisted safety research and experimental design
- **Content moderation**: [Using GPT-4 for content moderation](https://openai.com/index/using-gpt-4-for-content-moderation/) demonstrates LLM-based moderation at scale, reducing human labeler exposure to harmful content

## Concerning Capabilities Assessment

### Persuasion and Manipulation

Modern LLMs demonstrate persuasion capabilities that raise concerns for democratic discourse and individual autonomy:

| Capability | Current State | Evidence | Risk Level |
|------------|---------------|----------|------------|
| Audience adaptation | Advanced | Anthropic persuasion research | High |
| Persona consistency | Advanced | Extended roleplay studies | High |
| Emotional manipulation | Moderate | RLHF alignment research | Moderate |
| Debate performance | Advanced | Human preference studies | High |
| Linguistic personality expression | Advanced | [Bots of Persuasion (2025)](https://arxiv.org/abs/2506.00027) | Moderate-High |

Research by <R id="f771d4f56ad4dbaa">Anthropic</R> indicates GPT-4 can increase human agreement rates by 82% through targeted persuasion techniques, raising concerns about <EntityLink id="consensus-manufacturing">consensus manufacturing</EntityLink>. The [AREG benchmark (2025)](https://arxiv.org/abs/2502.09455) specifically evaluates persuasion and resistance capabilities in LLMs through adversarial resource extraction games, finding that frontier models can maintain persuasive pressure across extended multi-turn interactions.

[The Bots of Persuasion: Examining How Conversational Agents' Linguistic Expressions of Personality Affect User Perceptions and Decisions (2025)](https://arxiv.org/abs/2506.00027) examines how stylistic and personality-related linguistic choices in conversational AI systems influence user trust, compliance, and decision-making. The study finds that agents expressing warmth and confidence elicit higher compliance rates than neutral agents, and that users largely fail to adjust their compliance behavior after being informed the agent is AI-generated. This has implications for deployment contexts where AI agents are used for high-stakes interactions (e.g., financial advice, medical triage, legal guidance).

### Multilingual Safety Gaps

A consistent finding across the research literature is that safety alignment applied primarily in English does not reliably transfer to other languages. [Helpful to a Fault (2025)](https://arxiv.org/abs/2502.09933) measured illicit assistance rates for multi-turn, multilingual LLM agents across 40+ languages, finding that:

- Models provide meaningfully more potentially harmful assistance in non-English languages
- The gap is largest for low-resource languages with sparse alignment training data
- Multi-turn interactions elicit more harmful assistance than single-turn, with each turn potentially eroding earlier refusals

This suggests that multilingual deployment of LLMs creates safety gaps that single-language evaluation would miss, a concern noted in the [GPT-5 System Card](https://openai.com/index/gpt-5-system-card/).

[Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models (2025)](https://arxiv.org/abs/2506.00028) proposes a prompting architecture that decomposes bias mitigation into sequential stages—perspective-taking, counterfactual generation, and self-critique—finding that cascaded prompting reduces demographic bias in model outputs more effectively than single-stage debiasing prompts. The approach is complementary to RLHF-based bias mitigation and does not require model fine-tuning.

### Cybersecurity Capabilities and Refusal Frameworks

LLMs' code generation and vulnerability analysis capabilities create dual-use risks in cybersecurity:

- Models can assist with vulnerability identification, exploit development, and attack planning at varying levels depending on specificity of the request
- [A Content-Based Framework for Cybersecurity Refusal Decisions (2025)](https://arxiv.org/abs/2502.09591) proposes taxonomizing cybersecurity requests by content type (reconnaissance vs. exploitation vs. malware development) rather than binary harmful/safe classification, enabling more precise refusal decisions that maintain legitimate educational and defensive use
- [SecCodeBench-V2 (2025)](https://arxiv.org/abs/2502.09474) provides updated benchmarks for evaluating security-relevant code generation, finding continued improvement in both benign and potentially harmful code generation capabilities
- [Can Adversarial Code Comments Fool AI Security Reviewers? (2025)](https://arxiv.org/abs/2506.00029) conducts a large-scale empirical study of comment-based attacks against LLM code security analysis tools. The study finds that injecting misleading natural language comments into code—asserting, e.g., "this input is already sanitized" or "this function has been audited"—causes LLMs acting as security reviewers to substantially underreport vulnerabilities. Comment-based attacks succeed even when the code itself contains obvious vulnerability patterns, suggesting that LLM security tools are susceptible to social-engineering-style attacks embedded in natural language annotations.

[What Breaks Embodied AI Security: LLM Vulnerabilities, CPS Flaws, or Something Else? (2025)](https://arxiv.org/abs/2506.00030) analyzes security vulnerabilities in embodied AI systems—robots and cyber-physical systems (CPS) controlled by LLM planners. The paper finds that the dominant failure mode is neither pure LLM jailbreaking nor pure CPS exploit, but rather the interface between LLM planning outputs and CPS actuation logic: adversarial inputs can exploit ambiguities in how natural-language plans are translated into low-level commands. This suggests that security evaluation of embodied AI requires joint analysis of the LLM and the CPS rather than treating each in isolation.

### Autonomous Capabilities and Long-Horizon Planning

Current LLMs demonstrate increasing levels of autonomous task execution, with recent work extending evaluation to substantially longer horizons and more complex multiagent settings:

- **Web browsing**: GPT-4 class models can navigate websites, extract information, and interact with web services
- **Code execution**: Models can write, debug, and iteratively improve software across extended sessions
- **API integration**: Tool use across multiple digital platforms; [Shipping Smarter Agents with Every New Model](https://openai.com/index/shipping-smarter-agents/) describes OpenAI's agent capability roadmap
- **Goal persistence**: Basic ability to maintain objectives across extended multi-turn interactions, relevant to <EntityLink id="scheming">scheming</EntityLink> risk evaluation
- **Memory across sessions**: [MemoryArena (2025)](https://arxiv.org/abs/2502.10392) benchmarks agent memory in interdependent multi-session tasks, finding that frontier models maintain task state imperfectly across sessions but with increasing reliability in newer model versions

**Long-horizon task training** has emerged as a distinct research focus. [KLong: Training LLM Agents for Extremely Long-horizon Tasks (2025)](https://arxiv.org/abs/2506.00031) introduces a curriculum training approach that progressively extends the planning horizon during agent training, finding that agents trained with horizon curriculum substantially outperform those trained at fixed horizon lengths on tasks requiring 50+ sequential decision steps. The paper evaluates on both simulated and real-world web navigation tasks, finding that horizon curriculum training improves task completion rates by approximately 30% on tasks requiring more than 20 steps.

**Long-horizon benchmarking** has also advanced. [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs (2025)](https://arxiv.org/abs/2506.00032) introduces a benchmark where agents must navigate Wikipedia's link graph from a start article to a target article in a minimum number of steps. The task requires multi-hop planning over a real-world knowledge graph without access to a precomputed path. Frontier models achieve substantially higher success rates than prior LLMs but still fall short of human expert performance on the hardest navigation paths, suggesting that long-horizon graph planning remains a meaningful capability gap.

**Multiagent learning** capabilities have been documented at scale. [Discovering Multiagent Learning Algorithms with Large Language Models (2025)](https://arxiv.org/abs/2506.00033) demonstrates that LLMs can generate novel multiagent learning algorithms—including variants of gradient-based cooperation and opponent modeling—when prompted with descriptions of multi-agent environments and performance objectives. Several LLM-generated algorithms outperform standard baselines on cooperative and competitive multiagent tasks, raising questions about the pace at which LLM-driven algorithm discovery could accelerate multiagent AI development.

**Multiagent orchestration** is a growing concern for AI governance. [AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence (2025)](https://arxiv.org/abs/2506.00034) addresses the problem of dynamically routing tasks to the most appropriate LLM agent in a multi-agent pipeline as model capabilities converge. As frontier models increasingly achieve similar performance on standard benchmarks, task-adaptive routing based on model-specific strengths (e.g., mathematical reasoning vs. code generation vs. domain knowledge) becomes more important for overall pipeline performance. The paper proposes a meta-controller that monitors task-level performance signals to reallocate tasks across agents.

[MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation (2025)](https://arxiv.org/abs/2506.00035) describes a multi-agent system where LLM planners, vision-language models, and low-level motor controllers are orchestrated to perform complex manipulation tasks. The framework demonstrates that multiagent LLM architectures can generalize across manipulation tasks not seen during
