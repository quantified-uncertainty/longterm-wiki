---
title: Training Methods
description: Techniques for training AI systems to be aligned with human values and intentions, from RLHF to constitutional AI.
sidebar:
  label: Overview
  order: 0
subcategory: alignment-training
---
import {EntityLink} from '@components/wiki';

Training methods for alignment focus on shaping model behavior during the learning process.

**Core Approaches:**
- **<EntityLink id="rlhf">RLHF</EntityLink>**: Reinforcement Learning from Human Feedback - the foundation of modern alignment training
- **<EntityLink id="constitutional-ai">Constitutional AI</EntityLink>**: Self-critique based on principles
- **<EntityLink id="preference-optimization">Preference Optimization</EntityLink>**: Direct preference learning (DPO, IPO)

**Specialized Techniques:**
- **<EntityLink id="process-supervision">Process Supervision</EntityLink>**: Rewarding reasoning steps, not just outcomes
- **<EntityLink id="reward-modeling">Reward Modeling</EntityLink>**: Learning human preferences from comparisons
- **<EntityLink id="refusal-training">Refusal Training</EntityLink>**: Teaching models to decline harmful requests
- **<EntityLink id="adversarial-training">Adversarial Training</EntityLink>**: Robustness through adversarial examples

**Advanced Methods:**
- **<EntityLink id="weak-to-strong">Weak-to-Strong Generalization</EntityLink>**: Can weak supervisors train strong models?
- **<EntityLink id="capability-unlearning">Capability Unlearning</EntityLink>**: Removing dangerous knowledge
