---
numericId: E653
title: Training Methods
description: Techniques for training AI systems to be aligned with human values and intentions, from RLHF to constitutional AI.
sidebar:
  label: Overview
  order: 0
subcategory: alignment-training
---
import {EntityLink} from '@components/wiki';

Training methods for alignment focus on shaping model behavior during the learning process.

**Core Approaches:**
- **<EntityLink id="E259">RLHF</EntityLink>**: Reinforcement Learning from Human Feedback - the foundation of modern alignment training
- **<EntityLink id="E451">Constitutional AI</EntityLink>**: Self-critique based on principles
- **<EntityLink id="E454">Preference Optimization</EntityLink>**: Direct preference learning (DPO, IPO)

**Specialized Techniques:**
- **<EntityLink id="E455">Process Supervision</EntityLink>**: Rewarding reasoning steps, not just outcomes
- **<EntityLink id="E600">Reward Modeling</EntityLink>**: Learning human preferences from comparisons
- **<EntityLink id="E456">Refusal Training</EntityLink>**: Teaching models to decline harmful requests
- **<EntityLink id="E583">Adversarial Training</EntityLink>**: Robustness through adversarial examples

**Advanced Methods:**
- **<EntityLink id="E452">Weak-to-Strong Generalization</EntityLink>**: Can weak supervisors train strong models?
- **<EntityLink id="E453">Capability Unlearning</EntityLink>**: Removing dangerous knowledge
