---
numericId: "E653"
title: "Training Methods (Overview)"
readerImportance: 62
researchImportance: 51.5
description: "Techniques for training AI systems to be aligned with human values and intentions, from RLHF to constitutional AI."
sidebar:
  label: "Overview"
  order: 0
subcategory: "alignment-training"
entityType: "overview"
tacticalValue: 65
quality: 27
llmSummary: "A bare-bones index page listing 10 alignment training methods (RLHF, Constitutional AI, DPO, process supervision, etc.) with one-line descriptions and links to deeper pages, providing no analysis, evidence, or comparative assessment of effectiveness."
ratings:
  focus: 7.5
  novelty: 1.5
  rigor: 1
  completeness: 3.5
  concreteness: 2.5
  actionability: 2
  objectivity: 5.5
---
import {EntityLink} from '@components/wiki';

Training methods for alignment focus on shaping model behavior during the learning process.

**Core Approaches:**
- **<EntityLink id="rlhf">RLHF</EntityLink>**: Reinforcement Learning from Human Feedback - the foundation of modern alignment training
- **<EntityLink id="constitutional-ai">Constitutional AI</EntityLink>**: Self-critique based on principles
- **<EntityLink id="preference-optimization">Preference Optimization</EntityLink>**: Direct preference learning (DPO, IPO)

**Specialized Techniques:**
- **<EntityLink id="process-supervision">Process Supervision</EntityLink>**: Rewarding reasoning steps, not just outcomes
- **<EntityLink id="reward-modeling">Reward Modeling</EntityLink>**: Learning human preferences from comparisons
- **<EntityLink id="refusal-training">Refusal Training</EntityLink>**: Teaching models to decline harmful requests
- **<EntityLink id="adversarial-training">Adversarial Training</EntityLink>**: Robustness through adversarial examples

**Advanced Methods:**
- **<EntityLink id="weak-to-strong">Weak-to-Strong Generalization</EntityLink>**: Can weak supervisors train strong models?
- **<EntityLink id="capability-unlearning">Capability Unlearning</EntityLink>**: Removing dangerous knowledge
