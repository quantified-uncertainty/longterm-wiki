---
numericId: E652
title: "Theoretical Foundations (Overview)"
readerImportance: 69
researchImportance: 50
description: Fundamental concepts and formal approaches to AI alignment - corrigibility, scalable oversight, and mathematical safety guarantees.
sidebar:
  label: Overview
  order: 0
clusters:
  - ai-safety
subcategory: alignment-theoretical
entityType: overview
---
import {EntityLink} from '@components/wiki';

Theoretical alignment research establishes the conceptual and mathematical foundations for safe AI.

**Core Concepts:**
- **<EntityLink id="corrigibility">Corrigibility</EntityLink>**: Systems that allow correction and shutdown
- **<EntityLink id="goal-misgeneralization">Goal Misgeneralization</EntityLink>**: When learned goals differ from intended goals
- **<EntityLink id="agent-foundations">Agent Foundations</EntityLink>**: Mathematical foundations of agency

**Scalable Oversight:**
- **<EntityLink id="scalable-oversight">Scalable Oversight</EntityLink>**: Supervising superhuman systems
- **<EntityLink id="eliciting-latent-knowledge">Eliciting Latent Knowledge (ELK)</EntityLink>**: Getting models to report what they know
- **<EntityLink id="debate">AI Debate</EntityLink>**: Using AI to verify AI reasoning

**Formal Approaches:**
- **<EntityLink id="formal-verification">Formal Verification</EntityLink>**: Mathematical proofs of properties
- **<EntityLink id="provably-safe">Provably Safe AI</EntityLink>**: Guarantees through formal methods
- **<EntityLink id="cirl">CIRL</EntityLink>**: Cooperative Inverse Reinforcement Learning

**Multi-Agent:**
- **<EntityLink id="cooperative-ai">Cooperative AI</EntityLink>**: AI systems that cooperate with humans and each other
