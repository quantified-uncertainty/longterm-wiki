---
title: Theoretical Foundations
description: Fundamental concepts and formal approaches to AI alignment - corrigibility, scalable oversight, and mathematical safety guarantees.
sidebar:
  label: Overview
  order: 0
subcategory: alignment-theoretical
---
import {EntityLink} from '@components/wiki';

Theoretical alignment research establishes the conceptual and mathematical foundations for safe AI.

**Core Concepts:**
- **<EntityLink id="E79">Corrigibility</EntityLink>**: Systems that allow correction and shutdown
- **<EntityLink id="E151">Goal Misgeneralization</EntityLink>**: When learned goals differ from intended goals
- **<EntityLink id="E584">Agent Foundations</EntityLink>**: Mathematical foundations of agency

**Scalable Oversight:**
- **<EntityLink id="E271">Scalable Oversight</EntityLink>**: Supervising superhuman systems
- **<EntityLink id="E481">Eliciting Latent Knowledge (ELK)</EntityLink>**: Getting models to report what they know
- **<EntityLink id="E482">AI Debate</EntityLink>**: Using AI to verify AI reasoning

**Formal Approaches:**
- **<EntityLink id="E483">Formal Verification</EntityLink>**: Mathematical proofs of properties
- **<EntityLink id="E484">Provably Safe AI</EntityLink>**: Guarantees through formal methods
- **<EntityLink id="E586">CIRL</EntityLink>**: Cooperative Inverse Reinforcement Learning

**Multi-Agent:**
- **<EntityLink id="E590">Cooperative AI</EntityLink>**: AI systems that cooperate with humans and each other
