---
title: Model Registries
description: Centralized databases of frontier AI models that enable governments to track development, enforce safety requirements, and coordinate international oversight—serving as foundational infrastructure for AI governance analogous to drug registries for the FDA.
sidebar:
  order: 12
quality: 68
lastEdited: "2026-01-28"
importance: 75.5
update_frequency: 21
llmSummary: Analyzes model registries as foundational governance infrastructure across US (≥10^26 FLOP threshold), EU (≥10^25 FLOP), and state-level implementations, showing they enable pre-deployment review and incident tracking but don't prevent harm directly. Provides specific implementation recommendations including 30-90 day pre-deployment notification and 72-hour incident reporting, with medium-high confidence that registries improve visibility and incident learning.
ratings:
  novelty: 4
  rigor: 6.5
  actionability: 7
  completeness: 7.5
clusters:
  - ai-safety
  - governance
subcategory: governance
entityType: approach
---
import {Mermaid, EntityLink, DataExternalLinks} from '@components/wiki';

<DataExternalLinks pageId="model-registries" />

## Overview

Model registries represent a foundational governance tool for managing risks from advanced AI systems. Like drug registries that enable pharmaceutical regulation or aircraft registries that support aviation safety, AI model registries would create centralized databases containing information about frontier AI systems—their capabilities, training details, deployment contexts, and safety evaluations. This infrastructure provides governments with the visibility necessary to implement more sophisticated <EntityLink id="E608">AI governance</EntityLink> measures.

The policy momentum is significant. The U.S. Executive Order on AI (October 2023) mandated quarterly reporting for models trained above 10^26 FLOP. The <EntityLink id="E127">EU AI Act</EntityLink> requires registration of high-risk AI systems and general-purpose AI models. California's SB 53 (signed September 2025) requires transparency reports and incident reporting for frontier models above 10^26 FLOP. New York's RAISE Act requires incident reporting within 72 hours. These requirements create the skeleton of a registry system, though implementation remains fragmented and early-stage.

The strategic value of model registries lies in their enabling function. A registry alone doesn't prevent harm—but it provides the information foundation for safety requirements, pre-deployment review, incident tracking, and international coordination. Without knowing what models exist and what capabilities they possess, governments cannot effectively regulate AI development. Model registries transform AI governance from reactive to proactive by creating visibility into the development pipeline before deployment.

## Current Implementation Landscape

### United States

**Federal Level:**
The October 2023 Executive Order directed the Bureau of Industry and Security (BIS) to establish reporting requirements for advanced AI models. Under the proposed rule:
- Entities must report models trained with >10^26 FLOP
- Quarterly reporting on training activities
- Six-month forward-looking projections required
- Information includes ownership, compute access, safety testing

**State Level:**

| State | Legislation | Key Requirements | Status |
|-------|-------------|------------------|--------|
| **California** | [SB 53](https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/) | Transparency reports for models above 10^26 FLOP; 15-day incident reporting | Enacted Sep 2025; effective Jan 1, 2026 |
| **New York** | RAISE Act | 72-hour incident reporting; safety protocol publication; civil penalties up to \$1M | Enacted 2024 |
| **Colorado** | SB 24-205 | High-risk AI system registration; algorithmic impact assessments | Enacted May 2024 |

### European Union

The [EU AI Act](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai) (Regulation EU 2024/1689), which entered into force August 1, 2024, establishes the most comprehensive registry requirements to date:

- **General-Purpose AI Models**: Registration with EU AI Office if trained above 10^25 FLOP
- **High-Risk AI Systems**: [Registration in EU database](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-49) before market placement
- **Systemic Risk Models**: Additional transparency and safety requirements
- **Required Information**: Technical documentation, compliance evidence, intended use

The EU database will be publicly accessible for high-risk AI systems, with confidential technical documentation available to regulators. Per [Article 49](https://artificialintelligenceact.eu/article/49/), providers must register themselves and their systems before placing high-risk AI systems on the market. High-risk obligations become applicable in August 2026-2027.

### China

China has implemented registration requirements since 2023 under the Interim Measures for Generative AI Services:

- Deep synthesis (deepfake) algorithms must register with CAC
- Generative AI services require registration before public offering
- Algorithmic recommendation services subject to separate registry
- As of November 2025, 611 generative AI services and 306 apps had completed filing
- Apps must publicly disclose which filed model they use, including filing number
- Focus on content moderation and political sensitivity

### Comparison Table

| Jurisdiction | Compute Threshold | Pre/Post Deployment | Public Access | Penalties |
|--------------|------------------|--------------------|--------------| ----------|
| **US Federal** | 10^26 FLOP | Pre + ongoing | Limited (security) | Under development |
| **California** | 10^26 FLOP | Pre-deployment | Transparency reports public | Up to \$1M/violation |
| **New York** | Scale-based | Pre + incidents | Protocols public | Up to \$1M |
| **EU** | 10^25 FLOP | Pre-market | Partial | Up to 7% revenue |
| **China** | Any public AI | Pre-deployment | Limited | Service suspension |

## Strategic Assessment

### Benefits of Model Registries

| Benefit | Mechanism | Confidence |
|---------|-----------|------------|
| **Visibility for governance** | Know what exists before regulating | High |
| **Incident learning** | Track failures across the ecosystem | High |
| **Pre-deployment review** | Enable safety checks before release | Medium-High |
| **International coordination** | Common information standards | Medium |
| **Enforcement foundation** | Can't enforce rules without knowing who to apply them to | High |
| **Research ecosystem support** | Aggregate data for policy research | Medium |

### Limitations and Challenges

| Challenge | Description | Mitigation |
|-----------|-------------|------------|
| **Threshold gaming** | Developers structure training to avoid thresholds ([research shows](https://arxiv.org/html/2407.05694v1) model distillation and mixture-of-agents approaches can achieve frontier performance below thresholds) | Multiple thresholds; capability-based triggers |
| **Dual-use concerns** | Registry information could advantage competitors/adversaries | Tiered access; confidentiality provisions |
| **Open-source gap** | Registries focus on centralized developers | Post-release monitoring; community registries |
| **Enforcement difficulty** | Verifying submitted information is accurate | Auditing; whistleblower protections |
| **Rapid obsolescence** | Thresholds outdated as technology advances | Automatic update mechanisms; sunset provisions |
| **International gaps** | No global registry; jurisdiction shopping | International coordination (nascent) |

### Relationship to Other Governance Tools

Model registries are necessary but not sufficient for AI governance. They enable but don't replace:

<Mermaid chart={`
flowchart LR
    A[Model Registry] --> B[Pre-Deployment Review]
    A --> C[Safety Certification]
    A --> D[Compute Monitoring]
    A --> E[International Coordination]
    A --> F[Incident Investigation]
    A --> G[Liability Assignment]

    B --> H[Safe Deployment]
    C --> H
    D --> I[Development Oversight]
    E --> J[Global Coordination]
    F --> K[Systemic Learning]
    G --> L[Accountability]

    style A fill:#e1f5ff

    style H fill:#d4edda
    style I fill:#d4edda
    style J fill:#d4edda
    style K fill:#d4edda
    style L fill:#d4edda
`} />

## Implementation Recommendations

### Minimum Viable Registry

For jurisdictions establishing initial AI model registries:

1. **Compute-based threshold**: 10^25-10^26 FLOP (adjustable)
2. **Pre-deployment notification**: 30-90 days before public release
3. **Required information**:
   - Developer identity and contact
   - Training compute and data sources (categorical)
   - Intended use cases and deployment scope
   - Safety evaluation summary
   - Known risks and mitigations

4. **Incident reporting**: 72 hours for critical harms
5. **Annual updates**: Mandatory refresh of all information
6. **Tiered access**: Public summary + confidential technical details

### Best Practices from Research

Based on analysis by the [Institute for Law & AI](https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/):

| Principle | Rationale | Implementation |
|-----------|-----------|----------------|
| **Minimal burden** | Encourage compliance, reduce resistance | Require only information developers already track |
| **Interoperable** | Enable international coordination | Align with emerging international standards |
| **Updatable** | Technology changes faster than regulation | Built-in mechanism for threshold adjustment |
| **Complementary** | Registry enables other tools, doesn't replace them | Design for integration with safety requirements |
| **Proportionate** | Different requirements for different risk levels | Tiered obligations based on capability/deployment |

### Avoiding Common Pitfalls

**Don't:**
- Set thresholds so high only 2-3 models qualify (too narrow)
- Require disclosure of trade secrets unnecessarily (industry opposition)
- Create registry without enforcement mechanism (toothless)
- Assume static thresholds will remain appropriate (obsolescence)
- Ignore international coordination from the start (jurisdiction shopping)

## Future Trajectory

### Near-Term (2025-2026)

- California SB 53 effective January 2026 (transparency reports, incident reporting)
- EU high-risk AI database operational (August 2026-2027 compliance deadlines)
- [GovAI forecasts](https://www.governance.ai/research-paper/trends-in-frontier-ai-model-count-a-forecast-to-2028) 103-306 models exceeding 10^25 FLOP (EU threshold) by 2028
- 5-10 jurisdictions with some form of registry
- Initial international coordination discussions

### Medium-Term (2027-2030)

- Potential international registry framework
- Capability-based triggers supplement compute thresholds
- Integration with compute monitoring
- Real-time incident reporting systems
- Cross-border data sharing agreements

### Key Uncertainties

| Question | Optimistic Scenario | Pessimistic Scenario |
|----------|--------------------|--------------------|
| International coordination | Common standards, shared database | Fragmented, incompatible systems |
| Enforcement effectiveness | High compliance, meaningful oversight | Widespread evasion, symbolic only |
| Open-source coverage | Community registries, post-release tracking | Unmonitored proliferation |
| Threshold relevance | Adaptive thresholds track real risks | Outdated, easily gamed |

## Quick Assessment

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| **Tractability** | High | Active legislation in multiple jurisdictions |
| **If AI risk high** | High | Essential infrastructure for any governance |
| **If AI risk low** | Medium | Still useful for transparency and accountability |
| **Neglectedness** | Low-Medium | Active policy area but implementation gaps |
| **Timeline to impact** | 1-3 years | Requirements taking effect 2025-2026 |
| **Grade** | B+ | Foundational but not transformative alone |

## Risks Addressed

| Risk | Mechanism | Effectiveness |
|------|-----------|---------------|
| <EntityLink id="E239">Racing Dynamics</EntityLink> | Visibility into development timelines | Low-Medium |
| <EntityLink id="E392">Misuse Risks</EntityLink> | Know what capabilities exist | Medium |
| Regulatory arbitrage | Harmonized international requirements | Low (currently) |
| Incident learning gaps | Mandatory reporting creates database | Medium-High |

## Complementary Interventions

- Compute Governance - Hardware-based verification complements software registration
- <EntityLink id="E136">Export Controls</EntityLink> - Control inputs to models in registry
- <EntityLink id="E13">AI Safety Institutes</EntityLink> - Institutions to review registered models
- <EntityLink id="E252">Responsible Scaling Policies</EntityLink> - Industry commitments that registries can verify

## Sources

### Policy Analysis

- **Convergence Analysis (2024):** ["AI Model Registries: A Foundational Tool for AI Governance"](https://www.convergenceanalysis.org/research/ai-model-registries-a-foundational-tool-for-ai-governance) - Comprehensive design framework developed in collaboration with Gillian Hadfield; influenced US BIS consultation and EU GPAI Code of Practice
- **Institute for Law & AI (2024):** ["The Role of Compute Thresholds for AI Governance"](https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/) - Threshold design considerations
- **GovAI (2024):** ["Trends in Frontier AI Model Count: A Forecast to 2028"](https://www.governance.ai/research-paper/trends-in-frontier-ai-model-count-a-forecast-to-2028) - Projects 45-148 models exceeding 10^26 FLOP by 2028
- **Heim et al. (2024):** ["Training Compute Thresholds: Features and Functions in AI Regulation"](https://arxiv.org/html/2405.10799v2) - Analysis of compute thresholds as governance metrics
- **Hooker (2024):** ["On the Limitations of Compute Thresholds as a Governance Strategy"](https://arxiv.org/html/2407.05694v1) - Critiques threshold gaming via distillation and MoA approaches

### Legislation and Regulation

- **US Executive Order 14110 (October 2023):** "Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence" - Established 10^26 FLOP reporting threshold
- **EU AI Act (2024):** [Regulation (EU) 2024/1689](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai) - [Article 49](https://artificialintelligenceact.eu/article/49/) covers registration requirements
- **California SB 53 (2025):** [Transparency in Frontier Artificial Intelligence Act](https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/) - First US state frontier AI safety law; effective January 2026
- **New York RAISE Act (2024):** Requiring AI Safety and Excellence - 72-hour incident reporting

### Implementation Resources

- **NIST:** AI Risk Management Framework integration guidance
- **EU AI Office:** [High-risk AI system registration requirements](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-49)
- **Future of Privacy Forum (2025):** ["California's SB 53: The First Frontier AI Law, Explained"](https://fpf.org/blog/californias-sb-53-the-first-frontier-ai-law-explained/)

---

## AI Transition Model Context

Model registries improve the <EntityLink id="ai-transition-model" /> through <EntityLink id="E60" />:

| Factor | Parameter | Impact |
|--------|-----------|--------|
| <EntityLink id="E60" /> | <EntityLink id="E249" /> | Provides information foundation for any governance interventions |
| <EntityLink id="E60" /> | <EntityLink id="E167" /> | Enables pre-deployment review and incident learning |
| <EntityLink id="E60" /> | <EntityLink id="E171" /> | Common standards facilitate cross-border coordination |

Registries are necessary but not sufficient infrastructure; they enable rather than replace safety requirements, evaluations, and enforcement mechanisms.
