---
numericId: E649
title: Evaluation & Detection
description: Methods for testing AI alignment, detecting dangerous capabilities, and identifying deceptive or misaligned behavior.
sidebar:
  label: Overview
  order: 0
subcategory: alignment-evaluation
entityType: approach
---
import {EntityLink} from '@components/wiki';

Evaluation methods assess whether AI systems are aligned and safe to deploy.

**General Evaluation:**
- **<EntityLink id="E128">Evaluations (Evals)</EntityLink>**: Overview of AI evaluation approaches
- **<EntityLink id="E448">Alignment Evaluations</EntityLink>**: Testing for aligned behavior
- **<EntityLink id="E442">Dangerous Capability Evaluations</EntityLink>**: Assessing harmful potential

**Capability Assessment:**
- **<EntityLink id="E443">Capability Elicitation</EntityLink>**: Uncovering hidden capabilities
- **<EntityLink id="E449">Red Teaming</EntityLink>**: Adversarial testing for vulnerabilities
- **<EntityLink id="E450">Model Auditing</EntityLink>**: Systematic capability review

**Deception Detection:**
- **<EntityLink id="E441">Scheming Detection</EntityLink>**: Identifying strategic deception
- **<EntityLink id="E445">Sleeper Agent Detection</EntityLink>**: Finding hidden malicious behaviors

**Evaluation Scaling:**
- **<EntityLink id="E437">Eval Saturation & The Evals Gap</EntityLink>**: Accelerating benchmark saturation and its implications
- **<EntityLink id="E440">Scalable Eval Approaches</EntityLink>**: Practical tools for scaling evaluation capacity
- **<EntityLink id="E438">Evaluation Awareness</EntityLink>**: Models detecting and adapting to evaluation contexts

**Deployment Decisions:**
- **<EntityLink id="E444">Safety Cases</EntityLink>**: Structured arguments for deployment safety
