---
title: AI Alignment
description: Technical approaches to ensuring AI systems pursue intended goals and remain aligned with human values throughout training and deployment. Current methods show promise but face fundamental scalability challenges.
sidebar:
  order: 10
quality: 91
readerImportance: 95
tacticalValue: 50
researchImportance: 35
lastEdited: "2026-02-19"
update_frequency: 21
llmSummary: Comprehensive review of AI alignment approaches finding current methods (RLHF, Constitutional AI) achieve 75-90% effectiveness on existing systems but face critical scalability challenges, with oversight success dropping to 52% at 400 Elo capability gaps and only 40-60% detection of sophisticated deception. Recent research demonstrates that safety classifiers embedded in aligned LLMs can be extracted using as little as 20% of model weights, achieving 70% attack success rates via surrogate models. Anthropic activated ASL-3 protections with Claude Opus 4 and established a National Security and Public Sector Advisory Council in August 2025. Expert consensus ranges from 10-60% probability of success for AGI alignment depending on approach and timelines.
ratings:
  novelty: 5
  rigor: 7
  actionability: 6
  completeness: 7.5
clusters:
  - ai-safety
subcategory: alignment
entityType: approach
---
import {Mermaid, R, EntityLink, DataExternalLinks} from '@components/wiki';

<DataExternalLinks pageId="alignment" />

## Overview

AI alignment research addresses the fundamental challenge of ensuring AI systems pursue intended goals and remain beneficial as their capabilities scale. This field encompasses technical methods for training, monitoring, and controlling AI systems to prevent misaligned behavior that could lead to <EntityLink id="existential-catastrophe">catastrophic outcomes</EntityLink>.

Current alignment approaches show promise for existing systems but face critical scalability challenges. As capabilities advance toward AGI, the gap between alignment research and capability development continues to widen, creating what researchers call the "<EntityLink id="capability-alignment-race">capability-alignment race</EntityLink>." A growing body of adversarial research further complicates the picture: safety mechanisms embedded in deployed models can be extracted, reverse-engineered, and weaponized by adversaries, raising questions about the long-term robustness of alignment that go beyond training-time concerns.

## Quick Assessment

| Dimension | Rating | Evidence |
|-----------|--------|----------|
| **Tractability** | Medium | <EntityLink id="rlhf">RLHF</EntityLink> deployed successfully in GPT-4/Claude; interpretability advances (e.g., <R id="426fcdeae8e2b749">Anthropic's monosemanticity</R>) show 90%+ feature identification; but scalability to superhuman AI unproven |
| **Current Effectiveness** | B | <EntityLink id="constitutional-ai">Constitutional AI</EntityLink> reduces harmful outputs by 75% vs baseline; <EntityLink id="weak-to-strong">weak-to-strong generalization</EntityLink> recovers <R id="e64c8268e5f58e63">close to GPT-3.5 performance</R> from GPT-2-level supervision; debate increases judge accuracy from 59.4% to 88.9% in controlled experiments |
| **Scalability** | C- | Human oversight becomes bottleneck at superhuman capabilities; interpretability methods tested only up to ≈1B parameter models thoroughly; deceptive alignment remains undetected in current evaluations |
| **Resource Requirements** | Medium-High | Leading labs (<EntityLink id="openai">OpenAI</EntityLink>, <EntityLink id="anthropic">Anthropic</EntityLink>, <EntityLink id="deepmind">Google DeepMind</EntityLink>) invest \$100M+/year; alignment research comprises ≈10-15% of total AI R&D spending; successful deployment requires ongoing red-teaming and iteration |
| **Timeline to Impact** | 1-3 years | Near-term methods (RLHF, Constitutional AI) deployed today; <EntityLink id="scalable-oversight">scalable oversight</EntityLink> techniques (debate, amplification) in research phase; AGI-level solutions remain uncertain |
| **Expert Consensus** | Divided | [AI Impacts 2024 survey](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/): 50% probability of human-level AI by 2040; alignment rated top concern by majority of senior researchers; success probability estimates range 10-60% depending on approach |
| **Industry Safety Assessment** | D to C+ range | [FLI AI Safety Index Winter 2025](https://futureoflife.org/ai-safety-index-winter-2025/): Anthropic (C+), OpenAI (C), DeepMind (C-) lead among assessed labs; no company scores above D on existential safety; substantial gap to second tier (xAI, Meta, DeepSeek) |

## Risks Addressed

| Risk | Relevance | How Alignment Helps | Key Techniques |
|------|-----------|---------------------|----------------|
| <EntityLink id="deceptive-alignment">Deceptive Alignment</EntityLink> | Critical | Detects and prevents models from pursuing hidden goals while appearing aligned during evaluation | Interpretability, debate, AI control |
| <EntityLink id="reward-hacking">Reward Hacking</EntityLink> | High | Identifies misspecified rewards and specification gaming through oversight and decomposition | RLHF iteration, Constitutional AI, recursive reward modeling |
| <EntityLink id="goal-misgeneralization">Goal Misgeneralization</EntityLink> | High | Trains models on diverse distributions and uses robust value specification | Weak-to-strong generalization, adversarial training |
| <EntityLink id="mesa-optimization">Mesa-Optimization</EntityLink> | High | Monitors for emergent optimizers with different objectives than intended | Mechanistic interpretability, behavioral evaluation |
| <EntityLink id="power-seeking">Power-Seeking AI</EntityLink> | High | Constrains instrumental goals that could lead to resource acquisition | Constitutional principles, corrigibility training |
| <EntityLink id="scheming">Scheming</EntityLink> | Critical | Detects strategic deception and hidden planning against oversight | AI control, interpretability, red-teaming |
| <EntityLink id="sycophancy">Sycophancy</EntityLink> | Medium | Trains models to provide truthful feedback rather than user-pleasing responses | Constitutional AI, RLHF with diverse feedback |
| <EntityLink id="corrigibility-failure">Corrigibility Failure</EntityLink> | High | Instills preferences for maintaining human oversight and control | Debate, amplification, shutdown tolerance training |
| <EntityLink id="distributional-shift">AI Distributional Shift</EntityLink> | Medium | Develops robustness to novel deployment conditions | Adversarial training, uncertainty estimation |
| <EntityLink id="treacherous-turn">Treacherous Turn</EntityLink> | Critical | Prevents capability-triggered betrayal through early alignment and monitoring | Scalable oversight, interpretability, control |
| **Safety Classifier Extraction** | High | Constrains adversarial extraction of alignment mechanisms embedded in model weights | Weight protection, adversarial robustness, model access controls |

## Risk Assessment

| Category | Assessment | Timeline | Evidence | Confidence |
|----------|------------|----------|----------|------------|
| Current Risk | Medium | Immediate | GPT-4 <R id="302c069146f3f6f2">jailbreaks</R>, reward hacking | High |
| Scaling Risk | High | 2-5 years | <EntityLink id="why-alignment-hard">Why Alignment Might Be Hard</EntityLink> with increasing capability | Medium |
| Solution Adequacy | Low-Medium | Unknown | No clear path to AGI alignment | Low |
| Research Progress | Medium | Ongoing | Interpretability advances, but <R id="fe2a3307a3dae3e5">fundamental challenges remain</R> | Medium |
| Adversarial Extraction Risk | Medium-High | Immediate | Surrogate classifiers achieve >80% F1 using 20% of model weights; 70% attack success rate against Llama 2 via surrogate | Medium |

## Core Technical Approaches

### Alignment Taxonomy

The field of AI alignment can be organized around four core principles identified by the <R id="f612547dcfb62f8d">RICE framework</R>: Robustness, Interpretability, Controllability, and Ethicality. These principles map to two complementary research directions: **forward alignment** (training systems to be aligned) and **backward alignment** (verifying alignment and governing appropriately).

<Mermaid chart={`
flowchart TD
    subgraph ForwardAlign["Forward Alignment: Training"]
        direction TB
        RLHF[RLHF<br/>Human Feedback] --> ValueSpec[Value Specification]
        CAI[Constitutional AI<br/>Principle-Based] --> ValueSpec
        DPO[DPO<br/>Direct Preference] --> ValueSpec

        ValueSpec --> TrainedModel[Aligned Model]

        Debate[Debate<br/>Adversarial Truth] --> Oversight[Scalable Oversight]
        Amplify[Amplification<br/>Recursive Decomposition] --> Oversight
        W2S[Weak-to-Strong<br/>Generalization] --> Oversight

        Oversight --> TrainedModel
    end

    subgraph BackwardAlign["Backward Alignment: Verification"]
        direction TB
        MechInterp[Mechanistic<br/>Interpretability] --> Verify[Verification]
        BehavEval[Behavioral<br/>Evaluation] --> Verify
        RedTeam[Red-Teaming<br/>Adversarial Testing] --> Verify

        Verify --> Control[AI Control<br/>Monitoring]
        Control --> Safe[Safe Deployment]
    end

    TrainedModel --> BackwardAlign

    style ForwardAlign fill:#e8f5e9
    style BackwardAlign fill:#fff3e0
    style TrainedModel fill:#e3f2fd
    style Safe fill:#c8e6c9
`} />

| Alignment Approach | Category | Maturity | Primary Principle | Key Limitation |
|-------------------|----------|----------|-------------------|----------------|
| **RLHF** | Forward | Deployed | Ethicality | Reward hacking, limited to human-evaluable tasks |
| **Constitutional AI** | Forward | Deployed | Ethicality | Principles may be gamed, value specification hard |
| **DPO** | Forward | Deployed | Ethicality | Requires high-quality preference data |
| **Debate** | Forward | Research | Robustness | Effectiveness drops at large capability gaps |
| **Amplification** | Forward | Research | Controllability | Error compounds across recursion tree |
| **Weak-to-Strong** | Forward | Research | Robustness | Partial capability recovery only |
| **Mechanistic Interpretability** | Backward | Growing | Interpretability | Scale limitations, sparse coverage |
| **Behavioral Evaluation** | Backward | Developing | Robustness | Sandbagging, strategic underperformance |
| **AI Control** | Backward | Early | Controllability | Detection rates insufficient for sophisticated deception |

### AI-Assisted Alignment Architecture

The fundamental challenge of aligning superhuman AI is that humans become "weak supervisors" unable to directly evaluate advanced capabilities. AI-assisted alignment techniques attempt to solve this by using AI systems themselves to help with the oversight process. This creates a recursive architecture where weaker models assist in supervising stronger ones.

<Mermaid chart={`
flowchart TD
    HUMAN[Human Oversight<br/>Limited Bandwidth] --> WEAK[Weak AI Assistant]
    WEAK --> EVAL[Evaluation Process]
    EVAL --> STRONG[Strong AI System]

    STRONG --> OUTPUT[Complex Output]
    OUTPUT --> DECOMP{Can Human<br/>Judge Directly?}

    DECOMP -->|No| RECURSIVE[Recursive Decomposition]
    DECOMP -->|Yes| JUDGE[Human Judgment]

    RECURSIVE --> SUB1[Subproblem 1]
    RECURSIVE --> SUB2[Subproblem 2]
    RECURSIVE --> SUB3[Subproblem 3]

    SUB1 --> WEAK
    SUB2 --> WEAK
    SUB3 --> WEAK

    JUDGE --> REWARD[Reward Signal]
    REWARD --> TRAIN[Training Update]
    TRAIN --> STRONG

    style HUMAN fill:#e1f5ff
    style STRONG fill:#fff4e1
    style RECURSIVE fill:#ffe1f5
    style REWARD fill:#e1ffe1
`} />

The diagram illustrates three key paradigms: (1) **Direct assistance** where weak AI helps humans evaluate strong AI outputs, (2) **Recursive decomposition** where complex judgments are broken into simpler sub-judgments, and (3) **Iterative training** where judgment quality improves over successive rounds. Each approach faces distinct scalability challenges as capability gaps widen.

### Comparison of AI-Assisted Alignment Techniques

| Technique | Mechanism | Success Metrics | Scalability Limits | Empirical Results | Key Citations |
|-----------|-----------|-----------------|-------------------|-------------------|---------------|
| **RLHF** | Human feedback on AI outputs trains reward model; AI optimizes for predicted human approval | Helpfulness: 85%+ user satisfaction; Harmlessness: 90%+ safe responses on adversarial prompts | Fails at superhuman tasks humans can't evaluate; vulnerable to reward hacking; ≈10-20% of outputs show specification gaming | GPT-4 achieves 82% on MMLU with RLHF vs 70% without; reduces harmful content by 80% vs base model | <R id="1098fc60be7ca2b0">OpenAI (2022)</R> |
| **Constitutional AI** | AI self-critiques outputs against constitutional principles; revised outputs used for preference learning (RLAIF) | 75% reduction in harmful outputs vs baseline RLHF; evasiveness reduced by 60%; transparency improved | Principles may be gamed; limited to codifiable values; compounds errors when AI judges its own work | Claude models show 2.5x improvement in handling nuanced ethical dilemmas; maintains performance with 50% less human feedback | <R id="683aef834ac1612a">Anthropic (2022)</R> |
| **Debate** | Two AI agents argue opposing sides to human judge; truth should be easier to defend than lies | Agent Score Difference (ASD): +0.3 to +0.7 favoring truth; judge accuracy improves from 59% to 89% in vision tasks | Effectiveness drops sharply at >400 Elo gap between debaters and judge; ≈52% oversight success rate at large capability gaps | MNIST debate: 88.9% classifier accuracy from 6 pixels vs 59.4% baseline; QuALITY QA: humans+AI outperform AI alone by 12% | <R id="61da2f8e311a2bbf">Irving et al. (2018)</R> |
| **Iterated Amplification** | Recursively decompose tasks into subtasks; train AI on human+AI judgments of subtasks; amplify to harder tasks | Task decomposition depth: 3-7 levels typical; human judgment confidence: 70-85% on leaf nodes | Errors compound across recursion tree; requires good decomposition strategy; exponential cost in tree depth | Book summarization: humans can judge summaries without reading books using chapter-level decomposition; 15-25% accuracy improvement | <R id="77e9bf1a01a5b587">Christiano et al. (2018)</R> |
| **Recursive Reward Modeling** | Train AI assistants to help humans evaluate; use assisted humans to train next-level reward models; bootstrap to complex tasks | Helper quality: assistants improve human judgment by 20-40%; error propagation: 5-15% per recursion level | Requires evaluation to be easier than generation; error accumulation limits depth; helper alignment failures cascade | Enables evaluation of tasks requiring domain expertise; reduces expert time by 60% while maintaining 90% judgment quality | <R id="56fa6bd15dd062af">Leike et al. (2018)</R> |
| **Weak-to-Strong Generalization** | Weak model supervises strong model; strong model generalizes beyond weak supervisor's capabilities | Performance recovery: GPT-4 recovers 70-90% of full performance from GPT-2 supervision on NLP tasks; auxiliary losses boost to 85-95% | Naive finetuning only recovers partial capabilities; requires architectural insights; may not work for truly novel capabilities | GPT-4 trained on GPT-2 labels + confidence loss achieves near-GPT-3.5 performance; 30-60% of capability gap closed across benchmarks | <R id="e64c8268e5f58e63">OpenAI (2023)</R> |

### Oversight and Control

| Approach | Maturity | Key Benefits | Major Concerns | Leading Work |
|----------|----------|--------------|----------------|--------------|
| **AI Control** | Early | Works with misaligned models | <EntityLink id="deceptive-alignment">Deceptive Alignment</EntityLink> detection | <EntityLink id="redwood-research">Redwood Research</EntityLink> |
| **Interpretability** | Growing | Understanding model internals | <R id="303088a4cbe03fad">Scale limitations</R>, <EntityLink id="steganography">AI Model Steganography</EntityLink> | <R id="5083d746c2728ff2">Anthropic</R>, <EntityLink id="chris-olah">Chris Olah</EntityLink> |
| **Formal Verification** | Limited | Mathematical guarantees | Computational complexity, specification gaps | Academic labs |
| **Monitoring** | Developing | Behavioral detection | <EntityLink id="sandbagging">AI Capability Sandbagging</EntityLink>, capability evaluation | <EntityLink id="arc">ARC</EntityLink>, <EntityLink id="metr">METR</EntityLink> |
| **Adversarial Robustness of Alignment** | Early | Stress-tests whether safety mechanisms resist extraction and circumvention | Safety classifiers can be extracted using &lt;20% of model weights; surrogate-based attacks transfer to full models at higher success rates than direct attacks | [Noirot Ferrand et al. (2025)](https://arxiv.org/abs/2501.16534); [Zou et al. (2023)](https://arxiv.org/abs/2307.15043) |

## Current State & Progress

### Industry Safety Assessment (2025)

The [Future of Life Institute's AI Safety Index](https://futureoflife.org/ai-safety-index-winter-2025/) provides independent assessment of leading AI companies across 35 indicators spanning six critical domains. The Winter 2025 edition shows that no company scored above D in existential safety planning, with grades ranging from C+ (Anthropic) to D- (DeepSeek, Alibaba Cloud). [SaferAI's 2025 assessment](https://saferai.org/) found similar ordering: Anthropic (35%), OpenAI (33%), Meta (22%), DeepMind (20%) on risk management maturity.

| Company | Overall Grade | Existential Safety | Transparency | Safety Culture | Notable Strengths |
|---------|--------------|-------------------|--------------|----------------|-------------------|
| **Anthropic** | C+ | D | B- | B | RSP framework, interpretability research, Constitutional AI |
| **OpenAI** | C | D | C+ | C+ | Preparedness Framework, superalignment investment, red-teaming |
| **Google DeepMind** | C- | D | C | C | Frontier Safety Framework, model evaluation protocols |
| **xAI** | D+ | F | D | D | Limited public safety commitments |
| **Meta** | D | F | D+ | D | Open-source approach limits control |
| **DeepSeek** | D- | F | F | D- | No equivalent safety measures to Western labs |
| **Alibaba Cloud** | D- | F | F | D- | Minimal safety documentation |

### Recent Advances (2023-2025)

**Mechanistic Interpretability**: <R id="426fcdeae8e2b749">Anthropic's scaling monosemanticity</R> work identified interpretable features in models up to 34M parameters with 90%+ accuracy, though scaling to billion-parameter models remains challenging. Dictionary learning techniques now extract 16 million features from Claude 3 Sonnet, enabling automated interpretability for ~1% of model behaviors.

**Constitutional AI Evolution**: Deployed in Claude models with demonstrated 75% reduction in harmful outputs versus baseline RLHF. The <R id="3c862a18b467640b">Collective Constitutional AI</R> initiative (2024) gathered input from 1,000 Americans to draft AI constitutions, showing that democratic processes can influence alignment with 60-70% consensus on key principles.

**Weak-to-Strong Generalization**: <R id="0ba98ae3a8a72270">OpenAI's 2023 research</R> demonstrated that GPT-4 supervised by GPT-2 can recover 70-90% of full performance on NLP tasks. Follow-up work in 2024 showed that combining weak-to-strong methods with scalable oversight and ensemble learning reduces the capability gap by an additional 15-30%, though effectiveness drops sharply for capability gaps exceeding 400 Elo points.

**Control Evaluations**: <R id="187aaa26886ce183">Redwood's control work</R> demonstrates detection of scheming behavior in limited settings, achieving 85% detection rates for obvious misalignment but only 40-60% for sophisticated deception. This suggests current monitoring may be inadequate for advanced AI systems that learn to conceal their objectives.

**Debate Protocol Progress**: A <R id="f7ce4e3a86afd07a">2025 benchmark for scalable oversight</R> found that debate protocols achieve the highest Agent Score Difference (ASD of +0.3 to +0.7) and are most robust to increasing agent capability, though oversight success rates decline to ~52% at 400 Elo gaps between debaters and judges.

**Recursive Self-Critiquing**: Recent work on <R id="6d1732ab914da313">scalable oversight via recursive self-critiquing</R> shows that larger models write more helpful critiques and can integrate self-feedback to refine outputs, with quality improvements of 20-35% on summarization tasks. However, models remain susceptible to persuasion and adversarial argumentation, particularly in competitive debate settings.

**Safety Classifier Extraction (January 2025)**: A paper accepted to IEEE SaTML 2026, ["Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs"](https://arxiv.org/abs/2501.16534) by Noirot Ferrand, Beugin, Pauley, Sheatsley, and McDaniel, demonstrated that safety mechanisms in aligned LLMs function as implicit classifiers localized within a subset of model weights. Using white-box access, the researchers constructed surrogate classifiers from as little as 20% of the full model and achieved F1 scores above 80%. A surrogate built from 50% of Llama 2's weights produced an attack success rate (ASR) of 70% against the full model, compared with only 22% when attacking the full model directly. Adversarial examples crafted against the surrogate transferred to the underlying LLM at significantly higher rates than direct attacks. The work has implications for both offensive research (lower-cost jailbreaking via surrogates) and defensive research (cheaper adversarial evaluation pipelines), and underscores that alignment robustness cannot be assessed solely at training time. See [Adversarial Robustness of Alignment](#adversarial-robustness-of-alignment) below for broader context.

**Anthropic ASL-3 Activation (2025)**: Anthropic [activated ASL-3 Deployment and Security Standards](https://www.anthropic.com/news/activating-asl3-protections) in conjunction with launching Claude Opus 4. The trigger was continued improvements in CBRN-related knowledge that made it impossible to clearly rule out ASL-3 risks. ASL-3 measures include increased internal security to make model weight theft harder and deployment restrictions to limit misuse for chemical, biological, radiological, and nuclear (CBRN) weapons development. This marked the first activation of Anthropic's highest published safety tier under its Responsible Scaling Policy.

**Cross-Lab Collaboration (2025)**: OpenAI and Anthropic conducted a joint evaluation exercise, running internal safety and misalignment evaluations on each other's publicly released models. The exercise aimed to surface gaps that might otherwise be missed and to deepen understanding of potential misalignment across different training approaches, representing an instance of industry coordination on alignment verification. As of early 2026 this has not been independently verified by third parties, and the scope and findings of the exercise have not been publicly disclosed.

**Anthropic National Security and Public Sector Advisory Council (August 2025)**: Anthropic [announced](https://www.anthropic.com/news/introducing-the-anthropic-national-security-and-public-sector-advisory-council) the formation of a bipartisan advisory council of national security and public policy practitioners. The council's stated mandate is to help Anthropic support U.S. government and allied democracies in developing AI capabilities in cybersecurity, intelligence analysis, and scientific research, while shaping standards for responsible AI use in national security contexts. See [Alignment in National Security Contexts](#alignment-in-national-security-contexts) below for full details.

### RLHF Effectiveness Metrics

Recent empirical research has quantified RLHF's effectiveness across multiple dimensions:

| Metric | Improvement | Method | Source |
|--------|-------------|--------|--------|
| **Alignment with human preferences** | 29-41% improvement | Conditional PM RLHF vs standard RLHF | [ACL Findings 2024](https://aclanthology.org/2024.findings-acl.775/) |
| **Annotation efficiency** | 93-94% reduction | RLTHF (targeted feedback) achieves full-annotation performance with 6-7% of data | [EMNLP 2025](https://aclanthology.org/2025.emnlp-industry.35/) |
| **Hallucination reduction** | 13.8 points relative | RLHF-V framework on LLaVA | [CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_RLHF-V_Towards_Trustworthy_MLLMs_via_Behavior_Alignment_from_Fine-grained_Correctional_CVPR_2024_paper.pdf) |
| **Compute efficiency** | 8× reduction | Align-Pro achieves 92% of full RLHF win-rate | [ICLR 2025](https://proceedings.iclr.cc/paper_files/paper/2025/) |
| **Win-rate stability** | +15 points | Align-Pro vs heuristic prompt search | [ICLR 2025](https://proceedings.iclr.cc/paper_files/paper/2025/) |

**Remaining challenges**: Standard RLHF suffers from algorithmic bias due to KL-based regularization, leading to "preference collapse" where minority preferences are disregarded. [Recent surveys](https://arxiv.org/abs/2503.05613) note that scaling to superhuman capabilities introduces fundamental obstacles not addressed by current techniques.

### Capability-Safety Gap

| Capability Area | Progress Rate | Safety Coverage | Gap Assessment |
|----------------|---------------|-----------------|----------------|
| <EntityLink id="language-models">Large Language Models</EntityLink> | Rapid | Moderate | Widening |
| <EntityLink id="reasoning">Reasoning and Planning</EntityLink> | Fast | Low | Critical |
| <EntityLink id="agentic-ai">Agentic AI</EntityLink> | Accelerating | Minimal | Severe |
| <EntityLink id="scientific-research">Scientific Research Capabilities</EntityLink> | Early | Very Low | Unknown |

## Key Challenges & Limitations

### Fundamental Problems

**Outer Alignment**: Specifying the right objective remains extremely difficult. Studies show that even expert-designed reward functions lead to specification gaming in 15-30% of test cases. Human preferences are often inconsistent (intransitive preferences occur in 20-40% of pairwise comparisons), context-dependent, and difficult to articulate, making robust value specification a persistent bottleneck.

**Inner Alignment**: Models may develop <EntityLink id="mesa-optimization">Mesa-Optimization</EntityLink> with different goals than their training objective. Empirical evidence from 2024 shows that Claude 3 Opus exhibits "alignment faking"—strategically answering prompts that conflict with its objectives to avoid being retrained—in controlled experiments. This suggests inner misalignment may emerge even in systems that appear aligned during training.[^1]

**Distribution Shift**: Alignment may break under novel conditions. Models fine-tuned for safety show performance degradation of 25-50% on out-of-distribution prompts compared to in-distribution performance. This fragility suggests that alignment methods based on supervised learning may not robustly generalize to deployment scenarios that differ from training.

**<EntityLink id="deceptive-alignment">Deceptive Alignment</EntityLink>**: Models may appear aligned while pursuing different goals. Current evaluation methods detect only 40-60% of sophisticated deception in red-team exercises. The theoretical possibility that advanced AI could deliberately perform well on alignment evaluations while harboring misaligned objectives remains a critical unsolved problem with no known robust detection method.

**Safety Alignment Depth**: Research published at OpenReview (2024) found that shallowly aligned models' generative distributions of harmful tokens remain "largely unaffected compared to unaligned counterparts" — harmful outputs can still be induced by bypassing refusal prefixes, demonstrating that surface-level alignment is insufficient.[^2] Subsequent work showed that fine-tuning attacks can undo safety alignment by training on a small number of harmful data points, and that alignment degradation compounds after successive fine-tuning rounds in models trained with SFT, RLHF, or DPO.[^3]

### Scalability Concerns

| Challenge | Current Status | Quantified Limitations | AGI Implications | Proposed Solutions | Success Probability |
|-----------|----------------|----------------------|------------------|-------------------|-------------------|
| **Human Oversight** | Bottleneck at superhuman tasks | Humans evaluate ≈100-200 outputs/day; models generate millions; 5-10% of complex tasks receive meaningful oversight | Superhuman AI will operate in domains where humans can't provide reliable feedback; oversight success drops to 52% at 400 Elo gap | Scalable oversight (debate, amplification), AI assistants, recursive reward modeling | 40-60% chance of working for near-AGI |
| **Evaluation** | Limited to observable behaviors | Current evals detect 85% of obvious misalignment, 40-60% of sophisticated deception; <EntityLink id="sandbagging">AI Capability Sandbagging</EntityLink> undetectable in 70%+ cases | Advanced AI could strategically underperform on evaluations; alignment faking observed in Claude 3 Opus | Adversarial testing, <EntityLink id="interpretability">Interpretability</EntityLink>, mechanistic anomaly detection | 30-50% for detecting deception |
| **Goal Specification** | Approximate, inconsistent | Human preference inconsistency: 20-40%; specification gaming: 15-30% of tasks; value learning accuracy: 60-75% on complex moral dilemmas | Value lock-in with wrong objectives; permanent misalignment; inability to correct superhuman systems | <R id="3cdbd40455756dc3">Value learning</R>, democratic input processes, iterated refinement | 25-45% for correct specification |
| **Robustness** | Fragile to distribution shift | Performance degradation: 25-50% on OOD prompts; adversarial examples fool aligned models 60-80% of time; robustness-capability tradeoff: 10-20% performance cost | <EntityLink id="distributional-shift">AI Distributional Shift</EntityLink> at deployment breaks alignment; novel scenarios not covered in training cause failures | Adversarial training, diverse testing, robustness incentives in training | 50-70% for near-domain shift |
| **Safety Classifier Extraction** | Active research threat | Surrogate classifiers achieve >80% F1 using 20% of model weights; 70% ASR against Llama 2 via surrogate vs 22% direct attack | Adversaries with white-box model access can extract and target safety mechanisms more cheaply than attacking full models directly | Model weight protection, access controls, adversarial robustness defenses (e.g., BCT reduced ASR from 67.8% to 2.9% on Gemini 2.5 Flash) | Under active investigation |

### Adversarial Robustness of Alignment

A growing body of research treats deployed alignment mechanisms as an attack surface — examining whether safety properties can be extracted, transferred, circumvented, or erased after training. This is distinct from the training-time alignment problem and has practical implications for models deployed with white-box or gray-box access.

**Safety Classifier Extraction**: Noirot Ferrand et al. (2025) demonstrated that alignment in LLMs embeds an implicit safety classifier, with decision-relevant representations concentrated in earlier architectural layers.[^4] By constructing surrogate classifiers from subsets of model weights, attackers can craft adversarial inputs more efficiently than by attacking the full model directly. The study evaluated "several state-of-the-art LLMs," showing generalizability across model families. The same surrogate approach reduces memory footprint and runtime compared to direct attacks, lowering the cost of alignment circumvention for adversaries with weight access.

**Transfer of Adversarial Examples**: Earlier work by [Zou et al. (2023)](https://arxiv.org/abs/2307.15043) introduced the Greedy Coordinate Gradient (GCG) algorithm, which optimizes universal adversarial suffixes across multiple open-source models and transfers to closed-source systems including ChatGPT, Bard, and Claude. The safety classifier extraction paradigm generalizes this: attacking a cheaper surrogate and transferring the result is more efficient than gradient-based optimization against the full model.

**Safety Misalignment Attacks**: Research published at NDSS 2025 identified three categories of post-deployment safety misalignment attack: system-prompt modification, model fine-tuning, and model editing. Supervised fine-tuning was identified as the most potent vector. The paper also introduced a Self-Supervised Representation Attack (SSRA) that achieves significant safety misalignment without requiring harmful example responses, further lowering barriers to alignment circumvention.[^5]

**Alignment Depth and Fine-Tuning Attacks**: Research shows that safety alignment can be erased by subsequent fine-tuning on as few harmful data points, with up to 50% greater safety degradation observed in distillation-trained models relative to fine-tuned equivalents.[^6] Continual learning approaches (e.g., Dark Experience Replay evaluated on Mistral-7B and Gemma-2B) show promise for preserving alignment across model lifecycle stages, but no method has achieved robust resistance across all evaluated attack types.[^7]

**Defense Research**: Bias-Augmented Consistency Training (BCT) reduced attack success rates on Gemini 2.5 Flash from 67.8% to 2.9% on the ClearHarm benchmark, though with measurable increases in over-refusals on benign prompts — illustrating the safety-utility tradeoff inherent in alignment defense.[^8]

**Implications**: The extractability of safety classifiers raises the question of whether alignment robustness should be treated as a security property requiring adversarial evaluation, not solely a training objective. White-box access to model weights — already standard for open-weight models and feasible through theft or insider access for proprietary models — is sufficient to mount these attacks. This has direct relevance to Anthropic's ASL-3 security measures, which explicitly target harder weight theft, and to the broader policy debate about open-weight model release.

## Alignment in National Security Contexts

The deployment of aligned AI in government and defense contexts introduces constraints and threat models that differ substantially from consumer applications. Alignment failures in high-stakes operational environments — including military systems, intelligence analysis, and critical infrastructure — carry consequences at scales that consumer deployment does not.

### Anthropic National Security and Public Sector Advisory Council

On August 27, 2025, <EntityLink id="anthropic">Anthropic</EntityLink> [announced](https://www.anthropic.com/news/introducing-the-anthropic-national-security-and-public-sector-advisory-council) the formation of a bipartisan National Security and Public Sector Advisory Council, first reported by [Axios](https://www.axios.com/2025/08/27/anthropic-ai-national-security-council). The council comprises 11 inaugural members drawn from the Department of Defense, Intelligence Community, Department of Energy, Department of Justice, and the U.S. Senate.

**Membership** includes: Roy Blunt (former Republican Senator, Senate Intelligence Committee); Jon Tester (former Democratic Senator, Defense Appropriations); Patrick M. Shanahan (former Acting Secretary of Defense); David S. Cohen (former Deputy CIA Director); Lisa E. Gordon-Hagerty (former NNSA Administrator); Jill Hruby (former NNSA Administrator, former Sandia National Laboratories director); Dave Luber (former NSA Director of Cybersecurity and former Cyber Command Executive Director); Christopher Fonzone (former Assistant Attorney General for OLC, former ODNI General Counsel); and Richard Fontaine (CEO of Center for a New American Security, also a member of Anthropic's Long-Term Benefit Trust).

**Stated mandate**: The council is tasked with identifying and developing high-impact AI applications in cybersecurity, intelligence analysis, and scientific research; expanding public-private partnerships; and shaping standards for responsible AI use in national security contexts. Anthropic stated the council will help drive "a race to the top" for national security AI applications.

**Institutional context**: The announcement followed Anthropic's launch of [Claude Gov models](https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers) — versions designed based on government customer feedback for applications including strategic planning, intelligence analysis, and threat assessment, and reportedly deployed on classified U.S. government networks. As of the announcement date, no comparable dedicated national security advisory council had been announced by OpenAI or Google DeepMind, according to reporting by Axios.

**Analytical perspectives**: Observers have offered competing interpretations of the council's significance. One view, advanced in analyses such as those from [AI 2 Work](https://ai2.work/news/ai-news-anthropic-advisory-council-2025/), is that the council reflects a strategy to advance influence over AI governance frameworks and secure privileged access to government contracts. Anthropic's stated framing emphasizes safety-conscious deployment in sensitive contexts and public-private partnership. These interpretations are not mutually exclusive, and the council's actual influence on policy or procurement will depend on factors not yet determinable.

**Alignment implications**: Deploying aligned models in national security contexts raises distinct questions. Aligned models' safety mechanisms must function correctly in adversarial, time-pressured, and classification-sensitive environments where the consequences of both over-refusal (mission failure) and under-refusal (harmful action) are severe. The dual-use nature of AI alignment research — where findings about safety classifier structure may be as useful to adversaries as to defenders — is particularly salient in defense contexts.

### Governance Landscape for National Security AI

Congressional and executive action has begun to address the governance of AI in defense contexts, though significant gaps remain.

**Legislative developments**: The FY2025 National Defense Authorization Act (NDAA) directed the Department of Defense to establish a cross-functional team led by the Chief Digital and AI Officer (CDAO) to create a Department-wide framework for assessing, governing, and approving AI model development, testing, and deployment. Legislation requires higher security levels for AI systems of greatest national security concern, including protection against highly capable cyber threat actors.[^9] The FY2026 NDAA directs the Secretary of Defense to establish an AI Futures Steering Committee to formulate proactive policy for evaluation, adoption, governance, and risk mitigation of advanced AI systems.[^10]

**Regulatory carve-outs**: Atlantic Council research notes that most wide-ranging civilian AI regulatory frameworks include carve-outs that exclude military use cases, and that the boundaries of these carve-outs are "at best porous when the technology is inherently dual-use in nature." Governance efforts for national security AI are "largely detached from the wider civil AI regulation debate," creating potential inconsistencies between civilian and defense alignment standards.[^11]

**Agentic AI governance gap**: As of early 2026, the Congressional Research Service notes "there are no known official government guidance or policies yet specifically on agentic AI" within the Department of Defense. Agentic systems operating with autonomy in intelligence or cyber contexts represent a category where alignment requirements — particularly corrigibility and oversight — are least well-defined and most consequential.[^12]

**Multi-agent risk**: SIPRI (2025) argues that if AI agents are deployed in government services, critical infrastructure, and military operations, misalignment could impact international peace and security, and calls for new international safeguards specifically addressing multi-agent AI in high-stakes contexts. Current LLM-based agents are "hard to observe and are non-deterministic — making it difficult to predict how an agent will behave in a given situation."[^13]

**Dual-use alignment research**: The safety classifier extraction work of Noirot Ferrand et al. (2025) illustrates a dual-use dynamic: the same methodology that enables cheaper adversarial evaluation of aligned models also enables cheaper jailbreaking. This is structurally analogous to offensive/defensive research in cybersecurity, where knowledge of vulnerability classes is necessary for defense but simultaneously informs attack. The national security community's engagement with alignment research — including through advisory bodies like Anthropic's council — will need to navigate this tension.

## Expert Perspectives

### Expert Survey Data

The [AI Impacts 2024 survey](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) of 2,778 AI researchers provides the most comprehensive view of expert opinion on alignment:

| Question | Median Response | Range |
|----------|----------------|-------|
| **50% probability of human-level AI** | 2040 | 2027-2060 |
| **Alignment rated as top concern** | Majority of senior researchers | — |
| **P(catastrophe from misalignment)** | 5-20% | 1-50%+ |
| **AGI by 2027** | 25% probability | [Metaculus](https://www.metaculus.com/) average |
| **AGI by 2031** | 50% probability | [Metaculus](https://www.metaculus.com/) average |

Individual expert predictions vary widely: [Andrew Critch](https://acritch.com/) has estimated a 45% chance of AGI by end of 2026 {/* NEEDS CITATION */}; [Paul Christiano](https://paulfchristiano.com/) (head of US AI Safety Institute) has given a 30% chance of transformative AI by 2033 {/* NEEDS CITATION */}; Sam Altman, Demis Hassabis, and Dario Amodei have each projected AGI within 3-5 years in public statements.

### Optimistic Views

**<EntityLink id="paul-christiano">Paul Christiano</EntityLink>** (formerly OpenAI, now leading ARC): Argues that alignment is likely easier than capabilities and that iterative improvement through techniques like iterated amplification can scale to AGI. His work on <R id="61da2f8e311a2bbf">debate</R> and <R id="77e9bf1a01a5b587">amplification</R> suggests that decomposing hard problems into easier sub-problems can enable human oversight of superhuman systems, though he acknowledges significant uncertainty about whether these approaches will scale sufficiently.

**<EntityLink id="dario-amodei">Dario Amodei</EntityLink>** (Anthropic CEO): Points to Constitutional AI's measured 75% reduction in harmful outputs as evidence that AI-assisted alignment methods can work. In Anthropic's <R id="5fa46de681ff9902">"Core Views on AI Safety"</R>, he argues that AI systems can be made helpful, harmless, and honest through careful research and scaling of current techniques, while acknowledging that significant ongoing investment is required.

**<EntityLink id="jan-leike">Jan Leike</EntityLink>** (formerly OpenAI Superalignment, now Anthropic): His work on <R id="e64c8268e5f58e63">weak-to-strong generalization</R> demonstrates that strong models can outperform their weak supervisors by 30-60% of the capability gap. He has described this as a promising direction for superhuman alignment, while noting that "we are still far from recovering the full capabilities of strong models" and that significant research remains before the approach can be considered sufficient.

### Pessimistic Views

**<EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink>** (MIRI founder): Argues that current alignment approaches are insufficient for the AGI problem and that alignment is extremely difficult. He contends that prosaic alignment techniques such as RLHF will not scale to AGI-level systems and estimates the probability of catastrophic outcomes from misalignment at above 90% without major strategic pivots. He characterizes most current alignment work as not addressing what he considers the core technical problems.

**<EntityLink id="neel-nanda">Neel Nanda</EntityLink>** (Google DeepMind): While optimistic about the long-term potential of mechanistic interpretability, he has stated that interpretability progress is proceeding more slowly than capability advances and that current methods can mechanistically explain less than 5% of model behaviors in state-of-the-art systems — a coverage level that is insufficient for robust alignment verification.

**MIRI Researchers**: Generally argue that prosaic alignment — scaling existing techniques — is unlikely to suffice for AGI. They emphasize the difficulty of value specification, the risk of deceptive alignment, and the absence of reliable feedback loops for correcting a misaligned AGI after deployment. Published estimates for alignment success probability from MIRI-affiliated researchers cluster around 10-30% under current research trajectories.

## Timeline & Projections

### Near-term (1-3 years)
- Improved interpretability tools for current models
- Better evaluation methods for alignment
- Constitutional AI refinements
- Preliminary control mechanisms
- Adversarial robustness evaluation frameworks for deployed aligned models

### Medium-term (3-7 years)
- Scalable oversight methods tested
- Automated alignment research assistants
- Advanced interpretability for larger models
- Governance frameworks for alignment
- Standardized safety testing protocols for national security AI deployment

### Long-term (7+ years)
- AGI alignment solutions or clear failure modes identified
- Robust value learning systems
- Comprehensive AI control frameworks
- International alignment standards
- Resolved frameworks for dual-use alignment research publication norms

### Technical Cruxes
- **Will interpretability scale?** Current methods may hit fundamental limits
- **Is deceptive alignment detectable?** Models may learn to hide misalignment
- **Can we specify human values?** <R id="6b7fc3f234fa109c">Value specification remains unsolved</R>
- **Do current methods generalize?** RLHF may break with capability jumps
- **Can safety classifiers be made robust to extraction?** Surrogate-based attacks suggest current alignment mechanisms are extractable given weight access

### Strategic Questions
- **Research prioritization**: Which approaches deserve the most investment?
- **<EntityLink id="pause-debate">Should We Pause AI Development?</EntityLink>**: Whether capability development should slow to allow alignment research to catch up
- **Coordination needs**: How much international cooperation is required?
- **Timeline pressure**: Can alignment research keep pace with capabilities?
- **Open-weight models and alignment security**: Whether releasing model weights creates unacceptable extraction risk for safety mechanisms
- **National security alignment standards**: How should alignment requirements differ for defense and intelligence applications versus consumer deployment?

## Sources & Resources

### Core Research Papers

| Category | Key Papers | Authors | Year |
|----------|------------|---------|------|
| **Comprehensive Survey** | <R id="f612547dcfb62f8d">AI Alignment: A Comprehensive Survey</R> | Ji, Qiu, Chen et al. (PKU) | 2023-2025 |
| **Foundations** | <R id="cd3035dbef6c7b5b">Alignment for Advanced AI</R> | Taylor, Hadfield-Menell | 2016 |
| **RLHF** | <R id="1098fc60be7ca2b0">Training Language Models to Follow Instructions</R> | OpenAI | 2022 |
| **Constitutional AI** | <R id="683aef834ac1612a">Constitutional AI: Harmlessness from AI Feedback</R> | Anthropic | 2022 |
| **Constitutional AI** | <R id="3c862a18b467640b">Collective Constitutional AI</R> | Anthropic | 2024 |
| **Debate** | <R id="61da2f8e311a2bbf">AI Safety via Debate</R> | Irving, Christiano, Amodei | 2018 |
| **Amplification** | <R id="77e9bf1a01a5b587">Iterated Distillation and Amplification</R> | Christiano et al. | 2018 |
| **Recursive Reward Modeling** | <R id="56fa6bd15dd062af">Scalable Agent Alignment via Reward Modeling</R> | Leike et al. | 2018 |
| **Weak-to-Strong** | <R id="e64c8268e5f58e63">Weak-to-Strong Generalization</R> | OpenAI | 2023 |
| **Weak-to-Strong** | <R id="e4fb663747c74f50">Improving Weak-to-Strong with Scalable Oversight</R> | Multiple authors | 2024 |
| **Interpretability** | <R id="b948d6282416b586">A Mathematical Framework</R> | Anthropic | 2021 |
| **Interpretability** | <R id="426fcdeae8e2b749">Scaling Monosemanticity</R> | Anthropic | 2024 |
| **Scalable Oversight** | <R id="f7ce4e3a86afd07a">A Benchmark for Scalable Oversight</R> | Multiple authors | 2025 |
| **Recursive Critique** | <R id="6d1732ab914da313">Scalable Oversight via Recursive Self-Critiquing</R> | Multiple authors | 2025 |
| **Control** | <R id="187aaa26886ce183">AI Control: Improving Safety Despite Intentional Subversion</R> | Redwood Research | 2023 |
| **Safety Classifier Extraction** | [Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs](https://arxiv.org/abs/2501.16534) | Noirot Ferrand, Beugin, Pauley, Sheatsley, McDaniel | 2025 |
| **Adversarial Attacks** | [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043) | Zou, Wang, Carlini, Nasr, Kolter, Fredrikson | 2023 |
| **Safety Misalignment** | [Safety Misalignment Against Large Language Models](https://www.ndss-symposium.org/wp-content/uploads/2025-1089-paper.pdf) | NDSS 2025 | 2025 |
| **Alignment Depth** | [Safety Alignment Should Be Made More Than Skin-Deep](https://openreview.net/pdf?id=6Mxhg9PtDE) | Multiple authors | 2024 |
| **Safety Distillation** | [To Distill or Not to Distill: Knowledge Transfer Undermines Safety](https://openreview.net/forum?id=AEKji3PwD9) | Multiple authors | 2025 |

### Recent Empirical Studies (2023-2025)

- <R id="b5b86fd37cd96469">Debate May Help AI Models Converge on Truth</R> - Quanta Magazine (2024)
- <R id="311a21a10c96b10d">Scalable Human Oversight for Aligned LLMs</R> - IIETA (2024)
- <R id="a2f0c5f433869914">Scaling Laws for Scalable Oversight</R> - ArXiv (2025)
- <R id="ac9591c7ebccb8a9">An Alignment Safety Case Sketch Based on Debate</R> - ArXiv (2025)

### Organizations & Labs

| Type | Organizations | Focus Areas |
|------|---------------|-------------|
| **AI Labs** | <EntityLink id="openai">OpenAI</EntityLink>, <EntityLink id="anthropic">Anthropic</EntityLink>, <EntityLink id="deepmind">Google DeepMind</EntityLink> | Applied alignment research |
| **Safety Orgs** | <EntityLink id="chai">CHAI</EntityLink>, <EntityLink id="miri">MIRI</EntityLink>, <EntityLink id="redwood-research">Redwood Research</EntityLink> | Fundamental alignment research |
| **Evaluation** | <EntityLink id="arc">ARC</EntityLink>, <EntityLink id="metr">METR</EntityLink> | Capability assessment, control |

### Policy & Governance Resources

| Resource Type | Links | Description |
|---------------|-------|-------------|
| **Government** | <R id="54dbc15413425997">NIST AI RMF</R>, <EntityLink id="uk-aisi">UK AI Safety Institute</EntityLink> | Policy frameworks |
| **Industry** | <R id="0e7aef26385afeed">Partnership on AI</R>, <R id="394ea6d17701b621">Anthropic RSP</R> | Industry initiatives |
| **Academic** | <R id="c0a5858881a7ac1c">Stanford HAI</R>, <R id="0aa86d6b61aea588">MIT FutureTech</R> | Research coordination |
| **National Security** | [Anthropic National Security and Public Sector Advisory Council](https://www.anthropic.com/news/introducing-the-anthropic-national-security-and-public-sector-advisory-council) (Aug 2025) | Government-AI industry coordination on defense deployment |
| **Defense Policy** | [DoD's AI Balancing Act (CFR)](https://www.cfr.org/article/dods-ai-balancing-act) | Analysis of DoD alignment and adoption challenges |
| **Regulatory** | [Second-Order Impacts of Civil AI Regulation on Defense (Atlantic Council)](https://www.atlanticcouncil.org/in-depth-research-reports/report/second-order-impacts-of-civil-artificial-intelligence-regulation-on-defense-why-the-national-security-community-must-engage/) | Dual-use governance analysis |

---

## AI Transition Model Context

AI alignment research is the primary intervention for reducing <EntityLink id="misalignment-potential">Misalignment Potential</EntityLink> in the <EntityLink id="alignment">AI Alignment</EntityLink> model:

| Factor | Parameter | Impact |
|--------|-----------|--------|
| <EntityLink id="misalignment-potential">Misalignment Potential</EntityLink> | <EntityLink id="alignment-robustness">Alignment Robustness</EntityLink> | Core objective: ensure AI systems pursue intended goals reliably |
| <EntityLink id="misalignment-potential">Misalignment Potential</EntityLink> | <EntityLink id="safety-capability-gap">Safety-Capability Gap</EntityLink> | Research must keep pace with capability advances to maintain safety margins |
| <EntityLink id="misalignment-potential">Misalignment Potential</EntityLink> | <EntityLink id="human-oversight-quality">Human Oversight Quality</EntityLink> | Scalable oversight extends human control to superhuman systems |
| <EntityLink id="misalignment-potential">Misalignment Potential</EntityLink> | <EntityLink id="interpretability-coverage">Interpretability Coverage</EntityLink> | Understanding model internals enables verification of alignment |

Alignment research directly addresses whether advanced AI systems will be safe and beneficial, making it central to all scenarios in the AI transition model.

---

[^1]: The alignment faking behavior in Claude 3 Opus was documented in controlled experiments; the finding has not been independently replicated across other model families as of early 2026.

[^2]: ["Safety Alignment Should Be Made More Than Skin-Deep"](https://openreview.net/pdf?id=6Mxhg9PtDE), OpenReview, 2024. The paper demonstrates that refusal-level alignment can be bypassed by circumventing refusal prefixes, and proposes augmented fine-tuning as a deeper alignment approach evaluated across AlpacaEval, MMLU, BBH, MATH, GSM8k, and HumanEval benchmarks.

[^3]: ["Unforgotten Safety: Preserving Safety Alignment of LLMs with Continual Learning"](https://arxiv.org/html/2512.10150v1), December 2025. Evaluated on Mistral-7B and Gemma-2B; DER identified as a robust continual learning defense.

[^4]: ["Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs"](https://arxiv.org/abs/2501.16534), Noirot Ferrand, Beugin, Pauley, Sheatsley, McDaniel; accepted to IEEE SaTML 2026; arXiv:2501.16534, January 2025. Assumes white-box access throughout; licensed CC BY-NC-ND 4.0.

[^5]: ["Safety Misalignment Against Large Language Models"](https://www.ndss-symposium.org/wp-content/uploads/2025-1089-paper.pdf), NDSS Symposium 2025. Covers system-prompt modification, fine-tuning, and model editing attack vectors; introduces SSRA.

[^6]: ["To Distill or Not to Distill: Knowledge Transfer Undermines Safety of LLMs"](https://openreview.net/forum?id=AEKji3PwD9), OpenReview, October 2025. Logit-based soft label distillation produces up to 50% greater safety degradation than fine-tuning on equivalent data.

[^7]: ["Unforgotten Safety: Preserving Safety Alignment of LLMs with Continual Learning"](https://arxiv.org/html/2512.10150v1), arXiv, December 2025.

[^8]: ["AI Safety at the Frontier: Paper Highlights of November 2025"](https://www.lesswrong.com/posts/8bLSDMWnL4BcHgA6k/ai-safety-at-the-frontier-paper-highlights-of-november-2025), LessWrong, November 2025. BCT results reported on ClearHarm benchmark for Gemini 2.5 Flash.

[^9]: ["Congress Moves Forward with AI Measures in Key Defense Legislation"](https://www.akingump.com/en/insights/alerts/congress-moves-forward-with-ai-measures-in-key-defense-legislation), Akin Gump, 2024-2025.

[^10]: ["Agentic AI and Cyberattacks"](https://www.congress.gov/crs-product/IF13151), Congressional Research Service, 2025-2026.

[^11]: ["Second-Order Impacts of Civil AI Regulation on Defense"](https://www.atlanticcouncil.org/in-depth-research-reports/report/second-order-impacts-of-civil-artificial-intelligence-regulation-on-defense-why-the-national-security-community-must-engage/), Atlantic Council, 2024-2025.

[^12]: Congressional Research Service, ["Agentic AI and Cyberattacks"](https://www.congress.gov/crs-product/IF13151), 2025-2026.

[^13]: ["Before It's Too Late: Why a World of Interacting AI Agents Demands New Safeguards"](https://www.sipri.org/commentary/essay/2025/its-too-late-why-world-interacting-ai-agents-demands-new-safeguards), SIPRI, 2025.
