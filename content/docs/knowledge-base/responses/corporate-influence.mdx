---
title: Corporate Influence on AI Policy
description: A comprehensive analysis of directly influencing frontier AI labs through working inside them, shareholder activism, whistleblowing, and transparency advocacy. Examines the effectiveness, risks, and strategic considerations of corporate influence approaches to AI safety, including quantitative estimates of impact and career trajectories.
sidebar:
  order: 5
quality: 66
llmSummary: "Comprehensive analysis of corporate influence pathways (working inside labs, shareholder activism, whistleblowing) showing mixed effectiveness: safety teams influenced GPT-4 delays and responsible scaling policies, but ~50% of OpenAI's safety staff departed in 2024 and the November 2023 board crisis demonstrated commercial pressures override safety concerns. Provides specific compensation data ($115K-$190K for researchers), talent flow metrics (8x more likely to leave OpenAI for Anthropic), and detailed assessment that 1,500-2,500 people work in safety roles globally with 60% in SF Bay Area."
lastEdited: "2025-12-28"
importance: 72.5
update_frequency: 45
todos:
  - Complete 'How It Works' section
  - Complete 'Limitations' section (6 placeholders)
ratings:
  novelty: 4.5
  rigor: 6.5
  actionability: 7
  completeness: 7.5
clusters:
  - ai-safety
  - community
  - governance
subcategory: field-building
---
import {DataInfoBox, DisagreementMap, KeyQuestions, Mermaid, R, EntityLink, DataExternalLinks} from '@components/wiki';

<DataExternalLinks pageId="corporate-influence" />

<DataInfoBox entityId="E78" />

## Overview

**Direct corporate influence** represents one of the most immediate and controversial approaches to AI safety: working within or pressuring frontier AI labs to make safer decisions about developing and deploying advanced AI systems. Rather than building governance structures or conducting independent research, this approach attempts to shape the behavior of the organizations that are actually building potentially transformative AI systems.

The theory is compelling in its directness—if <EntityLink id="E218">OpenAI</EntityLink>, <EntityLink id="E22">Anthropic</EntityLink>, <EntityLink id="E98">Google DeepMind</EntityLink>, and other frontier labs are the entities closest to developing AGI, then influencing their decisions may be the most direct path to reducing existential risk. This could mean joining their safety teams, using shareholder pressure, exposing dangerous practices through whistleblowing, or advocating for better safety culture from within.

However, this approach involves significant moral complexity. Critics argue that working at frontier labs provides legitimacy and talent to organizations engaged in a dangerous race toward AGI, potentially accelerating risks even when intending to reduce them. The effectiveness depends heavily on whether safety-conscious individuals can meaningfully influence critical decisions, or whether competitive pressures ultimately override safety considerations. Current evidence suggests mixed results: while safety teams have influenced some deployment decisions and led to <EntityLink id="E461">responsible scaling policies</EntityLink>, they have also struggled to prevent concerning incidents like the OpenAI board crisis of November 2023 or the dissolution of OpenAI's Superalignment team in 2024.

### Quick Assessment

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| **Tractability** | Medium | Significant barriers to entry; influence often limited by commercial pressures |
| **Neglectedness** | Low | Well-funded with competitive compensation; 1,500-2,500 people in safety-relevant roles at frontier labs |
| **Scale of Impact** | Potentially High | Direct proximity to critical decisions, but influence often overridden |
| **Counterfactual** | Uncertain | Would roles be filled by less safety-conscious candidates? Evidence unclear |
| **Career Capital** | High | Technical skills, network access, and inside knowledge remain valuable regardless |
| **Moral Hazard** | Significant | Legitimization of <EntityLink id="E239">racing dynamics</EntityLink>; perspective capture risks |

## Strategic Landscape and Mechanisms

### Working Inside Frontier Labs

The most direct form of corporate influence involves joining frontier AI labs, particularly in safety-focused roles. This approach has grown significantly since 2020, with major labs now employing hundreds of people on safety-related work.

#### Frontier AI Lab Safety Staff (2024-2025)

| Lab | Total Staff | Safety Team Size | Safety % | Notable Changes | Risk Management Score |
|-----|-------------|------------------|----------|-----------------|----------------------|
| **Anthropic** | ≈1,100 (2025) | 150-300 estimated | 15-25% | Grew from 300 to 950 in 2024; intentionally slowed hiring | 35% (highest) |
| **Google DeepMind** | ≈6,600 | 30-50 AGI alignment + additional safety teams | ≈1-2% | New AI Safety and Alignment org formed Feb 2024; team grew 37% in 2024 | 20% |
| **OpenAI** | ≈4,400 (2025) | ~16 AGI safety (down from ≈30) | &lt;1% | Superalignment team disbanded May 2024; nearly 50% safety staff departed | 33% |
| **Meta AI** | ≈3,000+ | Unknown | Unknown | Minimal public safety commitments | 22% |
| **xAI** | ≈200-400 | Unknown | Unknown | No public safety framework | Not rated |

*Risk management scores from <R id="4a4e9653bfe81d29">SaferAI assessment</R>; "No AI company scored better than 'weak.'"*

**Anthropic** self-describes as "an AI safety and research company" and maintains dedicated Interpretability, Alignment, Societal Impacts, and Frontier Red Teams. **Google DeepMind** has an AGI Safety Council led by co-founder Shane Legg, plus a Responsibility and Safety Council and Ethics and Society unit. **OpenAI's safety landscape remains concerning** following the dissolution of its Superalignment team in May 2024 and the departure of key safety leaders including <EntityLink id="E163">Ilya Sutskever</EntityLink>, <EntityLink id="E182">Jan Leike</EntityLink>, and Miles Brundage.

Safety roles typically fall into several categories, each with different risk-benefit profiles. **Core safety researchers** work on alignment, interpretability, and evaluation problems with direct access to frontier models. Their influence comes through developing safety techniques, informing responsible scaling policies, and providing technical input on deployment decisions.

#### Compensation at Frontier AI Labs (2024)

| Role Type | Anthropic | OpenAI | DeepMind | Notes |
|-----------|-----------|--------|----------|-------|
| **Research Scientist** | \$115K-\$160K | \$100K-\$165K | \$150K-\$100K | Interpretability, alignment focus |
| **Research Engineer** | \$115K-\$190K | \$150K-\$130K | \$100K-\$150K | Up to \$190K at Anthropic for senior |
| **Software Engineer** | \$100K-\$159K | \$150K-\$150K | \$100K-\$100K | High variance based on seniority |
| **Policy/Trust & Safety** | \$198K-\$150K | \$150K-\$100K | \$150K-\$180K | Lower than technical roles |
| **Median Total Comp** | \$145K | ≈\$100K | ≈\$100K | Includes base, bonus, equity |

*Sources: <R id="bfe53c4aedc94ec0">Levels.fyi</R>, <R id="b0b9088dd0532507">AI Paygrades</R>, company job postings. Negotiation can increase offers 30-77%.*

**Safety-adjacent roles** include policy positions that shape lab stances on regulation, communications roles that frame AI safety for public consumption, and security positions preventing model theft and misuse. These roles carry lower complicity risks since they don't directly advance capabilities, but also typically have less technical influence over core safety decisions.

The most controversial category involves **capabilities researchers and engineers** who directly advance AI performance. Some safety advocates argue these roles are net negative regardless of individual intentions, since they accelerate the timeline to potentially dangerous systems. Others contend that having safety-conscious people in capabilities roles is crucial for ensuring safety considerations are integrated into fundamental research directions rather than bolted on afterward.

Evidence for insider influence comes from several documented cases. Safety teams influenced the delayed release of GPT-4 in 2023, conducted extensive red-teaming that identified concerning capabilities, and contributed to the development of responsible scaling policies at multiple labs. However, the limits of this influence were also demonstrated during OpenAI's November 2023 board crisis, where safety concerns about rushing deployment were ultimately overridden by investor and employee pressure to reinstate <EntityLink id="E269">Sam Altman</EntityLink>.

#### Corporate Influence Pathways

<Mermaid chart={`
flowchart TD
    subgraph Internal["Internal Influence"]
        SAFETY[Safety Team] --> DEPLOY[Deployment Decisions]
        SAFETY --> RSP[Responsible Scaling Policies]
        SAFETY --> EVAL[Model Evaluations]
        CAPS[Capabilities Staff] --> CULTURE[Safety Culture]
    end

    subgraph External["External Pressure"]
        INVESTOR[Investors] --> BOARD[Board/Governance]
        WHISTLE[Whistleblowing] --> MEDIA[Media/Public]
        REG[Regulators] --> COMPLIANCE[Compliance Requirements]
    end

    subgraph Outcomes["Outcomes"]
        DEPLOY --> DELAY[Delayed Releases]
        BOARD --> LEADERSHIP[Leadership Changes]
        MEDIA --> PRESSURE[Public Pressure]
        COMPLIANCE --> INVEST[Safety Investment]
    end

    BOARD --> DEPLOY
    PRESSURE --> BOARD

    style SAFETY fill:#cfc
    style WHISTLE fill:#ffc
    style INVESTOR fill:#ccf
`} />

### Shareholder Activism and Governance Pressure

Shareholder activism remains largely untapped due to the private nature of most frontier labs, but presents significant theoretical leverage.

#### The OpenAI Board Crisis (November 2023): A Case Study

On November 17, 2023, OpenAI's board removed CEO Sam Altman, citing concerns that he was "not consistently candid in his communications" and steering the company away from its safety-focused mission. Within five days, he was reinstated after massive investor and employee pressure:

| Day | Event | Key Actors |
|-----|-------|------------|
| Nov 17 | Board removes Altman; cites safety concerns | Board (Toner, McCauley, Sutskever, D'Angelo) |
| Nov 18 | <R id="26c052ddbbcbabdf">Microsoft and investors press for reinstatement</R> | Microsoft (\$10B+ invested), Thrive Capital |
| Nov 19 | Emmett Shear named interim CEO; board holds firm | OpenAI board |
| Nov 20 | <R id="1fcc2347f44b4242">700+ of 770 employees sign letter threatening resignation</R> | 90% of staff |
| Nov 22 | Altman reinstated; board reconstituted | New board excludes Toner, McCauley; Sutskever later departs |

**Key lesson:** Investor pressure and employee revolt overwhelmed governance structures explicitly designed to prioritize safety. The board members who orchestrated the removal—except D'Angelo—were replaced. Sutskever, who initially supported the removal, departed in May 2024 to found <R id="3fc4ee87e9bacb20">Safe Superintelligence Inc</R>.

Most frontier labs remain private or are subsidiaries of larger companies, limiting direct shareholder pressure. Anthropic is privately held with significant investment from Google and Amazon. OpenAI operates under an unusual capped-profit structure but remains largely privately controlled. Only Google (parent of DeepMind) and Microsoft (OpenAI's key partner) are fully public companies where traditional shareholder activism could apply, but AI represents a small fraction of their overall business.

The potential for shareholder influence may increase as the AI industry matures. <R id="acefec9235932fb2">Bloomberg Intelligence projects</R> **global ESG assets will reach \$10 trillion by 2030**, up from \$10+ trillion in 2022. Over half of global institutional assets are now managed by UN Principles for Responsible Investment signatories. However, popularity of major tech stocks among ESG investors began cooling in 2023 after AI data center energy demands raised environmental concerns.

Effective shareholder activism would require coordinated efforts across multiple investor types: pension funds concerned about long-term stability, ESG-focused funds emphasizing governance, and individual investors willing to file shareholder resolutions. The key challenge lies in aligning investor incentives with safety outcomes rather than purely financial returns.

### Whistleblowing and Transparency Mechanisms

Whistleblowing represents perhaps the highest-risk, highest-potential-impact form of corporate influence. **Current legal protections for AI whistleblowers remain weak**, with limited precedent for protection. However, 2024 saw unprecedented activity in AI whistleblowing.

#### Key Whistleblowing Events (2024)

| Date | Event | Actors | Outcome |
|------|-------|--------|---------|
| May 2024 | <R id="e751ccb632c5857b">Jan Leike resigns, posts "safety culture has taken a backseat to shiny products"</R> | Jan Leike (Superalignment lead) | Joined Anthropic; significant media coverage |
| June 2024 | <R id="194b8a6feedff102">Open letter from 13 AI workers</R> on safety risks and whistleblower protections | 11 OpenAI + 2 DeepMind employees | Catalyzed legislative action |
| July 2024 | <R id="ed8b161851bc498b">Anonymous SEC whistleblower complaint</R> alleging illegal NDAs | Anonymous | SEC investigation; Congressional letters to OpenAI |
| Aug 2024 | <R id="970d203f69571bd2">Daniel Kokotajlo reveals ≈50% AGI safety staff departed</R> | Former OpenAI researcher | Confirmed safety team exodus |
| Oct 2024 | <R id="0a8da5fe117a4c50">Miles Brundage resigns; AGI Readiness team disbanded</R> | Miles Brundage | Another major safety departure |

**Effective whistleblowing faces several structural challenges.** The <R id="4301747d3cf92c14">SEC whistleblower complaint</R> alleged four violations in OpenAI's employment agreements: non-disparagement clauses lacking SEC disclosure exemptions, requiring company consent for federal disclosures, confidentiality requirements covering agreements with embedded violations, and requiring employees to waive SEC whistleblower compensation. OpenAI spokesperson Hannah Wong stated the company would remove nondisparagement terms from future exit paperwork.

#### Legislative Response: AI Whistleblower Protection Act

In response to these events, <R id="d9723cc62cbaaaac">Senate Judiciary Committee Chair Chuck Grassley introduced the AI Whistleblower Protection Act</R>, a bipartisan bill co-sponsored by Senators Coons (D-Del.), Blackburn (R-Tenn.), Klobuchar (D-Minn.), Hawley (R-Mo.), and Schatz (D-Hawai'i). Key provisions include:

- **Prohibition on retaliation** against employees reporting AI safety failures
- **Relief mechanisms** including reinstatement, double back pay, and compensatory damages
- **Complaint process** through Department of Labor with federal court appeals
- **Explicit protection** for communications to Congress and federal agencies

The bill received support from 22 groups including the National Whistleblower Center. However, as of late 2024, it has not yet been enacted.

## Current Deployment and Quantitative Assessment

The direct corporate influence approach has grown substantially since 2020, driven by increased recognition of AI risks and significant funding for safety work. **Current estimates suggest 1,500-2,500 people globally work in safety-relevant positions at frontier AI labs**, though this depends heavily on how "safety-relevant" is defined.

#### Talent Flow Dynamics

A notable asymmetry has emerged in talent flows between labs. According to industry analyses, **engineers are 8x more likely to leave OpenAI for Anthropic than the reverse**. Key researchers who departed OpenAI—including Jan Leike, Chris Olah, and other founding members—joined Anthropic, which has positioned itself as the "safety-first" alternative. This suggests safety culture may be a significant factor in talent decisions.

**The geographical distribution of safety roles remains heavily concentrated**, with approximately 60% in the San Francisco Bay Area, 25% in London (primarily DeepMind), and 15% distributed across other locations including New York, Boston, and remote positions. This concentration creates both advantages (critical mass of expertise) and risks (groupthink and similar perspectives).

#### Counterfactual Impact Assessment

Assessment of counterfactual impact remains highly uncertain. The key questions for career decisions include:

| Question | Optimistic View | Pessimistic View | Current Evidence |
|----------|----------------|------------------|------------------|
| Would someone less safety-conscious fill the role? | Yes—talent is scarce | No—labs would hire fewer | Limited data; likely varies by lab |
| Do safety teams influence critical decisions? | Yes—GPT-4 delays, RSPs | No—commercial pressure dominates | Mixed; OpenAI crisis suggests limits |
| Does working at labs provide legitimacy? | Minimal effect | Yes—signals responsible development | Plausibly significant |
| Is perspective capture a real risk? | Minimal—people maintain values | Yes—financial and social incentives | Anecdotal reports both ways |
| Are skills transferable to other safety work? | Yes—technical and network value | Partially—some lock-in | Generally positive |

Career progression data shows **relatively high retention in safety roles (80-85% after two years)** compared to capabilities research (70-75%), suggesting either greater job satisfaction or fewer alternative opportunities. However, this may change as the independent safety research ecosystem grows and provides more exit opportunities for lab employees.

## Safety Implications and Risk Assessment

The direct corporate influence approach presents both significant opportunities and concerning risks for AI safety. On the promising side, **safety teams have demonstrably influenced critical deployment decisions**. The staged release of GPT-4, extensive red-teaming programs, and development of responsible scaling policies all reflect safety input into lab operations. These interventions may have prevented premature deployment of dangerous capabilities or at minimum slowed development timelines.

**Responsible scaling policies represent perhaps the most significant positive development.** Anthropic's AI Safety Level framework creates explicit thresholds for enhanced safety measures as capabilities increase. If models reach concerning capability levels (like advanced biological weapons design), the policy triggers enhanced security measures, testing requirements, and potentially deployment pauses. Similar frameworks at DeepMind and other labs suggest growing acceptance of structured approaches to safety-performance tradeoffs.

However, the approach also carries substantial risks that critics argue may outweigh benefits. **The legitimacy provided by safety teams may accelerate dangerous development** by making it appear responsible and well-governed. Talented safety researchers joining labs signals to investors, regulators, and the public that risks are being managed, potentially reducing pressure for external governance or more fundamental changes to development practices.

**Competitive dynamics pose perhaps the greatest challenge to internal safety influence.** Even well-intentioned labs face pressure to match competitors' capabilities and deployment timelines. Safety concerns that might delay products or limit capabilities face strong internal resistance when competitors appear to be racing ahead. The OpenAI board crisis demonstrated how even governance structures explicitly designed to prioritize safety can be overwhelmed by commercial pressure.

**Perspective capture represents a more subtle but potentially serious risk.** Employees of AI labs naturally develop inside views that may systematically underestimate risks or overestimate the effectiveness of safety measures. The social environment, financial incentives, and professional relationships all create pressure to view lab activities favorably. Some former lab employees report that concerns that seemed urgent from the outside appeared less pressing from the inside, though they disagreed about whether this reflected better information or problematic bias.

Recent safety team departures highlight the limits of internal influence. **Jan Leike's resignation statement that "safety culture has taken a backseat to shiny products" at OpenAI** suggests that even senior safety leaders can feel their influence is insufficient. Similar concerns have been reported at other labs, though usually more privately.

## Future Trajectory and Development Scenarios

### Near-term Development (1-2 years)

The landscape for direct corporate influence will likely evolve significantly in the near term as AI capabilities advance and regulatory pressure increases. **Safety team sizes are expected to grow 50-100%** across major labs, driven by both increasing recognition of risks and potential regulatory requirements for safety staff. However, this growth may be outpaced by expansion in capabilities research, potentially reducing safety teams' relative influence.

**Regulatory developments will significantly shape the effectiveness of corporate influence approaches.** The EU AI Act's requirements for high-risk AI systems may force labs to invest more heavily in safety infrastructure, while potential US legislation could mandate safety testing and disclosure requirements. These external requirements could strengthen the hand of internal safety advocates by providing regulatory backing for safety measures that might otherwise be overruled by competitive pressure.

**The privateness of most frontier labs represents a major limiting factor for shareholder activism**, but this may change. Several labs are reportedly considering public offerings or major funding rounds that could create opportunities for investor pressure. The growing interest from ESG-focused funds and pension funds in AI governance could create significant pressure if appropriate mechanisms exist.

Whistleblowing may become more common and effective as legal protections develop and public interest in AI safety increases. **Several jurisdictions are considering AI-specific whistleblower protections**, while media coverage of AI safety has grown substantially, creating more opportunities for impactful disclosure of concerning practices.

### Medium-term Evolution (2-5 years)

Over a 2-5 year horizon, the effectiveness of direct corporate influence will depend heavily on how competitive dynamics and regulatory frameworks evolve. **If international coordination on AI development emerges**, internal safety advocates could gain significantly more influence by having external backing for safety measures. Conversely, if competition intensifies further, internal pressure to prioritize capabilities over safety may increase.

**The maturation of AI capabilities will test responsible scaling policies** and other safety frameworks developed by corporate safety teams. If models begin demonstrating concerning capabilities like advanced biological weapons design or autonomous research capability, the effectiveness of current safety measures will become apparent. Success in managing these transitions could validate the corporate influence approach, while failures might discredit it.

**Public market dynamics may become increasingly relevant** as more AI companies go public or mature funding markets develop. This could enable more traditional forms of shareholder activism and corporate governance pressure. However, it might also increase short-term pressure for financial returns that conflicts with long-term safety considerations.

The independent AI safety ecosystem is likely to mature significantly, providing more attractive exit opportunities for lab employees and potentially changing recruitment dynamics. **If organizations like Redwood Research, ARC, or new government AI safety institutions** can offer competitive compensation and resources, they may attract talent away from frontier labs or provide credible outside options that strengthen negotiating positions.

## Key Uncertainties and Research Priorities

Several critical uncertainties determine the ultimate effectiveness of direct corporate influence approaches. **The question of net impact remains fundamentally unresolved**: does working at frontier labs reduce existential risk by improving safety practices, or increase risk by accelerating development and providing legitimacy to dangerous racing dynamics?

**Measurement challenges complicate assessment of impact.** Unlike some other safety interventions, it's difficult to quantify the counterfactual effects of safety team work. When a concerning capability is identified during testing, how much does this reduce ultimate risk compared to discovering it after deployment? When a safety team influences deployment decisions, how much additional risk reduction does this provide beyond what would have occurred anyway due to reputational concerns or liability issues?

**The durability of safety culture improvements remains highly uncertain.** Current safety investments might represent genuine long-term commitments to responsible development, or they might be temporary responses to public and regulatory pressure that could erode when that pressure diminishes or competitive dynamics intensify. The speed of potential culture change in either direction is also unclear.

**Regulatory development trajectories will significantly impact the relative value of different corporate influence approaches.** Strong regulatory frameworks with meaningful enforcement could make internal safety advocacy much more effective by providing external backing. Weak or captured regulatory frameworks might make internal influence less valuable relative to other interventions.

Key research priorities include developing better methods for measuring safety team impact, analyzing the conditions under which internal safety advocates maintain influence over critical decisions, and understanding how competitive dynamics affect the sustainability of safety investments. Comparative analysis of safety culture across different labs and tracking changes over time could provide important insights for career decisions and strategic planning.

---

**Direct corporate influence represents a high-stakes, morally complex approach to AI safety that may prove either essential or counterproductive depending on implementation details and external factors. Its ultimate effectiveness will likely depend on maintaining genuine safety influence within labs while avoiding the legitimization of dangerous racing dynamics—a balance that remains challenging to achieve.**

---

## Sources & Further Reading

### Primary Sources

- **CNBC (May 2024):** <R id="33a4513e1449b55d">OpenAI dissolves Superalignment AI safety team</R> - Comprehensive coverage of the dissolution and Jan Leike's departure
- **Washington Post (July 2024):** <R id="ed8b161851bc498b">OpenAI illegally barred staff from airing safety risks, whistleblowers say</R> - SEC whistleblower complaint details
- **Bloomberg (November 2023):** <R id="1fcc2347f44b4242">90% of OpenAI Staff Threaten to Go to Microsoft If Board Doesn't Quit</R> - The employee revolt during the Altman crisis
- **Fortune (August 2024):** <R id="970d203f69571bd2">OpenAI Exodus: Nearly half of AGI safety team gone</R> - Daniel Kokotajlo's revelations about safety staff departures

### Legislative & Policy

- **Senate Judiciary Committee:** <R id="863da0838b7bc974">Grassley Introduces AI Whistleblower Protection Act</R> - Full text and co-sponsors of the AI WPA
- **Institute for Law & AI:** <R id="2ae7df16aad64338">Protecting AI whistleblowers</R> - Analysis of legal protections needed

### Industry Analysis

- **Levels.fyi:** <R id="bfe53c4aedc94ec0">Anthropic Salaries</R> - Verified compensation data
- **SaferAI:** Risk management assessment of frontier AI companies (35% highest score for Anthropic; "no company scored better than 'weak'")
- **Bloomberg Intelligence:** <R id="acefec9235932fb2">Global ESG assets predicted to hit \$10 trillion by 2030</R>

### Career Resources

- **80,000 Hours:** <R id="b81d89ad5c71c87b">Nick Joseph on Anthropic's safety approach</R> - Inside perspective on RSPs
- **IAPS:** <R id="c4fbe78110edcfab">Mapping Technical Safety Research at AI Companies</R> - Comparative analysis of lab safety work
- **AI Lab Watch:** <R id="91ca6b1425554e9a">Commitments tracker</R> - Monitoring lab safety commitments

---

## AI Transition Model Context

Corporate influence improves the <EntityLink id="ai-transition-model" /> through multiple factors:

| Factor | Parameter | Impact |
|--------|-----------|--------|
| <EntityLink id="E205" /> | <EntityLink id="E264" /> | ≈30% of Anthropic's 300 staff work on safety due to mission-driven hiring |
| <EntityLink id="E205" /> | <EntityLink id="E20" /> | Insider researchers push for better safety practices and testing |
| <EntityLink id="E358" /> | <EntityLink id="E242" /> | Shareholder activism (\$10T ESG funds) creates pressure for responsible development |

Insider influence shows mixed results: GPT-4 was delayed for safety testing, but OpenAI's board attempt to slow development failed and resulted in Altman's reinstatement.
