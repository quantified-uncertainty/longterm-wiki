---
title: Bletchley Declaration
description: World-first international agreement on AI safety signed by 28 countries at the November 2023 AI Safety Summit, committing to cooperation on frontier AI risks.
readerImportance: 52.5
researchImportance: 36
lastEdited: "2026-02-01"
update_frequency: 21
sidebar:
  order: 65
ratings:
  novelty: 4
  rigor: 7
  actionability: 5
  completeness: 8
quality: 60
llmSummary: The Bletchley Declaration represents a significant diplomatic achievement in establishing international consensus on AI safety cooperation among 28 countries including the US and China, though its non-binding nature and lack of enforcement mechanisms limit its practical impact. While creating valuable institutional frameworks like national AI Safety Institutes, the declaration's long-term effectiveness depends entirely on sustained voluntary compliance and follow-through.
clusters:
  - governance
  - ai-safety
subcategory: governance
entityType: approach
---
import {EntityLink, KeyPeople, KeyQuestions, Section} from '@components/wiki';

## Quick Assessment

| Dimension | Assessment |
|-----------|------------|
| **Type** | International agreement / Policy initiative |
| **Date** | November 1-2, 2023 |
| **Signatories** | 28 countries + European Union |
| **Focus** | Frontier AI safety, international cooperation |
| **Status** | Active (follow-up summits in Seoul 2024, Paris 2025) |
| **Binding** | Non-binding voluntary agreement |
| **Key Outcome** | Establishment of national <EntityLink id="ai-safety-institutes">AI Safety Institutes</EntityLink> |

## Key Links

| Source | Link |
|--------|------|
| Official Website | [gov.uk](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023) |
| Wikipedia | [en.wikipedia.org](https://en.wikipedia.org/wiki/AI_Safety_Summit) |

## Overview

The **Bletchley Declaration** is a landmark international agreement on AI safety signed at the first global AI Safety Summit, held at Bletchley Park, United Kingdom, on November 1-2, 2023.[^1] The declaration was endorsed by 28 countries—including the United States, China, United Kingdom, Australia, Brazil, Canada, France, Germany, Japan, Saudi Arabia, and Singapore—plus the European Union, representing a rare consensus between geopolitical rivals on <EntityLink id="ai-governance">AI governance</EntityLink>.[^2][^3]

The agreement centers on managing risks from **frontier AI**: highly capable general-purpose AI models that can perform a wide variety of tasks and pose potential for "serious, even catastrophic, harm, either deliberate or unintentional."[^4] Signatories committed to identify shared AI safety risks through scientific research, develop risk-based policies with transparency and accountability, and establish international cooperation mechanisms including national AI Safety Institutes.[^5][^6]

The summit's symbolic location at Bletchley Park—the WWII codebreaking site where Alan Turing and others developed early computers—emphasized the historical significance of the moment and the need for collaborative innovation in technology governance.[^7] While critics described the declaration as offering "good optics, less substance" with mainly platitudes and non-binding commitments,[^8] supporters hailed it as a diplomatic breakthrough establishing momentum for ongoing international AI safety cooperation.[^9]

## History and Development

### Origins and Summit Organization

The AI Safety Summit emerged from growing international concern about rapid advances in AI capabilities and their potential risks. The UK government, under Prime Minister Rishi Sunak, positioned itself as a leader in AI safety governance and hosted the inaugural summit as a national priority.[^10][^11] The choice of Bletchley Park as the venue was deliberately symbolic, connecting the summit to the site's legacy of wartime collaboration on computational technology and cryptography, where innovations like the Colossus computer helped crack the German Enigma code.[^7]

The summit brought together over 100 representatives from governments, businesses, civil society, academia, and international organizations including the United Nations.[^12][^13] Notable attendees included US Vice President Kamala Harris, European Commission President Ursula von der Leyen, <EntityLink id="elon-musk">Elon Musk</EntityLink> (CEO of Tesla and owner of X/Twitter), <EntityLink id="sam-altman">Sam Altman</EntityLink> (CEO of <EntityLink id="openai">OpenAI</EntityLink>), and Mustafa Suleyman (co-founder of <EntityLink id="deepmind">DeepMind</EntityLink>).[^10] China's participation, represented by Vice-Minister of Science and Technology Wu Zhaohui, was particularly significant given ongoing US-China tensions over technology and AI development.[^14]

### Declaration Content and Core Commitments

The Bletchley Declaration establishes several foundational commitments for international AI safety cooperation:[^4][^5]

**Risk Identification and Understanding**: Signatories committed to building a shared, evidence-based understanding of AI safety risks, particularly from frontier AI systems. The declaration acknowledges risks including:
- Misuse for terrorism, crime, and warfare
- Loss of human control over AI systems
- Catastrophic harm from unintended consequences
- Amplification of risks in cybersecurity and biotechnology
- Societal harms including misinformation, bias, and privacy violations

**Developer Responsibility**: The declaration emphasizes that actors developing frontier AI capabilities bear "particularly strong responsibility" for ensuring system safety through rigorous safety testing, evaluations, and appropriate mitigation measures.[^4][^15]

**Transparency and Accountability**: Frontier AI developers are urged to provide transparency about their plans to measure, monitor, and mitigate potentially harmful capabilities, while maintaining appropriate protections for proprietary information.[^5]

**Policy Development**: Countries committed to developing risk-based policies tailored to their national circumstances and legal frameworks, while promoting international cooperation and alignment on safety standards.[^4][^16]

**International Research Networks**: The declaration calls for establishing an "internationally inclusive network of scientific research on frontier AI safety" to complement existing initiatives and build collective expertise.[^5]

### Five Summit Objectives

The AI Safety Summit was organized around five core objectives that shaped the declaration's focus:[^17][^2]

1. Developing shared understanding of frontier AI risks and opportunities
2. Establishing frameworks for international collaboration on AI safety
3. Identifying appropriate safety measures for organizations developing frontier AI
4. Advancing research collaboration on AI capabilities, risks, and safety standards
5. Showcasing AI's positive applications for global challenges

## Institutional Outcomes

### National AI Safety Institutes

One of the most concrete outcomes of the summit was the establishment or announcement of national AI Safety Institutes designed to evaluate, test, and develop safety standards for frontier AI systems.[^18][^19]

**<EntityLink id="uk-aisi">UK AI Safety Institute</EntityLink>**: Announced at the summit, the UK institute (rebranded from the Frontier AI Taskforce) was designed to conduct independent evaluations of advanced AI systems, with major AI companies agreeing to provide early access to models for safety testing before public release.[^10][^20] The institute was partly modeled on the Intergovernmental Panel on Climate Change (IPCC) approach to building scientific consensus.[^18]

**<EntityLink id="us-aisi">US AI Safety Institute</EntityLink>**: Established within the National Institute of Standards and Technology (NIST), the US institute focuses on developing evaluation metrics, safety testing frameworks, and coordination with international partners.[^21] President Joe Biden signed an executive order requiring AI developers to share safety test results with the government, though this order was later rescinded by President Donald Trump.[^22][^23]

These institutes formed the nucleus of an emerging international network for AI safety research and evaluation, with commitments for ongoing collaboration and information sharing.[^19]

### Private Sector Commitments

Seven leading AI companies published voluntary safety policies covering nine safety areas including misuse prevention, transparency, and collaboration with government safety institutes.[^24] At a follow-up event, 16 technology companies from North America, Asia, Europe, and the Middle East signed Frontier AI Safety Commitments to publish safety frameworks, establish risk thresholds, and commit to refraining from deploying models that exceed their defined safety thresholds.[^25]

## Follow-Up Process and Evolution

### Seoul Summit (May 2024)

The Seoul Summit, co-hosted by the UK and South Korea in May 2024, reaffirmed and extended the Bletchley commitments.[^26] Key outcomes included:

**Seoul Declaration**: Articulated core AI safety principles including transparency, interpretability and explainability, privacy and accountability, meaningful human oversight, and effective data management and protection.[^26]

**Frontier AI Safety Commitments**: Sixteen companies committed to publishing detailed safety frameworks and establishing clear thresholds for "intolerable" risks, with pledges to halt development or deployment of models exceeding those thresholds.[^26][^25]

**Seoul Statement of Intent**: Established framework for interdisciplinary AI safety science and cooperation between national AI Safety Institutes.[^26]

### Paris AI Action Summit (February 2025)

The AI Action Summit in Paris, scheduled for February 10-11, 2025, was planned as a critical checkpoint for assessing sustained commitment to the Bletchley process and determining whether participating nations could deliver meaningful progress on <EntityLink id="technical-ai-safety">technical AI safety</EntityLink> collaboration, testing frameworks, and risk verification.[^27][^28] The French government positioned this summit to move "from safety to action," responding to criticism that earlier events focused on "splashy production values" without sufficient follow-through on concrete safety measures.[^28]

## Key Themes and Policy Approaches

### Frontier AI Definition and Scope

The declaration focuses specifically on "frontier AI"—defined as highly capable general-purpose AI models that can perform a wide variety of tasks and match or exceed the capabilities of today's most advanced systems.[^3][^29] This includes foundation models and <EntityLink id="language-models">large language models</EntityLink> with potential dual-use applications in both beneficial and harmful contexts. The emphasis on frontier AI reflects concern that as capabilities increase, so do both opportunities and catastrophic risks, requiring proactive safety measures before deployment.[^4]

### Balancing Innovation and Safety

A recurring theme throughout the declaration is the commitment to "pro-innovation governance" that maximizes AI benefits while managing risks.[^16][^30] Signatories affirmed that AI should be designed, developed, deployed, and used in a manner that is "safe, human-centric, trustworthy, and responsible" throughout its lifecycle.[^3][^5] This approach attempts to avoid stifling beneficial innovation while ensuring adequate safety measures, though critics note the tension between these goals and the lack of binding enforcement mechanisms.[^31]

### Inclusivity and the Digital Divide

The declaration explicitly calls for bridging the digital divide and ensuring inclusive global dialogue involving governments, companies, civil society, and academia.[^32][^33] This reflects recognition that AI risks and benefits are inherently international and that effective governance requires participation beyond major AI-developing nations. However, the initial summit's limited attendance beyond major powers like the US, China, and EU raised questions about how effectively this inclusivity principle would be realized in practice.[^26]

## Criticisms and Limitations

### Non-Binding and Voluntary Nature

The declaration's most significant limitation is its non-binding, voluntary character. It aims for "largely voluntary agreement" on safety assessments without enforcement mechanisms or globally harmonized thresholds for unacceptable risks.[^26][^34] This voluntary approach leaves implementation dependent on national will and resources, with no penalties for non-compliance or mechanisms to ensure consistent application of safety standards across jurisdictions.

### Lack of Specificity and Resources

Critics described the declaration as offering "good optics, less substance," with primarily platitudes and vague commitments that rely on existing initiatives rather than creating new binding obligations.[^8] The agreement lacks details on specific resources, funding commitments, or concrete implementation plans. The UK's domestic Central Risk Function, for example, was criticized for providing limited information on capacity and immediate risk handling procedures.[^35]

### Divergent National Approaches

While the declaration brings together major powers, it explicitly acknowledges that policies "may vary across countries according to national circumstances and applicable legal frameworks."[^4][^16] This recognition of differing approaches potentially undermines efforts toward unified global standards, as countries with less aggressive regulatory stances (like the UK compared to the EU's AI Act) may create regulatory arbitrage opportunities.[^36]

### Uncertain Long-Term Impact

Several experts noted that while the declaration represents progress in establishing dialogue, its effectiveness depends entirely on sustained follow-through.[^37][^38] The politicized nature of AI governance and the lack of binding commitments mean the agreement's long-term impact remains uncertain. As one assessment noted, the declaration and summit process could prove either a "building block for global cooperation" or merely symbolic gestures without meaningful policy outcomes.[^26]

## Significance and Context

### Geopolitical Dimension

The inclusion of both the United States and China as signatories represents a notable diplomatic achievement given escalating tensions over technology, trade, and AI development between these powers.[^14][^39] The declaration demonstrates that despite competitive dynamics, there exists some common ground on the risks posed by advanced AI systems and the need for international cooperation on safety measures.

### Positioning in Global AI Governance

The Bletchley Declaration sits within a broader landscape of AI governance initiatives, including:
- The <EntityLink id="eu-ai-act">EU AI Act</EntityLink> (comprehensive regulatory framework)
- OECD AI Principles (ethical guidelines)
- UNESCO Recommendation on the Ethics of AI
- Various bilateral and multilateral partnerships

The declaration's contribution is its specific focus on frontier AI safety risks and the establishment of dedicated national safety institutes as implementation mechanisms.[^19][^40] It complements rather than replaces these existing frameworks, operating through "existing international fora and other relevant initiatives."[^5]

### Research and Technical Collaboration

Beyond policy commitments, the declaration catalyzed concrete research collaboration through the network of AI Safety Institutes and commitments to produce a "State of the Science" report on frontier AI risks.[^19][^26] This technical cooperation on evaluation methodologies, safety testing protocols, and risk assessment frameworks may prove more durable than high-level political commitments, establishing working relationships between researchers and institutions across borders.

## Key Uncertainties

Several critical questions remain about the Bletchley Declaration's implementation and impact:

**Implementation mechanisms**: How will commitments translate into concrete actions? What resources will nations allocate to AI Safety Institutes and related research? Will voluntary compliance prove sufficient, or will binding international agreements eventually be necessary?

**Standard harmonization**: Can participating nations develop sufficiently aligned safety standards and risk thresholds despite differing regulatory philosophies and national interests? What mechanisms will resolve disagreements about acceptable risk levels?

**Inclusion of developing nations**: Will the process effectively include voices and interests from developing countries that may lack resources to participate fully in AI safety research and governance? How will benefits and risks be distributed globally?

**Commercial incentives**: Will commercial pressures to deploy advanced AI systems undermine safety commitments? How will governments balance economic competitiveness with safety precautions?

**Technical feasibility**: Are current AI safety evaluation and testing methods adequate to identify and mitigate risks from increasingly capable systems? What research breakthroughs are needed to make safety commitments practically achievable?

**Sustained political will**: Can participating nations maintain high-level political commitment across election cycles and changing administrations? The rescission of President Biden's executive order by President Trump illustrates the fragility of policy continuity.[^23]

## Sources

[^1]: [AI Safety Summit - Wikipedia](https://en.wikipedia.org/wiki/AI_Safety_Summit)
[^2]: [Hot Topics: AI Safety Summit 2023 - A Rundown of the Bletchley Declaration](https://hottopics.ht/insights/ai-safety-summit-2023-a-rundown-of-the-bletchley-declaration)
[^3]: [Enterprise Nation: Bletchley Declaration on Safety of Artificial Intelligence](https://www.enterprisenation.com/learn-something/bletchley-declaration-safety-artificial-intelligence-ai/)
[^4]: [UK Government: The Bletchley Declaration](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)
[^5]: [Ganado: What is the Bletchley Declaration and Why Does it Matter for AI Safety?](https://ganado.com/what-is-the-bletchley-declaration-and-why-does-it-matter-for-ai-safety/)
[^6]: [Partnership on AI: UK AI Safety Summit](https://partnershiponai.org/uk-ai-safety-summit/)
[^7]: [Bletchley Park: Bletchley Park Makes History Again as Host of World's First AI Safety Summit](https://www.bletchleypark.org.uk/bletchley-park-makes-history-again-as-host-of-the-worlds-first-ai-safety-summit/)
[^8]: [Lowy Institute: Bletchley Park AI Summit - Good Optics, Less Substance](https://www.lowyinstitute.org/the-interpreter/bletchley-park-artificial-intelligence-summit-good-optics-less-substance)
[^9]: [Sidley Data Matters: World-First Agreement on AI Reached](https://datamatters.sidley.com/2023/12/07/world-first-agreement-on-ai-reached/)
[^10]: [AI Safety Summit - Wikipedia](https://en.wikipedia.org/wiki/AI_Safety_Summit)
[^11]: [UK AI Summit - YouTube (Rishi Sunak announcement)](https://www.youtube.com/watch?v=jm1Re0r4Txw)
[^12]: [White Case: Guardians of the AI Galaxy - Lessons from Bletchley Park](https://www.whitecase.com/insight-our-thinking/guardians-ai-galaxy-lessons-bletchley-park)
[^13]: [AP Law Solution: The Bletchley Declaration - A New Paradigm for Global AI Safety](https://www.ap-lawsolution.com/actio/regulatory-alert-the-bletchley-declaration-a-new-paradigm-for-global-ai-safety)
[^14]: [AI Safety Summit - Wikipedia](https://en.wikipedia.org/wiki/AI_Safety_Summit)
[^15]: [EA Forum: The Bletchley Declaration on AI Safety](https://forum.effectivealtruism.org/posts/j2TreuRZT9mBFEMEs/the-bletchley-declaration-on-ai-safety)
[^16]: [CFG Europe: Bletchley Declaration on Safety of Frontier AI](https://cfg.eu/bletchley-declaration-safety-frontier-ai/)
[^17]: [AI Safety Summit - Wikipedia](https://en.wikipedia.org/wiki/AI_Safety_Summit)
[^18]: [AI Safety Summit - Wikipedia](https://en.wikipedia.org/wiki/AI_Safety_Summit)
[^19]: [Partnership on AI: UK AI Safety Summit](https://partnershiponai.org/uk-ai-safety-summit/)
[^20]: [White Case: Guardians of the AI Galaxy - Lessons from Bletchley Park](https://www.whitecase.com/insight-our-thinking/guardians-ai-galaxy-lessons-bletchley-park)
[^21]: [AI Safety Summit - Wikipedia](https://en.wikipedia.org/wiki/AI_Safety_Summit)
[^22]: [Tech Policy Press: From Safety to Action - The Upcoming French AI Summit](https://techpolicy.press/from-safety-to-action-the-upcoming-french-ai-summit)
[^23]: [Tech Policy Press: From Safety to Action - The Upcoming French AI Summit](https://techpolicy.press/from-safety-to-action-the-upcoming-french-ai-summit)
[^24]: [White Case: Guardians of the AI Galaxy - Lessons from Bletchley Park](https://www.whitecase.com/insight-our-thinking/guardians-ai-galaxy-lessons-bletchley-park)
[^25]: [UK Government: Historic First as Companies Agree Safety Commitments on AI](https://www.gov.uk/government/news/historic-first-as-companies-spanning-north-america-asia-europe-and-middle-east-agree-safety-commitments-on-development-of-ai)
[^26]: [Brookings: The Bletchley Park Process Could Be a Building Block for Global Cooperation on AI Safety](https://www.brookings.edu/articles/the-bletchley-park-process-could-be-a-building-block-for-global-cooperation-on-ai-safety/)
[^27]: [Brookings: The Bletchley Park Process Could Be a Building Block for Global Cooperation on AI Safety](https://www.brookings.edu/articles/the-bletchley-park-process-could-be-a-building-block-for-global-cooperation-on-ai-safety/)
[^28]: [Tech Policy Press: From Safety to Action - The Upcoming French AI Summit](https://techpolicy.press/from-safety-to-action-the-upcoming-french-ai-summit)
[^29]: [HCL Tech: AI Safety Summit 2023 - Bletchley Declaration Forges New Path for AI Safety](https://www.hcltech.com/trends-and-insights/ai-safety-summit-2023-bletchley-declaration-forges-new-path-ai-safety)
[^30]: [Ganado: What is the Bletchley Declaration and Why Does it Matter for AI Safety?](https://ganado.com/what-is-the-bletchley-declaration-and-why-does-it-matter-for-ai-safety/)
[^31]: [UK and EU: The UK AI Summit - Successes, Trade-offs, and an Uncertain Path Ahead](https://ukandeu.ac.uk/the-uk-ai-summit-successes-trade-offs-and-an-uncertain-path-ahead/)
[^32]: [Ganado: What is the Bletchley Declaration and Why Does it Matter for AI Safety?](https://ganado.com/what-is-the-bletchley-declaration-and-why-does-it-matter-for-ai-safety/)
[^33]: [White Case: Guardians of the AI Galaxy - Lessons from Bletchley Park](https://www.whitecase.com/insight-our-thinking/guardians-ai-galaxy-lessons-bletchley-park)
[^34]: [Brookings: The Bletchley Park Process Could Be a Building Block for Global Cooperation on AI Safety](https://www.brookings.edu/articles/the-bletchley-park-process-could-be-a-building-block-for-global-cooperation-on-ai-safety/)
[^35]: [UK and EU: The UK AI Summit - Successes, Trade-offs, and an Uncertain Path Ahead](https://ukandeu.ac.uk/the-uk-ai-summit-successes-trade-offs-and-an-uncertain-path-ahead/)
[^36]: [RPC Legal: AI Safety Summit and the Bletchley Declaration](https://www.rpclegal.com/snapshots/technology-digital/winter-2023/ai-safety-summit-and-the-bletchley-declaration/)
[^37]: [Oxford AI Experts: Comment on Outcomes of UK AI Safety Summit](https://www.ox.ac.uk/news/2023-11-03-expert-comment-oxford-ai-experts-comment-outcomes-uk-ai-safety-summit)
[^38]: [UK and EU: The UK AI Summit - Successes, Trade-offs, and an Uncertain Path Ahead](https://ukandeu.ac.uk/the-uk-ai-summit-successes-trade-offs-and-an-uncertain-path-ahead/)
[^39]: [AI Safety Summit - Wikipedia](https://en.wikipedia.org/wiki/AI_Safety_Summit)
[^40]: [Partnership on AI: UK AI Safety Summit](https://partnershiponai.org/uk-ai-safety-summit/)

