---
title: Safety Responses
description: Interventions and approaches to address AI safety risks
sidebar:
  label: Overview
  order: 0
---
import {EntityLink} from '@components/wiki';


## Overview

This section documents interventions and approaches being developed to address AI safety risks. Responses span technical research, governance mechanisms, institutional development, and public engagement.

## Response Categories

### <EntityLink id="responses/alignment/interpretability">Technical Alignment</EntityLink>
Research aimed at ensuring AI systems behave as intended:
- <EntityLink id="responses/alignment/interpretability">Mechanistic Interpretability</EntityLink> - Understanding model internals
- <EntityLink id="responses/alignment/rlhf">RLHF</EntityLink> - Reinforcement learning from human feedback
- <EntityLink id="responses/alignment/constitutional-ai">Constitutional AI</EntityLink> - Training with explicit principles
- <EntityLink id="responses/alignment/ai-control">AI Control</EntityLink> - Limiting AI autonomy regardless of alignment
- <EntityLink id="responses/alignment/evals">Evaluations</EntityLink> - Testing for dangerous capabilities

### <EntityLink id="responses/governance/compute-governance/thresholds">Governance</EntityLink>
Policy and regulatory approaches:
- <EntityLink id="responses/governance/compute-governance/thresholds">Compute Governance</EntityLink> - Controlling AI through hardware
- <EntityLink id="responses/governance/compute-governance/export-controls">Export Controls</EntityLink> - Restricting chip access
- <EntityLink id="responses/governance/industry/responsible-scaling-policies">Responsible Scaling Policies</EntityLink> - Lab commitments
- <EntityLink id="responses/governance/legislation/eu-ai-act">Legislation</EntityLink> - Government regulation

### <EntityLink id="responses/institutions/ai-safety-institutes">Institutions</EntityLink>
Organizations and structures for AI safety:
- <EntityLink id="responses/institutions/ai-safety-institutes">AI Safety Institutes</EntityLink> - Government research bodies
- <EntityLink id="responses/institutions/standards-bodies">Standards Bodies</EntityLink> - Technical standard development

### <EntityLink id="responses/epistemic-tools/coordination-tech">Epistemic Tools</EntityLink>
Technologies to preserve information integrity:
- <EntityLink id="responses/epistemic-tools/coordination-tech">Coordination Technologies</EntityLink> - Enabling cooperation
- <EntityLink id="responses/epistemic-tools/content-authentication">Content Authentication</EntityLink> - Verifying authentic media
- <EntityLink id="responses/epistemic-tools/prediction-markets">Prediction Markets</EntityLink> - Aggregating forecasts

### <EntityLink id="responses/field-building/training-programs">Field Building</EntityLink>
Growing the AI safety research community:
- <EntityLink id="responses/field-building/training-programs">Training Programs</EntityLink> - Researcher development
- <EntityLink id="responses/field-building/corporate-influence">Corporate Influence</EntityLink> - Engaging industry

### Biosecurity
Interventions addressing AI-enabled biological risks:
- DNA Synthesis Screening - Preventing dangerous pathogen reconstruction (SecureDNA, IBBIS)
- Metagenomic Surveillance - Pathogen-agnostic early warning (NAO/SecureBio)
- Medical Countermeasures - Resilience-based defenses (Red Queen Bio, platform vaccines)
- Far-UVC & Physical Defenses - Environmental pathogen reduction (Blueprint Biosecurity)
- AI Bio-Capability Evaluations - Measuring AI biological uplift (VCT, red-teaming)

## Evaluating Responses

Each response page includes assessments of:
- **Tractability** - How feasible is progress?
- **Neglectedness** - How much attention is it getting?
- **Potential Impact** - How much could it help if successful?

See the <EntityLink id="responses/intervention-portfolio">Intervention Portfolio</EntityLink> for comparative analysis.
