---
title: Interpretability
description: Understanding the internal workings of AI systems - from mechanistic interpretability to representation engineering.
sidebar:
  label: Overview
  order: 0
subcategory: alignment-interpretability
---
import {EntityLink} from '@components/wiki';

Interpretability research aims to understand what AI systems are "thinking" and why they behave as they do.

**Overview:**
- **<EntityLink id="E174">Interpretability</EntityLink>**: The field and its importance for safety

**Mechanistic Approaches:**
- **<EntityLink id="E477">Mechanistic Interpretability</EntityLink>**: Reverse-engineering neural networks
- **<EntityLink id="E480">Sparse Autoencoders</EntityLink>**: Learning interpretable features
- **<EntityLink id="E596">Probing</EntityLink>**: Testing for specific knowledge or concepts
- **<EntityLink id="E478">Circuit Breakers</EntityLink>**: Identifying and modifying specific circuits

**Representation-Based:**
- **<EntityLink id="E479">Representation Engineering</EntityLink>**: Controlling behavior via internal representations
