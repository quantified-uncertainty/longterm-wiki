---
numericId: E650
title: Interpretability
importance: 89.5
researchImportance: 26.5
description: Understanding the internal workings of AI systems - from mechanistic interpretability to representation engineering.
sidebar:
  label: Overview
  order: 0
subcategory: alignment-interpretability
entityType: approach
---
import {EntityLink} from '@components/wiki';

Interpretability research aims to understand what AI systems are "thinking" and why they behave as they do.

**Overview:**
- **<EntityLink id="interpretability">Interpretability</EntityLink>**: The field and its importance for safety

**Mechanistic Approaches:**
- **<EntityLink id="mech-interp">Mechanistic Interpretability</EntityLink>**: Reverse-engineering neural networks
- **<EntityLink id="sparse-autoencoders">Sparse Autoencoders</EntityLink>**: Learning interpretable features
- **<EntityLink id="probing">Probing</EntityLink>**: Testing for specific knowledge or concepts
- **<EntityLink id="circuit-breakers">Circuit Breakers</EntityLink>**: Identifying and modifying specific circuits

**Representation-Based:**
- **<EntityLink id="representation-engineering">Representation Engineering</EntityLink>**: Controlling behavior via internal representations
