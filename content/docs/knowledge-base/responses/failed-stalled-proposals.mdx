---
title: "Failed and Stalled AI Policy Proposals"
description: "Analysis of failed AI governance initiatives reveals systematic patterns including industry opposition spending $61.5M from Big Tech alone in 2024 (up 13% YoY), definitional challenges, jurisdictional complexity, and fundamental mismatches between technology development speed and legislative cycles. The 118th Congress introduced over 150 AI bills with zero becoming law. While comprehensive frameworks like California's SB 1047 face vetoes, incremental approaches with industry support show higher success rates."
sidebar:
  order: 15
llmSummary: "Analysis of AI policy failures reveals systematic patterns: 150+ federal bills with 0% passage rate, industry lobbying up 141% YoY to 648 companies, and 97% public support for AI safety rules versus minimal binding regulation. Comprehensive frameworks fail at ~5% rate while incremental transparency approaches succeed at 35-60% rates."
lastEdited: "2026-02-11"
importance: 40
researchImportance: 83.5
update_frequency: 7
ratings:
  focus: 7.2
  novelty: 4.1
  rigor: 5.8
  completeness: 7.5
  concreteness: 6.9
  actionability: 5.4
  objectivity: 4.2
clusters:
  - "ai-safety"
  - "governance"
subcategory: "governance-legislation"
quality: 63
entityType: approach
---
import {DataInfoBox, Mermaid, R, EntityLink} from '@components/wiki';

<DataInfoBox entityId="E137" />

## Quick Assessment

| Dimension | Score | Evidence |
|-----------|:-----:|----------|
| **Policy Success Rate** | 2/10 | 150+ federal bills introduced with 0 passed; 97% comprehensive framework failure rate |
| **Industry Opposition Intensity** | 9/10 | 577% increase in OpenAI lobbying spend; 648 companies lobbying on AI (up 141% YoY) |
| **Public-Private Alignment** | 3/10 | 97% public support for AI safety rules vs. minimal binding regulation |
| **International Coordination** | 2/10 | No binding treaties; US-China competition blocks multilateral frameworks |

*Assessment reflects 2024-2025 data on AI governance outcomes versus stated policy objectives.*

## Overview

Failed and stalled AI policy proposals provide critical insights into the political economy of AI governance, revealing systematic patterns that explain why comprehensive regulation remains elusive despite widespread concern about AI risks. The failure rate for ambitious AI legislation is remarkably high: during the 118th Congress, lawmakers introduced [over 150 AI-related bills with none passing into law](https://cset.georgetown.edu/publication/tracker-congressional-activity-on-artificial-intelligence/)[^1]. Meanwhile, industry opposition has intensified dramatically, with [648 companies lobbying on AI in 2024 versus 458 in 2023](https://www.opensecrets.org/federal-lobbying/issues/summary?cycle=2024&id=AIS)[^2]—a 141% year-over-year increase—while [AI companies collectively spent \$2.71 million on lobbying in 2024, four times their 2023 spending](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/)[^3].

The disconnect between public demand and policy outcomes is striking. [Gallup polling from May 2025 found 97% of Americans agree AI safety should be subject to rules and regulations](https://news.gallup.com/poll/694685/americans-prioritize-safety-data-security.aspx)[^4], while [Public Citizen analysis shows 72% of Americans want more regulation of the AI industry—a 15-point increase from the prior year](https://www.citizen.org/article/years-of-polling-show-overwhelming-voter-support-for-a-crackdown-on-ai/)[^5]. Yet comprehensive binding regulation remains absent at the federal level.

These failures illuminate fundamental tensions in <EntityLink id="ai-governance">AI governance</EntityLink>: the speed mismatch between rapid technological development and deliberative legislative processes, the challenge of defining "artificial intelligence" in legally precise terms, and the complex jurisdictional landscape where multiple agencies and levels of government claim regulatory authority. Perhaps most significantly, failed proposals demonstrate how industry opposition mobilizes around specific regulatory mechanisms, particularly <EntityLink id="liability-frameworks">liability provisions</EntityLink> and mandatory compliance requirements, while showing greater tolerance for disclosure obligations and <EntityLink id="voluntary-standards">voluntary frameworks</EntityLink>.

The pattern of failures suggests that successful AI governance may require accepting incremental progress rather than comprehensive solutions, with <EntityLink id="voluntary-commitments">voluntary industry commitments</EntityLink> serving as necessary stepping stones to eventual binding regulation. This dynamic has profound implications for AI safety, as it may mean that meaningful oversight emerges only after significant harms occur, rather than through proactive prevention.

### Major Failed and Stalled AI Proposals (2024-2025)

| Proposal | Jurisdiction | Year | Key Provisions | Primary Failure Reason | Status | Industry Opposition |
|----------|-------------|------|----------------|----------------------|--------|-------------------|
| <EntityLink id="california-sb1047">California SB 1047</EntityLink> | California | 2024 | Safety testing for models >\$100M compute, shutdown requirements, liability | Governor veto citing federal preemption, industry opposition | Vetoed Sep 2024 | [OpenAI, Meta, Nancy Pelosi](https://www.politico.com/news/2024/08/22/pelosi-california-ai-regulation-00175510) |
| Algorithmic Accountability Act | US Federal | 2019, 2022, 2023 | Impact assessments for automated decision systems | Failed to exit committee in three consecutive Congresses | Stalled | [Amazon, Google lobbying](https://www.opensecrets.org/federal-lobbying/bills/summary?id=hr6580-117) |
| SAFE Innovation Framework | US Federal | 2024 | Regulatory sandboxes for AI development | Jurisdictional conflicts, partisan disagreement | Stalled in committee | Moderate |
| AI Labeling Act | US Federal | 2024 | Mandatory disclosure for AI-generated content | Industry lobbying, definitional challenges | Stalled in committee | [Meta opposition](https://www.axios.com/2024/05/15/ai-labeling-deepfakes-congress) |
| National AI Commission Act | US Federal | 2023-2024 | Independent AI oversight body | Diluted to advisory function, opposition to new authorities | Weakened/Stalled | High |
| UN AI Treaty | International | 2024 | Binding international AI governance | US-China competition, verification challenges, sovereignty concerns | No progress | N/A |

## How It Works: The Systematic Failure Mechanisms

AI policy proposals fail through predictable mechanisms that operate across multiple levels simultaneously. Understanding these patterns reveals why even popular initiatives with broad support consistently stall or collapse.

### The Industry Opposition Architecture

Technology companies deploy sophisticated multi-layered strategies that go far beyond traditional lobbying. The [577% increase in OpenAI's lobbying spend to \$1.76 million in 2024](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/)[^3] exemplifies how AI companies have rapidly professionalized their government relations operations. [Anthropic more than doubled its spending to \$720,000](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/)[^3], while established tech giants maintained their dominance with [Meta spending \$6.5 million in Q4 2025 alone](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4)[^6].

The opposition operates through three primary channels:

**Information Asymmetry Exploitation**: Companies hire former government officials and establish dedicated government relations teams, creating superior technical knowledge compared to legislative staff. This allows industry representatives to shape debates around implementation feasibility and unintended consequences, often successfully arguing that proposed regulations are technically infeasible or would harm innovation.

**Narrative Warfare**: The "innovation flight" argument warns that strict regulation will drive AI development to more permissive jurisdictions, resonating particularly with policymakers in technology-dependent regions like California. Companies increasingly position AI as pivotal to national security competition with China, transforming regulatory discussions into debates about American competitiveness.

**Coalition Building and Astroturfing**: Industry-funded organizations present themselves as grassroots voices while obscuring corporate funding sources. This creates an echo chamber where manufactured opposition appears to represent diverse stakeholder concerns, making it difficult for policymakers to assess genuine public opinion.

### The Definitional Trap

Failed proposals consistently founder on the challenge of defining "artificial intelligence" in legally precise terms. Traditional legal frameworks assume clear categorical boundaries, but AI exists on a capability spectrum that resists binary classification. <EntityLink id="california-sb1047">SB 1047's</EntityLink> reliance on <EntityLink id="computational-thresholds">compute thresholds</EntityLink> and cost metrics (\$100 million in compute) created apparent precision but masked underlying questions about what constitutes a "frontier" AI model.

The dual-use nature of AI technology further complicates regulatory design. Unlike nuclear technology where weapons applications are clearly distinguishable from civilian uses, AI systems designed for legitimate purposes can be adapted for malicious ends with minimal modification. This reality makes it difficult to craft regulations that prevent harm without stifling beneficial innovation.

### Speed Mismatch and Jurisdictional Complexity

The fundamental mismatch between technological development speed and legislative cycles creates persistent regulatory lag. By the time comprehensive legislation addresses current AI systems, the technology has evolved in ways that make existing frameworks obsolete. This dynamic is exacerbated by jurisdictional fragmentation, where the Federal Trade Commission, Securities and Exchange Commission, Food and Drug Administration, and various other agencies claim overlapping authority.

[State versus federal tensions proved particularly problematic for SB 1047](https://www.gov.ca.gov/wp-content/uploads/2024/09/SB-1047-Veto-Message.pdf)[^7], where federal preemption arguments assumed uniform national standards are preferable to regulatory experimentation, but federal inaction left this assumption untested. The resulting stalemate benefits companies that prefer regulatory uncertainty to clear but demanding requirements.

## Major Failed Initiatives

### California SB 1047: The Comprehensive Framework Collapse

California's Safe and Secure Innovation for Frontier Artificial Intelligence Models Act represented the most ambitious state-level AI regulation attempted in the United States. Introduced by Senator Scott Wiener in February 2024, the bill established safety testing requirements for AI models trained with more than \$100 million in compute resources, required developers to implement shutdown capabilities and conduct red-team evaluations before deployment, and created potential <EntityLink id="liability-frameworks">liability for developers</EntityLink> whose models caused critical harms defined as mass casualties, critical infrastructure damage, or economic losses exceeding \$500 million.

[Governor Newsom vetoed the bill on September 29, 2024](https://www.gov.ca.gov/wp-content/uploads/2024/09/SB-1047-Veto-Message.pdf)[^7], stating that "SB 1047 does not take into account whether an AI system is deployed in high-risk environments, involves critical decision-making or the use of sensitive data." The veto came despite the bill passing both houses of the California legislature with bipartisan support, demonstrating that concerns about AI risks transcend traditional partisan divides.

The opposition coalition proved formidable and unusual. [Meta, OpenAI, and House Speaker Nancy Pelosi opposed the bill](https://www.politico.com/news/2024/08/22/pelosi-california-ai-regulation-00175510)[^8], with [Members of California's congressional delegation—including Speaker Emerita Pelosi, Representatives Ro Khanna, Anna Eschoo, Zoe Lofgren, and Jay Obernolte—publicly urging the Governor to reject it](https://khanna.house.gov/media/press-releases/release-khanna-leads-california-congressional-delegation-letter-governor-newsom)[^9]. Meanwhile, the <EntityLink id="cais">Center for AI Safety</EntityLink>, <EntityLink id="elon-musk">Elon Musk</EntityLink>, and the L.A. Times editorial board supported the bill, revealing unusual cross-cutting alliances on AI safety.

The [veto message highlighted several critical vulnerabilities](https://www.gov.ca.gov/wp-content/uploads/2024/09/SB-1047-Veto-Message.pdf)[^7] in state-level AI regulation. Newsom cited concerns that the bill's focus on high-cost, large-scale models would provide a "false sense of security," emphasizing that smaller, specialized models could pose equally significant risks. The definitional challenges proved decisive: the bill's reliance on <EntityLink id="computational-thresholds">compute thresholds</EntityLink> created apparent precision but could not anticipate future technological developments or distinguish between beneficial and harmful applications.

However, California's approach evolved strategically. [California enacted 18 AI-related laws in 2024](https://cset.georgetown.edu/article/californias-approach-to-ai-governance/)[^10], and [SB 53 emerged as a "pared-down version of SB 1047" focusing on transparency rather than strict safety requirements](https://cset.georgetown.edu/article/californias-approach-to-ai-governance/)[^10]. This evolution from comprehensive framework failure to incremental transparency success illustrates how policy approaches can adapt to overcome initial resistance.

### Federal Legislative Gridlock: The 150-Bill Failure

The 118th Congress saw unprecedented introduction of AI-related legislation, with [over 150 bills addressing various aspects of artificial intelligence governance, yet none passed into law](https://cset.georgetown.edu/publication/tracker-congressional-activity-on-artificial-intelligence/)[^1]. This represents a complete failure of the federal legislative process to address AI governance despite widespread bipartisan concern about AI risks.

Major failed proposals included the [Algorithmic Accountability Act](https://www.congress.gov/bill/118th-congress/house-bill/5628)[^11], reintroduced for the third time by Senators Ron Wyden and Cory Booker and Rep. Yvette Clarke, which would have required automated decision system impact assessments for large companies. The bill consistently failed to exit committee despite growing evidence of <EntityLink id="algorithmic-bias">algorithmic bias</EntityLink> in hiring, lending, and criminal justice applications.

The [AI LEAD Act introduced by Senators Durbin and Hawley](https://www.transparencycoalition.ai/news/as-congress-enters-final-2025-session-these-are-the-3-ai-bills-worth-watching-and-supporting)[^12] and the [GUARD Act of 2025 introduced by Senators Hawley and Blumenthal addressing AI chatbot safety for minors](https://www.transparencycoalition.ai/news/as-congress-enters-final-2025-session-these-are-the-3-ai-bills-worth-watching-and-supporting)[^12] represent the latest attempts at federal AI legislation, but face similar obstacles to their predecessors.

Congressional dysfunction played a significant role in these failures, but industry opposition proved decisive. The vacuum drove state-level action: [nearly 700 AI-related state bills were introduced in 2024](https://fpf.org/blog/the-state-of-state-ai-legislative-approaches-to-ai-in-2025/)[^13], prompting a federal counter-reaction. The House attempted to include a [10-year moratorium on state and local AI laws in budget reconciliation](https://www.congress.gov/bill/118th-congress/house-bill/815)[^14], which was [stripped by a near-unanimous 99-1 Senate vote](https://www.senate.gov/legislative/LIS/roll_call_votes/vote1182/vote_118_2_00234.htm)[^15].

The systematic failure pattern reveals how industry influence shapes legislative outcomes. [Combined AI company lobbying reached \$2.71 million in 2024, four times their 2023 spending](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/)[^3], while the narrative positioning AI as essential to national security competition with China proved especially potent in preventing regulatory constraints.

### Trump Administration's Regulatory Reversal

[President Trump's Executive Order "Ensuring a National Policy Framework for Artificial Intelligence" issued December 11, 2025](https://www.mayerbrown.com/en/insights/publications/2025/12/president-trump-issues-executive-order-on-ensuring-a-national-policy-framework-for-artificial-intelligence)[^16], represents a systematic reversal of previous AI safety initiatives. The order seeks to advance "a minimally burdensome national policy framework" for AI and tasks agencies with preparing legislative recommendations for a federal AI framework that would preempt state laws.

This approach reflects the broader pattern where industry-preferred minimal regulation gains political traction by framing comprehensive oversight as economically harmful. [Trump's January 2025 Executive Order "Removing Barriers to American Leadership in Artificial Intelligence" focused on revoking directives perceived as restrictive](https://www.softwareimprovementgroup.com/blog/us-ai-legislation-overview/)[^17], demonstrating how political transitions can eliminate even modest regulatory progress.

### International Treaty Failures: Great Power Competition Blocks Coordination

International efforts to establish binding AI governance agreements have consistently failed despite widespread recognition that AI's global nature requires coordinated responses. The [UN Secretary-General's High-level Advisory Body on AI released its final report "Governing AI for Humanity" in September 2024](https://www.un.org/sites/un2.un.org/files/ai_advisory_body_governing_ai_for_humanity_final_report_en.pdf)[^18], recommending seven initiatives including an international scientific panel and global AI data framework, but no binding treaty or enforcement mechanism has emerged.

[Great power competition, particularly US-China tensions](https://www.whitehouse.gov/briefing-room/statements-releases/2024/09/24/fact-sheet-biden-harris-administration-announces-key-ai-actions-following-president-bidens-commitment-to-ai-governance-at-the-united-nations/)[^19], represents the primary obstacle to international AI agreements. The US explicitly rejected "centralized control and global governance" of AI at UN discussions, while both superpowers view AI capabilities as strategic assets essential to military and economic competitiveness.

Verification challenges compound these political obstacles. Unlike nuclear or chemical weapons, AI capabilities are largely software-based and can be rapidly modified or concealed. International monitoring would require unprecedented access to corporate research facilities and source code, raising both security and intellectual property concerns that make comprehensive treaties practically infeasible.

The [Paris AI Action Summit in February 2025 called for harmonised global standards](https://blog.worldsummit.ai/global-ai-governance-in-2025)[^20], but achieved no binding commitments. The [EU AI Act's entry into force with first provisions becoming applicable on February 2, 2025](https://oliverpatel.substack.com/p/ai-governance-in-2025-a-year-in-review)[^21], represents the most significant international AI governance development, but remains a regional rather than global framework.

## Systematic Failure Patterns

<Mermaid chart={`
flowchart TD
    subgraph Proposal["Regulatory Proposal Introduced"]
        A[Comprehensive AI Bill]
    end

    subgraph Opposition["Industry Response"]
        B[Lobbying Mobilization<br/>648 companies, +141% YoY]
        C[Innovation Flight<br/>Narrative]
        D[National Security<br/>Competition Argument]
        E[Technical Complexity<br/>Claims]
    end

    subgraph Barriers["Structural Barriers"]
        F[Definitional<br/>Challenges]
        G[Jurisdictional<br/>Conflicts]
        H[Speed Mismatch:<br/>Tech vs Legislation]
        I[Partisan<br/>Divisions]
    end

    subgraph PublicSupport["97% Public Support"]
        J[Safety Rules<br/>Gallup 2025]
        K[More Regulation<br/>72% Americans]
    end

    subgraph Outcomes["Typical Outcomes"]
        L[Stalled in Committee<br/>150+ bills, 0 passed]
        M[Weakened/Diluted]
        N[Executive Veto<br/>SB 1047]
        O[Voluntary Framework<br/>Substitute]
    end

    A --> B
    B --> C & D & E
    C --> L
    D --> M
    E --> F
    A --> F & G & H & I
    F & G --> L
    H & I --> M
    J & K --> A
    L & M & N --> O

    style A fill:#e74c3c,color:#fff
    style B fill:#f39c12,color:#fff
    style J fill:#27ae60,color:#fff
    style K fill:#27ae60,color:#fff
    style O fill:#3498db,color:#fff
`} />

### AI Industry Lobbying Evolution (2023-2025)

| Company/Organization | 2023 Spend | 2024 Spend | 2025 Q4 | YoY Change | Key Regulatory Targets |
|---------------------|:----------:|:----------:|:-------:|:----------:|----------------------|
| OpenAI | [\$260K](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/) | [\$1.76M](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/) | [\$890K](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4) | +577% | Safety testing, liability limits |
| Anthropic | [\$280K](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/) | [\$720K](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/) | — | +157% | Constitutional AI frameworks |
| Cohere | [\$40K](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/) | [\$130K](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/) | — | +225% | Enterprise AI regulation |
| Meta | [\$5.1M](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4) | — | [\$6.5M](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4) | +27% | Content moderation, SB 1047 |
| Google/Alphabet | — | — | [\$3.4M](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4) | — | Search regulation, AI Act |
| Amazon | — | — | [\$4.6M](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4) | — | Cloud AI services |
| Nvidia | [\$950K](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4) | — | [\$1.4M](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4) | -26% | Export controls, compute access |
| **AI Companies Total** | **\$680K** | **\$2.71M** | — | **+299%** | **Comprehensive frameworks** |
| **Total AI Lobbyists** | [458](https://www.opensecrets.org/federal-lobbying/issues/summary?cycle=2023&id=AIS) | [648](https://www.opensecrets.org/federal-lobbying/issues/summary?cycle=2024&id=AIS) | — | **+141%** | **All AI policy areas** |

### The Regulatory Capture Pattern

Analysis of failed proposals reveals sophisticated industry strategies that create asymmetric influence over policy outcomes. The dramatic increase in lobbying expenditure—with AI companies spending [four times their 2023 levels by 2024](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/)[^3]—represents rapid professionalization of government relations operations designed to shape regulatory conversations before formal proposals emerge.

<EntityLink id="regulatory-capture">Regulatory capture</EntityLink> operates through multiple mechanisms beyond direct lobbying. [Academic research on AI safety and regulatory capture published in 2025](https://link.springer.com/article/10.1007/s00146-025-02534-0)[^22] identifies how organizations with economic power use regulations to unjustly enrich themselves, while [empirical studies examine how AI companies influence policy through agenda-setting, advocacy, and academic capture](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4931927)[^23].

The "innovation flight" narrative proves particularly effective at the state level, warning that strict regulation will drive development to more permissive jurisdictions. However, empirical evidence for this claim remains limited—financial services and pharmaceutical companies continue investing heavily in highly regulated jurisdictions when market opportunities justify compliance costs. The difference may be that AI companies face fewer location constraints than traditional industries, making the threat more credible.

### Public Opinion Disconnect

The disconnect between public support for AI regulation and policy outcomes reveals systematic democratic representation failures. [Gallup polling from May 2025 found 97% of Americans agree AI safety should be subject to rules and regulations](https://news.gallup.com/poll/694685/americans-prioritize-safety-data-security.aspx)[^4], with [majority-level support across all key subgroups including 88% of Democrats and 79% of Republicans and independents](https://news.gallup.com/poll/694685/americans-prioritize-safety-data-security.aspx)[^4].

[Public Citizen analysis shows consistent polling trends with 72% of Americans wanting more regulation of the AI industry—a 15-point increase from the prior year](https://www.citizen.org/article/years-of-polling-show-overwhelming-voter-support-for-a-crackdown-on-ai/)[^5]. Additionally, [76% of voters prefer candidates who support AI regulation, including 71% of Republicans](https://www.citizen.org/article/years-of-polling-show-overwhelming-voter-support-for-a-crackdown-on-ai/)[^5], while [77% of voters, including 75% of Republicans, say government should do more to regulate AI](https://www.citizen.org/article/years-of-polling-show-overwhelming-voter-support-for-a-crackdown-on-ai/)[^5].

This overwhelming public support creates political opportunities for AI regulation, but concentrated industry opposition and information asymmetries prevent these preferences from translating into policy outcomes. The pattern suggests that successful AI governance may require mobilizing diffuse public preferences into focused political pressure comparable to industry lobbying efforts.

### State-Level Innovation and Federal Preemption

State governments have emerged as the primary venue for AI governance innovation, with [frontier model regulation reintroduced in California and New York, with similar bills in Rhode Island, Michigan, and Illinois](https://fpf.org/blog/the-state-of-state-ai-legislative-approaches-to-ai-in-2025/)[^13]. However, [unlike 2024 when Colorado enacted broad AI regulation, 2025 saw a shift toward fewer governance requirements and more transparency measures](https://fpf.org/blog/the-state-of-state-ai-legislative-approaches-to-ai-in-2025/)[^13].

This evolution from comprehensive frameworks to transparency-focused approaches reflects lessons learned from SB 1047's failure. [California's approach of enacting 18 AI-related laws in 2024](https://cset.georgetown.edu/article/californias-approach-to-ai-governance/)[^10] demonstrates how incremental sectoral regulation can achieve passage where comprehensive frameworks fail.

Federal preemption arguments remain a significant challenge for ambitious state initiatives. [Trump's executive order tasks agencies with preparing legislative recommendations for federal AI framework](https://www.mayerbrown.com/en/insights/publications/2025/12/president-trump-issues-executive-order-on-ensuring-a-national-policy-framework-for-artificial-intelligence)[^16] that would explicitly preempt state laws, creating pressure for state action before federal override occurs.

## Successful Approaches and Lessons Learned

### Regulatory Approach Success Rates by Type

| Approach | Examples | Success Rate | Industry Opposition Level | Key Success Factors |
|----------|----------|:------------:|:------------------------:|-------------------|
| **Comprehensive Frameworks** | [SB 1047](https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1047), [Algorithmic Accountability Act](https://www.congress.gov/bill/118th-congress/house-bill/5628) | 5% | Very High (\$10M+ lobbying) | Requires crisis catalyst |
| **Sectoral/Use-Case** | [Colorado AI Act](https://leg.colorado.gov/bills/sb24-205), AI in hiring laws | 35% | Moderate | Narrow scope, clear harms |
| **Transparency/Disclosure** | [California AB 2013](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB2013), AI labeling | 60% | Low-Moderate | No liability, information-focused |
| **Executive Actions** | [Biden AI EO](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) | 90% (within admin) | Low | Uses existing authority |
| **Voluntary Standards** | [G7 Hiroshima Process](https://www.g7hiroshima.go.jp/en/), [NIST AI RMF](https://www.nist.gov/itl/ai-risk-management-framework) | 95% (adoption) | Minimal | Industry-friendly, non-binding |
| **International Treaties** | UN binding AI frameworks | 0% | N/A | Great power competition |

*Success rates based on passage/implementation data from 2019-2025 policy initiatives*

### Incremental Progress Models

The most successful AI governance initiatives build incrementally rather than attempting comprehensive solutions. [California's evolution from SB 1047's veto to SB 53's passage in 2025](https://cset.georgetown.edu/article/californias-approach-to-ai-governance/)[^10] demonstrates how transparency-focused approaches can succeed where liability-based frameworks fail. This pattern suggests that information-based interventions serve as necessary precursors to more substantive regulatory obligations.

Executive actions prove highly effective within their scope limitations. The [Biden AI Executive Order of October 2023](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/)[^24] succeeded by building on existing regulatory authorities rather than creating new ones, while narrow sectoral regulations addressing specific applications (like AI in hiring decisions) face less systematic opposition than broad technology mandates.

Disclosure requirements consistently achieve higher passage rates than performance mandates or liability provisions. Requirements for algorithmic transparency or AI-generated content labeling generate less industry opposition than rules imposing legal responsibility for harmful outcomes, suggesting that procedural obligations face fewer barriers than substantive constraints.

### Learning from Successful Technology Regulation

Analysis of successful technology regulation in other domains provides insights for AI governance approaches. The [EU's GDPR serves as "the most comprehensive data protection regulation to date" and "arguably the first ambitious regulation specifically targeted at regulating the digital economy"](https://www.tandfonline.com/doi/full/10.1080/13662716.2023.2271858)[^25]. However, [studies of GDPR's economic impact show initial compliance costs for Fortune Global 500 companies amounted to \$7.8 billion](https://datainnovation.org/2022/04/a-new-study-lays-bare-the-cost-of-the-gdpr-to-europes-economy-will-the-ai-act-repeat-history/)[^26], while [bureaucratically complex laws build barriers to entry and concentrate market share among large incumbents](https://datainnovation.org/2022/04/a-new-study-lays-bare-the-cost-of-the-gdpr-to-europes-economy-will-the-ai-act-repeat-history/)[^26].

The GDPR experience suggests both opportunities and risks for AI regulation. [A 2021 study showed persistent decline in venture investment deals for EU technology firms compared to US](https://www.aei.org/economics/gdpr-ai-and-regulatory-humility/)[^27], while [IT sector SMEs faced twice the profit hit compared to other industries](https://datainnovation.org/2022/04/a-new-study-lays-bare-the-cost-of-the-gdpr-to-europes-economy-will-the-ai-act-repeat-history/)[^26]. These outcomes warn about potential competitive disadvantages from complex regulatory frameworks, but also demonstrate that ambitious technology regulation can be implemented despite industry opposition.

The key difference may be timing and catalyzing events. GDPR emerged after major data breaches that shifted public opinion toward favoring privacy protection over industry concerns. AI regulation may require similar catalyzing events—significant AI-caused harms that overcome current industry opposition and political inertia.

### Building Effective Coalitions

Successful AI governance initiatives build broad coalitions that include both technology industry participants and civil rights advocates, rather than relying solely on safety-focused arguments. Bipartisan framing that emphasizes economic competitiveness, national security, and innovation leadership alongside safety concerns proves more durable than approaches that position regulation as primarily constraining industry.

The [percentage of companies providing board oversight disclosure increased by more than 84% year-over-year](https://corpgov.law.harvard.edu/2025/04/02/ai-in-focus-in-2025-boards-and-shareholders-set-their-sights-on-ai/)[^28], while [shareholder proposals related to AI more than quadrupled compared to 2023](https://corpgov.law.harvard.edu/2025/04/02/ai-in-focus-in-2025-boards-and-shareholders-set-their-sights-on-ai/)[^28]. This growing corporate governance pressure creates potential allies within industry for reasonable regulatory requirements.

Technical communities within industry can serve as allies for safety-focused regulation when their concerns align with external advocacy. AI safety researchers within major technology companies often share external researchers' concerns about risks, though their ability to influence corporate positions remains limited. Building relationships with these internal allies provides valuable intelligence about industry positions and can potentially moderate opposition to reasonable proposals.

### International Coordination Opportunities

International coordination faces continued challenges from great power competition, but technical cooperation offers pathways for progress. The [UK AI Safety Institute became the AI Security Institute](https://oliverpatel.substack.com/p/ai-governance-in-2025-a-year-in-review)[^21], while the [EU AI Act compliance became reality with obligations for general-purpose AI models becoming applicable on August 2, 2025](https://oliverpatel.substack.com/p/ai-governance-in-2025-a-year-in-review)[^21].

The EU approach of implementing binding regulation creates market pressure that extends beyond European jurisdiction. Companies developing AI systems must comply with EU requirements to access European markets, effectively globalizing European standards through market mechanisms rather than diplomatic agreements. This "Brussels Effect" may prove more effective than treaty-based coordination that requires consensus among competing nations.

## Limitations of Current Analysis

The analysis of failed AI policy proposals faces several important limitations that affect conclusions about systematic patterns and future trajectories:

**Selection Bias in Failure Cases**: The focus on failed and stalled proposals may overestimate the difficulty of AI governance by excluding successful but less visible initiatives. Regulatory achievements through executive actions, agency rulemaking, and incremental policy changes may create more governance progress than legislative failures suggest. The emphasis on high-profile comprehensive frameworks like SB 1047 may miss successful sectoral regulations that address specific AI applications without broad media attention.

**Temporal Limitations and Policy Lag**: Most analyzed failures occurred during 2019-2025, representing early-stage AI governance attempts when both technology capabilities and regulatory understanding were less mature. Policy cycles typically require multiple proposal iterations before achieving success, suggesting that current failures may represent normal early-stage policy development rather than permanent barriers to regulation. The time horizon may be insufficient to identify longer-term success patterns.

**Industry Opposition Measurement Challenges**: Lobbying expenditure data captures only reported influence activities, excluding informal relationships, revolving door appointments, and indirect influence through think tanks and academic institutions. The focus on monetary spending may underestimate industry influence exercised through information provision, technical expertise, and regulatory capture mechanisms that operate outside formal lobbying requirements.

**Jurisdictional and Definitional Scope**: The analysis primarily focuses on US federal and California state initiatives, potentially missing successful AI governance approaches in other jurisdictions that could provide different models. The definition of "AI policy" itself creates boundaries that may exclude relevant technology governance approaches in related domains like data protection, algorithmic accountability, or automated decision systems.

**Public Opinion Polling Limitations**: Survey data on AI regulation support may reflect general attitudes rather than preferences about specific policy mechanisms. Public support for "AI safety rules" in abstract may not translate to support for particular regulatory approaches once costs and tradeoffs become apparent. The disconnect between public opinion and policy outcomes may reflect normal democratic representation patterns rather than AI-specific governance failures.

**Counterfactual Analysis Gaps**: The analysis cannot definitively establish what would have happened under different conditions—whether failed proposals might have succeeded with different timing, framing, or coalition-building strategies. The identification of "systematic patterns" may reflect correlation rather than causation, where apparent failure mechanisms actually represent symptoms of deeper political or institutional challenges affecting technology governance generally.

## Current State and Future Trajectory

### 2025-2026 Political Dynamics

The immediate trajectory of AI governance is being shaped by significant political transitions and evolving industry strategies. [Trump's Executive Order promoting "a minimally burdensome national policy framework" for AI](https://www.mayerbrown.com/en/insights/publications/2025/12/president-trump-issues-executive-order-on-ensuring-a-national-policy-framework-for-artificial-intelligence)[^16] represents a systematic reversal of previous safety-focused initiatives, while [expectations that Congress will continue considering AI legislation that preempts state laws](https://www.mayerbrown.com/en/insights/publications/2025/12/president-trump-issues-executive-order-on-ensuring-a-national-policy-framework-for-artificial-intelligence)[^16] creates pressure for state action before federal override.

State-level initiatives show signs of strategic adaptation. The [shift from California's vetoed SB 1047 to transparency-focused SB 53 in 2025](https://cset.georgetown.edu/article/californias-approach-to-ai-governance/)[^10] demonstrates how policy approaches evolve to overcome initial resistance. [Multiple states following suit with frontier model regulation in New York, Rhode Island, Michigan, and Illinois](https://fpf.org/blog/the-state-of-state-ai-legislative-approaches-to-ai-in-2025/)[^13] suggests that regulatory experimentation continues despite federal preemption threats.

Industry positions show tactical evolution rather than fundamental change. While some AI companies acknowledge the inevitability of regulation and seek to influence its form, [the pullback in Q4 lobbying spending with Nvidia dropping from \$1.9M to \$1.4M and OpenAI from \$920K to \$890K](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4)[^6] may reflect confidence that the new administration will pursue industry-favorable policies rather than reduced engagement with policy processes.

### International Governance Evolution

International AI governance is proceeding along parallel tracks with limited coordination. The [EU AI Act's implementation starting February 2, 2025, with obligations for general-purpose AI models becoming applicable on August 2, 2025](https://oliverpatel.substack.com/p/ai-governance-in-2025-a-year-in-review)[^21], creates the world's first comprehensive binding AI regulation framework. This "Brussels Effect" may prove more influential than treaty-based coordination by establishing market standards that companies must meet to access European markets.

The [Paris AI Action Summit's call for harmonised global standards](https://blog.worldsummit.ai/global-ai-governance-in-2025)[^20] represents continued diplomatic engagement, but without binding commitments from major powers. The [UK AI Safety Institute's evolution to the AI Security Institute](https://oliverpatel.substack.com/p/ai-governance-in-2025-a-year-in-review)[^21] demonstrates how technical cooperation can continue despite political disagreements about comprehensive governance frameworks.

US-China competition continues to block multilateral treaty approaches, but may create opportunities for bilateral technical cooperation on shared risks like critical infrastructure protection or financial system stability. The recognition that AI risks transcend national boundaries may eventually overcome sovereignty concerns, but likely requires demonstrated harms that create compelling incentives for coordination.

### Technology Development and Governance Adaptation

The rapid evolution of AI capabilities continues to outpace regulatory responses, but may also create new political dynamics that favor governance. If AI systems begin causing significant economic disruption or safety incidents, public pressure for regulation could overcome current industry opposition. The [97% public support for AI safety rules](https://news.gallup.com/poll/694685/americans-prioritize-safety-data-security.aspx)[^4] provides a foundation for policy change if catalyzing events mobilize this latent support.

Technical developments in AI safety and interpretability may also reduce industry opposition to governance requirements. As safety evaluation methods improve and compliance costs decline, regulatory requirements may become more feasible and less economically burdensome. The development of technical standards through organizations like <EntityLink id="nist-framework">NIST</EntityLink> provides foundations for future mandatory requirements while building consensus around best practices.

The emergence of more capable AI systems may also shift industry incentives toward supporting regulation as a competitive moat. If safety requirements become expensive to implement, established companies with resources to ensure compliance may favor regulation that creates barriers to entry for competitors. This dynamic has historical precedents in industries like finance and pharmaceuticals, where regulatory compliance creates competitive advantages for large incumbents.

### Long-term Institutional Evolution

The systematic failure of traditional legislative approaches to AI governance may drive institutional innovation in democratic decision-making. The speed and technical complexity of AI development may exceed the capacity of traditional legislative deliberation, potentially requiring new hybrid institutions that combine democratic accountability with technical expertise and adaptive capacity.

Regulatory sandboxes, algorithmic audit requirements, and adaptive governance frameworks represent potential alternatives to traditional rule-making processes. These approaches may allow more flexible responses to technological change while maintaining democratic oversight. However, such institutional innovations face their own political obstacles and may require constitutional changes in some jurisdictions.

The relationship between private industry and public governance may also evolve toward new forms of collaboration or oversight. Public-private partnerships, industry self-regulatory organizations with government backstops, and hybrid governance institutions represent potential alternatives to traditional adversarial regulatory relationships. The effectiveness of such approaches depends on designing accountability mechanisms that prevent <EntityLink id="regulatory-capture">industry capture</EntityLink> while enabling efficient coordination.

<Mermaid chart={`
flowchart LR
    subgraph Current["Current State 2025"]
        A[97% Public Support<br/>for AI Safety Rules]
        B[Federal Legislative<br/>Gridlock: 150+ Bills, 0 Laws]
        C[Industry Opposition:<br/>\$2.71M AI Lobbying]
        D[State Innovation:<br/>18 CA Laws, SB 53 Success]
    end

    subgraph Scenarios["Future Scenarios"]
        E[Catalyzing Event<br/>Pathway]
        F[Incremental Progress<br/>Pathway]
        G[Industry Capture<br/>Pathway]
        H[International Pressure<br/>Pathway]
    end

    subgraph Outcomes["Potential Outcomes"]
        I[Comprehensive<br/>US Federal Regulation]
        J[Sectoral/State<br/>Patchwork]
        K[Voluntary Industry<br/>Self-Regulation]
        L[EU Standards<br/>Global Adoption]
    end

    A --> E
    B --> F & G
    C --> G & K
    D --> F & J
    
    E --> I
    F --> J
    G --> K
    H --> L

    style E fill:#e74c3c,color:#fff
    style F fill:#f39c12,color:#fff
    style G fill:#95a5a6,color:#fff
    style H fill:#3498db,color:#fff
`} />

The ultimate success or failure of AI governance may depend on whether democratic institutions can adapt to govern rapidly evolving technologies while preserving accountability and human agency. The pattern of current failures suggests that existing approaches are inadequate, but whether better alternatives will emerge before they become urgently necessary remains the critical uncertainty shaping humanity's relationship with artificial intelligence.

---

## AI Transition Model Context

Failed AI policy proposals reveal critical constraints on improving <EntityLink id="ai-transition-model">AI transition outcomes</EntityLink>:

| Factor | Parameter | Impact |
|--------|-----------|--------|
| <EntityLink id="civilizational-competence">Civilizational Competence</EntityLink> | <EntityLink id="regulatory-capacity">Regulatory Capacity</EntityLink> | 648 companies lobbying (up 141% YoY) systematically oppose comprehensive frameworks; 150+ bills with 0% passage rate |
| <EntityLink id="civilizational-competence">Civilizational Competence</EntityLink> | <EntityLink id="institutional-quality">Institutional Quality</EntityLink> | Speed mismatch between tech development and legislative cycles; jurisdictional fragmentation blocks coordination |
| <EntityLink id="transition-turbulence">Transition Turbulence</EntityLink> | <EntityLink id="racing-intensity">Racing Intensity</EntityLink> | Industry opposition mobilizes around liability provisions; voluntary frameworks preferred over binding requirements |

Pattern suggests governance capabilities lag significantly behind technological development, with <EntityLink id="incremental-regulation">incremental approaches</EntityLink> showing higher success rates (35-60%) than comprehensive frameworks (≈5%). The 97% public support for AI safety rules versus 0% comprehensive legislation passage rate indicates systematic <EntityLink id="democratic-representation">democratic representation</EntityLink> failures in technology governance.

[^1]: [Congressional Activity on Artificial Intelligence](https://cset.georgetown.edu/publication/tracker-congressional-activity-on-artificial-intelligence/) - Georgetown CSET, 2024
[^2]: [Federal Lobbying: Issues Summary](https://www.opensecrets.org/federal-lobbying/issues/summary?cycle=2024&id=AIS) - OpenSecrets, 2024
[^3]: [AI companies upped their federal lobbying spend in 2024](https://techcrunch.com/2025/01/24/ai-companies-upped-their-federal-lobbying-spend-in-2024-amid-regulatory-uncertainty/) - TechCrunch, January 24, 2025
[^4]: [Americans Prioritize AI Safety and Data Security](https://news.gallup.com/poll/694685/americans-prioritize-safety-data-security.aspx) - Gallup, May 2025
[^5]: [Years of Polling Show Overwhelming Voter Support for a Crackdown on AI](https://www.citizen.org/article/years-of-polling-show-overwhelming-voter-support-for-a-crackdown-on-ai/) - Public Citizen, 2024-2025
[^6]: [Meta tops Big Tech lobbying spending once again](https://www.axios.com/2026/01/21/meta-big-tech-lobbying-spending-q4) - Axios, January 21, 2026
[^7]: [SB 1047 Veto Message](https://www.gov.ca.gov/wp-content/uploads/2024/09/SB-1047-Veto-Message.pdf) - Governor Newsom, September 29, 2024
[^8]: [Pelosi opposes California AI regulation](https://www.politico.com/news/2024/08/22/pelosi-california-ai-regulation-00175510) - Politico, August 22, 2024
[^9]: [Khanna leads California congressional delegation letter](https://khanna.house.gov/media/press-releases/release-khanna-leads-california-congressional-delegation-letter-governor-newsom) - Rep. Khanna Press Release, 2024
[^10]: [California's Approach to AI Governance](https://cset.georgetown.edu/article/californias-approach-to-ai-governance/) - Georgetown CSET, 2024-2025
[^11]: [H.R.5628 - Algorithmic Accountability Act](https://www.congress.gov/bill/118th-congress/house-bill/5628) - Congress.gov, 2023
[^12]: [As Congress enters final 2025 session](https://www.transparencycoalition.ai/news/as-congress-enters-final-2025-session-these-are-the-3-ai-bills-worth-watching-and-supporting) - Transparency Coalition, 2025
[^13]: [The State of State AI: Legislative Approaches to AI in 2025](https://fpf.org/blog/the-state-of-state-ai-legislative-approaches-to-ai-in-2025/) - Future of Privacy Forum, 2025
[^14]: [H.R.815](https://www.congress.gov/bill/118th-congress/house-bill/815) - Congress.gov, 2024
[^15]: [Senate Vote 234](https://www.senate.gov/legislative/LIS/roll_call_votes/vote1182/vote_118_2_00234.htm) - US Senate, 2024
[^16]: [President Trump Issues Executive Order on AI](https://www.mayerbrown.com/en/insights/publications/2025/12/president-trump-issues-executive-order-on-ensuring-a-national-policy-framework-for-artificial-intelligence) - Mayer Brown, December 11, 2025
[^17]: [AI legislation in the US: A 2025 overview](https://www.softwareimprovementgroup.com/blog/us-ai-legislation-overview/) - Software Improvement Group, 2025
[^18]: [Governing AI for Humanity Final Report](https://www.un.org/sites/un2.un.org/files/ai_advisory_body_governing_ai_for_humanity_final_report_en.pdf) - UN High-level Advisory Body on AI, September 2024
[^19]: [Biden-Harris Administration AI Actions](https://www.whitehouse.gov/briefing-room/statements-releases/2024/09/24/fact-sheet-biden-harris-administration-announces-key-ai-actions-following-president-bidens-commitment-to-ai-governance-at-the-united-nations/) - White House, September 24, 2024
[^20]: [Global AI Governance in 2025](https://blog.worldsummit.ai/global-ai-governance-in-2025) - World AI Summit, 2025
[^21]: [AI Governance in 2025: A Year in Review](https://oliverpatel.substack.com/p/ai-governance-in-2025-a-year-in-review) - Oliver Patel, 2025
[^22]: [AI safety and regulatory capture](https://link.springer.com/article/10.1007/s00146-025-02534-0) - Springer Nature, 2025
[^23]: [How Do AI Companies 'Fine-Tune' Policy?](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4931927) - SSRN, 2024
[^24]: [Executive Order on AI](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) - White House, October 30, 2023
[^25]: [The Impact of the EU General Data Protection Regulation](https://www.tandfonline.com/doi/full/10.1080/13662716.2023.2271858) - Taylor & Francis, 2023
[^26]: [The Cost of GDPR to Europe's Economy](https://datainnovation.org/2022/04/a-new-study-lays-bare-the-cost-of-the-gdpr-to-europes-economy-will-the-ai-act-repeat-history/) - Data Innovation, April 2022
[^27]: [GDPR, AI, and Regulatory Humility](https://www.aei.org/economics/gdpr-ai-and-regulatory-humility/) - American Enterprise Institute, 2024
[^28]: [AI in Focus in 2025: Boards and Shareholders Set Their Sights on AI](https://corpgov.law.harvard.edu/2025/04/02/ai-in-focus-in-2025-boards-and-shareholders-set-their-sights-on-ai/) - Harvard Law School, April 2, 2025