---
numericId: E840
title: Knowledge Base
description: Comprehensive documentation of AI safety risks, responses, organizations, and key debates
sidebar:
  label: Overview
  order: 0
---
import {EntityLink} from '@components/wiki';


## Overview

The LongtermWiki Knowledge Base provides structured documentation of the AI safety landscape, covering risks, interventions, organizations, and key debates. Content is organized to help researchers, funders, and policymakers understand the current state of AI safety and make informed decisions about resource allocation.

## Content Categories

### <EntityLink id="scheming">Risks</EntityLink>
Documentation of potential failure modes and hazards from advanced AI systems, organized by type:
- **Accident Risks** - Unintended failures like scheming, <EntityLink id="deceptive-alignment">deceptive alignment</EntityLink>, <EntityLink id="mesa-optimization">mesa-optimization</EntityLink>
- **Misuse Risks** - Deliberate harmful applications like <EntityLink id="bioweapons">bioweapons</EntityLink>, <EntityLink id="cyberweapons">cyberweapons</EntityLink>, <EntityLink id="disinformation">disinformation</EntityLink>
- **Structural Risks** - Systemic issues like <EntityLink id="racing-dynamics">racing dynamics</EntityLink>, <EntityLink id="concentration-of-power">concentration of power</EntityLink>, <EntityLink id="lock-in">lock-in</EntityLink>
- **Epistemic Risks** - Threats to knowledge and truth like <EntityLink id="authentication-collapse">authentication collapse</EntityLink>, trust erosion

### <EntityLink id="interpretability">Responses</EntityLink>
Interventions and approaches to address AI risks:
- **Technical Alignment** - Interpretability, <EntityLink id="rlhf">RLHF</EntityLink>, <EntityLink id="constitutional-ai">constitutional AI</EntityLink>, <EntityLink id="ai-control">AI control</EntityLink>
- **Governance** - Compute governance, <EntityLink id="international-coordination">international coordination</EntityLink>, legislation
- **Institutional** - <EntityLink id="ai-safety-institutes">AI safety institutes</EntityLink>, standards bodies
- **Epistemic Tools** - <EntityLink id="prediction-markets">Prediction markets</EntityLink>, content authentication, <EntityLink id="coordination-tech">coordination technologies</EntityLink>

### <EntityLink id="carlsmith-six-premises">Models</EntityLink>
Analytical frameworks for understanding AI risk dynamics:
- **Framework Models** - Carlsmith's six premises, <EntityLink id="instrumental-convergence">instrumental convergence</EntityLink>
- **Risk Models** - Deceptive alignment decomposition, <EntityLink id="scheming">scheming</EntityLink> likelihood
- **Dynamics Models** - Racing dynamics impact, feedback loops
- **Societal Models** - Trust erosion, lock-in mechanisms

### <EntityLink id="openai">Organizations</EntityLink>
Profiles of key actors in AI development and safety:
- **AI Labs** - <EntityLink id="openai">OpenAI</EntityLink>, <EntityLink id="anthropic">Anthropic</EntityLink>, DeepMind, xAI
- **Safety Research Orgs** - <EntityLink id="miri">MIRI</EntityLink>, ARC, Redwood, <EntityLink id="apollo-research">Apollo Research</EntityLink>
- **Government Bodies** - US AISI, UK AISI

### <EntityLink id="paul-christiano">People</EntityLink>
Profiles of influential researchers and leaders in AI safety.

### <EntityLink id="agentic-ai">Capabilities</EntityLink>
Documentation of AI capability domains and their safety implications.

### <EntityLink id="is-ai-xrisk-real">Debates</EntityLink>
Structured analysis of key disagreements in the field.

### <EntityLink id="accident-risks">Cruxes</EntityLink>
Key uncertainties that drive disagreement and prioritization decisions.

## How to Use This Knowledge Base

1. **Exploring risks**: Start with the <EntityLink id="scheming">scheming</EntityLink> page for the most discussed risk, then browse related accident risks
2. **Understanding responses**: See <EntityLink id="interpretability">interpretability</EntityLink> for a well-documented technical approach
3. **Analytical depth**: The <EntityLink id="carlsmith-six-premises">Carlsmith six-premise model</EntityLink> provides a rigorous framework for AI risk estimation
4. **Browse everything**: Use the [Browse page](/browse/) to search and filter all entries

## Quality Indicators

Pages include quality and importance ratings:
- **Quality (0-100)**: How well-developed and accurate the content is
- **Importance (0-100)**: How significant the topic is for AI safety decisions

High-priority pages (quality < importance) are actively being improved.
