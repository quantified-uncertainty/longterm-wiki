---
title: "Community Building Organizations (Overview)"
description: Organizations building intellectual communities, hosting events, and creating infrastructure for the effective altruism and rationality communities that underpin much of the AI safety field.
sidebar:
  label: Overview
  order: 0
subcategory: community-building
---
import {EntityLink} from '@components/wiki';

## Overview

The AI safety field is embedded within and draws heavily from the effective altruism (EA) and rationality communities. Several organizations provide community infrastructure—forums, conferences, training programs, and physical spaces—that facilitate the intellectual exchange and talent development essential to AI safety work.

## Key Organizations

| Organization | Type | Key Activities |
|---|---|---|
| **<EntityLink id="E517">Centre for Effective Altruism (CEA)</EntityLink>** | Community hub | EA Global conferences, community building grants, online forum |
| **<EntityLink id="E538">LessWrong</EntityLink>** | Online forum | Rationality and AI alignment discussion platform; hosts the Alignment Forum |
| **<EntityLink id="E539">Lighthaven</EntityLink>** | Event venue | Conference center in Berkeley hosting EA and rationality events |
| **<EntityLink id="E545">Manifest</EntityLink>** | Conference | Annual prediction market and forecasting conference |
| **<EntityLink id="E525">EA Global</EntityLink>** | Conference series | Flagship EA conference series held in multiple cities annually |
| **<EntityLink id="E518">Center for Applied Rationality (CFAR)</EntityLink>** | Training | Workshops teaching applied rationality and decision-making skills |
| **<EntityLink id="E534">Gratified</EntityLink>** | Platform | Community engagement and gratitude platform |
| **<EntityLink id="E570">The Sequences</EntityLink>** | Writing collection | Eliezer Yudkowsky's foundational essays on rationality and AI risk |

## Role in AI Safety

These organizations contribute to AI safety through several mechanisms:

1. **Talent pipeline**: EA Global, LessWrong, and CFAR workshops expose people to AI safety ideas and recruit talent into the field
2. **Intellectual infrastructure**: LessWrong and the Alignment Forum host much of the public technical discussion on alignment research
3. **Coordination**: Conferences and physical spaces (Lighthaven) enable face-to-face coordination among researchers and funders
4. **Epistemic norms**: The rationality community emphasizes calibrated beliefs, intellectual honesty, and quantitative reasoning—norms that influence AI safety culture

## Key Dynamics

**Berkeley concentration**: Much of the community infrastructure is physically concentrated in the San Francisco Bay Area, particularly Berkeley (Lighthaven, CFAR, MIRI). This creates benefits for in-person collaboration but also insularity risks.

**EA-safety relationship**: The 2022 FTX collapse disrupted EA community funding and raised questions about community governance. The AI safety field has since become somewhat more independent from the broader EA movement, though institutional connections remain strong.
