---
title: "OpenAI"
description: "Leading AI lab that developed GPT models and ChatGPT, analyzing organizational evolution from non-profit research to commercial AGI development amid safety-commercialization tensions"
sidebar:
  order: 2
entityType: "organization"
subcategory: labs
quality: 62
readerImportance: 72.4
researchImportance: 44.5
tacticalValue: 92
lastEdited: "2026-02-20"
update_frequency: 3
llmSummary: "Comprehensive organizational profile of OpenAI documenting evolution from 2015 non-profit to Public Benefit Corporation, with detailed analysis of governance crisis, 2024-2025 ownership restructuring (conversion from capped-profit LLC to PBC, with specific post-conversion equity percentages subject to regulatory finalization), key leadership departures, and capability advancement (o1/o3 reasoning models). Updated with 2025 developments including o3-mini release, 800M weekly active users, and Altman's AGI timeline statements."
ratings:
  focus: 7.2
  novelty: 3.5
  rigor: 5.8
  completeness: 7.5
  concreteness: 7.8
  actionability: 4.5
  objectivity: 6.5
clusters:
  - "ai-safety"
  - "community"
  - "governance"
---
import {DataInfoBox, DisagreementMap, KeyPeople, KeyQuestions, Section, R, EntityLink, DataExternalLinks} from '@components/wiki';

<DataExternalLinks pageId="openai" />

<DataInfoBox entityId="E218" />

## Overview

OpenAI is the AI research company that catalyzed mainstream artificial intelligence adoption through ChatGPT and the GPT model series. Founded in 2015 as a non-profit with the mission to ensure AGI benefits humanity, OpenAI has undergone significant organizational evolution: from open research lab to commercial entity, and from a non-profit governance structure to a Public Benefit Corporation pursuing stated AGI development goals.

The company achieved capability advances through massive scale (<F e="openai" f="7c9b9073">175B parameters</F> for GPT-3), pioneered <EntityLink id="rlhf">Reinforcement Learning from Human Feedback</EntityLink> as a practical alignment technique, and launched ChatGPT—reaching 800 million weekly active users by October 2025[^1] and maintaining 81.13% market share in generative AI[^2]. OpenAI's trajectory has involved ongoing tensions between commercial pressures and safety priorities, exemplified by the November 2023 board crisis that temporarily ousted CEO <EntityLink id="sam-altman">Sam Altman</EntityLink> and the 2024 departures of key safety researchers including co-founder <EntityLink id="ilya-sutskever">Ilya Sutskever</EntityLink>.

With over \$13 billion in <EntityLink id="microsoft">Microsoft</EntityLink> investment and capability advancement through reasoning models like o1 and the recent o3-mini release[^3], OpenAI sits at the center of debates about AI safety governance, racing dynamics, and whether commercial incentives can align with existential risk mitigation. In 2024–2025, the company undertook a formal legal restructuring from a capped-profit LLC to a Public Benefit Corporation (PBC), a transition with significant implications for ownership, governance, and mission accountability.

## Ownership Structure

OpenAI's 2024–2025 conversion from a capped-profit LLC to a Public Benefit Corporation (PBC) substantially altered how equity and economic interests are distributed among stakeholders. Under the previous structure, Microsoft held approximately 49% of capped profits in exchange for its multi-billion-dollar investment, while the non-profit board nominally controlled the organization's mission. The PBC restructuring replaced profit-share arrangements with direct equity stakes and introduced new stakeholder claims—most notably, a proposed equity grant to CEO Sam Altman, who held no equity under the original charter. The restructuring requires approval from the California Attorney General, given OpenAI's non-profit origins and California incorporation, and specific post-conversion equity percentages remain subject to regulatory finalization.

| Stakeholder | Interest Type | Notes |
|-------------|--------------|-------|
| **OpenAI Non-Profit Foundation** | Direct equity stake (post-PBC conversion) | Retains an equity stake to fund charitable mission; exact percentage and governance rights over equity management subject to regulatory review |
| **Microsoft** | Transitioning from capped-profit share to direct equity | Original structure gave Microsoft 49% of profits up to a return cap; PBC restructuring converts this to a direct equity stake. Final terms subject to ongoing negotiation and regulatory approval |
| **Sam Altman** | No equity under original charter; proposed equity grant under discussion | Under OpenAI's original non-profit charter, Altman received no equity. A proposed equity grant is reported to be part of restructuring negotiations; terms not finalized |
| **October 2024 funding round investors** | Equity (primary round) | October 2024 funding round led by Thrive Capital at <F e="openai" f="6ce1d805">\$157B valuation</F>; Tiger Global, Khosla Ventures among reported participants |

**Valuation context:** OpenAI's October 2024 funding round valued the company at <F e="openai" f="6ce1d805">\$157B</F>. Secondary market activity has suggested higher valuations in subsequent months, though no primary funding round had confirmed those figures as of early 2025.

**Regulatory context:** The restructuring requires approval from the California Attorney General, given OpenAI's non-profit origins. Legal challenges to the conversion have been filed by outside parties; the status of those proceedings remained ongoing as of early 2025.

## Recent Developments (2024-2025)

### Capability Advances

| Model | Release Date | Key Capabilities | Performance | Strategic Impact |
|-------|--------------|------------------|-------------|------------------|
| **o1 (December 2024)** | December 2024 | Full reasoning model release | Advanced mathematical/scientific reasoning | Demonstrated test-time compute scaling |
| **o3-mini** | January 31, 2025 | Latest reasoning model | More efficient reasoning capabilities[^5] | Broader reasoning model availability |
| **Sora 2** | 2025 | Video generation | Enhanced video creation[^6] | Multimodal generation |

### Market Dominance and Financial Performance

**User Growth and Market Position:**
- 800 million weekly active users as of October 2025 (doubled from 400M in February 2025)[^7]
- 15.5 million paying subscribers generating approximately \$3 billion annually[^8]
- Additional \$1 billion from API access[^9]
- Over 92% of Fortune 500 companies now use OpenAI products or APIs[^10]

**Developer Ecosystem Growth:**
- API business generates ≈\$41M monthly revenue from ≈530 billion tokens[^11]
- 10% monthly growth in API usage between December 2023 and June 2024[^12]
- GPT Store reached 3 million custom GPTs with 1,500 daily additions[^13]
- OpenAI's share of API-based AI infrastructure now exceeds 50%[^14]

### International Expansion Strategy

**OpenAI for Countries Initiative:**
- Launched partnership program with individual nations for data center capacity[^15]
- Focus on data sovereignty and local industry building
- 10 planned country-specific projects

**Asia-Pacific Growth:**
- APAC region shows highest user growth globally[^16]
- ChatGPT usage in APAC grew more than fourfold over 2024
- Regional offices established in Tokyo and Seoul from Singapore hub[^17]

## AGI Timeline and Leadership Confidence

### Sam Altman's 2025 Statements

In January 2025, CEO Sam Altman made notably confident statements about AGI development:

> "We are now confident we know how to build AGI as we have traditionally understood it... AGI will probably get developed during Trump's term."[^18]

**Key Claims:**
- AGI defined as AI capable of working as a remote software engineer[^19]
- "In 2025, we may see the first AI agents join the workforce"
- Capability to "materially change the output of companies"
- Acknowledgment that "AGI has become a very sloppy term"

**Context:**
- These statements are among the most specific public predictions Altman has made regarding AGI timelines
- Some observers interpret this as an acceleration relative to prior public statements; Altman has made optimistic statements at various points and his characterization of what "AGI" means has evolved over time
- The statements may influence competitive dynamics and regulatory responses
- Other industry voices have offered more cautious assessments of near-term AGI timelines

## Risk Assessment

The following risk assessments synthesize published views from safety researchers, academic commentators, and industry analysts; they do not represent the editorial position of this wiki. Trend labels reflect the direction of risk as assessed in cited safety literature, not an independent determination.

| Risk Category | Severity | Likelihood | Timeline | Trend (per safety literature) | Evidence |
|---------------|----------|------------|----------|-------------------------------|----------|
| **Capability-Safety Misalignment** | High | High | 1-2 years | Increasing concern | Safety team departures, Superalignment dissolution |
| **AGI Race Acceleration** | High | High | Immediate | Increasing concern | Confident AGI timeline statements, competitive pressure |
| **Governance Failure** | High | Medium | Ongoing | Stable | Nov 2023 crisis showed constraints on board authority |
| **Commercial Override of Safety** | High | High | 1-2 years | Increasing concern | Jan Leike: "Safety culture has taken backseat to shiny products"[^34] |
| **AGI Deployment Without Alignment** | Very High | Medium | 2-3 years | Uncertain | o3 shows rapid capability gains; alignment solutions remain an active research area |

## Organizational Evolution

### Founding Vision vs. Current Reality

| Aspect | 2015 Foundation | 2025 Reality | Change |
|--------|-----------------|--------------|--------|
| **Structure** | Non-profit | Public Benefit Corporation (converted from capped-profit LLC) | Major structural change |
| **Funding** | ≈\$1B founder commitment | \$13B+ Microsoft investment | 13x scale increase |
| **Openness** | "Open by default" research publishing | Proprietary models, limited disclosure | Substantial shift toward proprietary development |
| **Mission Priority** | "AGI benefits all humanity" | Product revenue and market leadership | Contested; company argues commercial success funds mission |
| **Safety Approach** | "Safety over competitive advantage" | Safety integrated as constraint within product development | Disputed; safety researchers cite deprioritization, company disputes characterization |
| **Governance** | Independent non-profit board | Post-November 2023 board with commercial representation; critics argue reduced independent oversight capacity | Restructured; interpretations differ |

### Key Milestones and Capability Jumps

| Date | Development | Parameters/Scale | Significance | Safety Implications |
|------|-------------|------------------|--------------|-------------------|
| **2018** | GPT-1 | 117M | First transformer LM | Established architecture |
| **2019** | GPT-2 | 1.5B | Initially withheld | Demonstrated misuse concerns |
| **2020** | GPT-3 | <F e="openai" f="7c9b9073">175B</F> | <EntityLink id="emergent-capabilities">Few-shot learning</EntityLink> breakthrough | Sparked scaling race |
| **2022** | InstructGPT/ChatGPT | GPT-3.5 + <EntityLink id="rlhf">RLHF</EntityLink> | Mainstream AI adoption | RLHF as alignment technique |
| **2023** | GPT-4 | Undisclosed multimodal | Human-level performance on many tasks | Dangerous capabilities acknowledged |
| **2024** | o1 reasoning | Advanced chain-of-thought | Mathematical/scientific reasoning | Hidden reasoning, deception risks |
| **2024** | o3 preview | Next-generation reasoning | Near-frontier performance on some tasks | Rapid capability advancement |
| **2025** | o3-mini | Efficient reasoning | Broader reasoning availability | Democratized advanced capabilities |

## Technical Contributions and Evolution

### Major Research Breakthroughs

| Innovation | Impact | Adoption | Limitations |
|------------|--------|----------|-------------|
| **GPT Architecture** | Established transformer LMs as dominant paradigm | Universal across industry | Scaling may encounter physical limits |
| **RLHF/InstructGPT** | Made LMs helpful, harmless, honest | Standard alignment technique | May not scale to superhuman tasks |
| **Scaling Laws** | Predictable performance from compute/data | Drove \$100B+ industry investment | Unclear whether they continue to AGI-level systems |
| **Chain-of-Thought Reasoning** | Test-time compute for complex problems | Adopted by <EntityLink id="anthropic">Anthropic</EntityLink>, Google | Hidden reasoning creates interpretability challenges |
| **Deliberative Alignment** | Reasoning-based safety specifications | Used in o-series models[^20] | Limited external evaluation in practice |

### Safety Research Evolution

**Current Methodology (2025):**
- **Deliberative Alignment**: Teaching reasoning models human-written safety specifications[^21]
- **Scalable Evaluations**: Automated tests measuring capability proxies[^22]
- **Cross-Lab Collaboration**: Joint evaluations with <EntityLink id="anthropic">Anthropic</EntityLink> and other labs[^23]
- **Red Teaming**: Human adversarial testing complementing automated evaluations

**Safety Framework Assessment:**
- Preparedness Framework established capability thresholds and evaluation protocols
- Safety evaluations now include third-party assessments beyond internal teams
- Alignment research continues post-Superalignment dissolution but with reduced external visibility
- Safety measures are integrated into product development rather than maintained as a separate research track

## Competitive Landscape Analysis

### Capability Comparison (Late 2025)

| Company | Latest Model | Key Strengths | Market Position |
|---------|--------------|---------------|-----------------|
| **OpenAI** | o3-mini, o1 | Reasoning capabilities, broad deployment | Market leader (81% generative AI share) |
| **<EntityLink id="anthropic">Anthropic</EntityLink>** | Claude (current series) | Safety research emphasis, coding benchmarks | Strong challenger in enterprise segment |
| **Google** | Gemini 2.5 | Research depth, multimodal, integration | Significant technology position |
| **<EntityLink id="meta-ai">Meta AI</EntityLink>** | Llama 4 | Open source approach | Alternative paradigm |

**Performance Benchmarks (o1 series):**
- o1 leads mathematical reasoning: <F e="openai" f="84af6115">83%</F> on AIME math competition
- o1 on SWE-bench Verified: <F e="openai" f="25d2308f">71.7%</F>
- Context length and safety remain key differentiators across providers

## Developer Ecosystem and Business Strategy

### API and Integration Platform

**Market Penetration:**
- API monthly revenue: ≈\$41M from 530 billion tokens (June 2024)[^27]
- Gross margins: 75% decreasing to 55% with pricing adjustments[^28]
- Azure OpenAI Service: 64% year-over-year growth adoption[^29]
- Enterprise integration across Microsoft Office 365, GitHub Copilot

**Developer Adoption:**
- GPT Store: 159,000 public GPTs from 3 million total created[^30]
- Average 1,500 new models added daily to marketplace[^31]
- API infrastructure market share exceeding 50% industry-wide
- Integration partnerships with major enterprise software providers

## Financial and Commercial Dynamics

### Revenue and Investment Structure

**2024-2025 Financial Performance:**
- Projected 2024 revenue: \$3.4 billion (ChatGPT subscriptions + API)[^32]
- Growth rate: <F e="openai" f="be463d29">1,700%</F> from early 2023 to September 2024
- Operating losses: \$5 billion in 2024 despite revenue growth[^33]
- Primary cost drivers: compute infrastructure, talent acquisition, research investment. OpenAI leadership has characterized these losses as reflecting deliberate investment in infrastructure and talent rather than financial distress.

### Microsoft Partnership

| Component | Details | Strategic Implications |
|-----------|---------|----------------------|
| **Investment** | <F e="openai" f="931278d9">\$13B+</F> total; 49% profit share under original structure (transitioning to direct equity under PBC restructuring; final terms subject to regulatory approval) | Creates commercial pressure for rapid deployment; restructuring alters long-term economic alignment |
| **Compute Access** | Exclusive Azure partnership | Enables massive model training but creates infrastructure dependency |
| **Product Integration** | Bing, Office 365, GitHub Copilot | Drives revenue but requires consumer-ready systems |
| **API Monetization** | Enterprise and developer access | Success depends on maintaining capability lead |

## Governance Crisis Analysis

### November 2023 Board Transition

| Timeline | Event | Stakeholders | Outcome |
|----------|-------|--------------|---------|
| **Nov 17** | Board removes Sam Altman, citing lack of candor | Non-profit board, Ilya Sutskever | Initial dismissal |
| **Nov 18-19** | Employee letter, Microsoft intervention | <F e="sam-altman" f="6783fa6c">738 of 770 employees</F> signed letter; Microsoft leadership | Pressure for reversal |
| **Nov 21-22** | Altman reinstated, board reconstituted | New board (Bret Taylor chair, Lawrence Summers) | Governance restructured |

**Structural observations (contested interpretations):**
- The episode demonstrated that employee and investor sentiment significantly constrained the non-profit board's practical authority
- Critics argue the reconstituted board has reduced independence from commercial leadership; OpenAI leadership contends the new board has greater relevant expertise
- The Microsoft partnership's scale creates financial interdependencies that influence operational decisions; the degree to which this constitutes a constraint over safety-motivated decisions is disputed
- The episode is cited by governance scholars as a case study in the practical limits of non-profit oversight of commercial AI operations

## Key Leadership Departures (2024)

The following departures occurred in 2024. Stated reasons varied across individuals; the safety-motivation framing is most explicitly supported by Leike and Schulman's public statements, while other departures involved personal or exploratory motivations.

| Researcher | Role | Departure Date | Stated Reasons | Destination |
|------------|------|----------------|---------------|-------------|
| **<EntityLink id="ilya-sutskever">Ilya Sutskever</EntityLink>** | Co-founder, Chief Scientist | May 2024 | "Personal project" (<EntityLink id="superintelligence">SSI</EntityLink>) | Safe Superintelligence Inc |
| **<EntityLink id="jan-leike">Jan Leike</EntityLink>** | Superalignment Co-lead | May 2024 | "Safety culture backseat to products"[^34] | <EntityLink id="anthropic">Anthropic</EntityLink> Head of Alignment |
| **John Schulman** | Co-founder, PPO inventor | Aug 2024 | "Deepen <EntityLink id="alignment">AI alignment</EntityLink> focus" | <EntityLink id="anthropic">Anthropic</EntityLink> |
| **Mira Murati** | Chief Technology Officer | Sept 2024 | "Personal exploration" | Not disclosed |

**Context:**
- <F e="openai" f="fba50864">75%</F> of co-founders had departed within 9 years of founding
- Leike and Schulman explicitly cited safety prioritization concerns in public statements; Sutskever and Murati gave different reasons
- <EntityLink id="anthropic">Anthropic</EntityLink> subsequently hired multiple senior OpenAI researchers into alignment-focused roles
- OpenAI leadership disputed characterizations of a systematic safety deprioritization, pointing to ongoing safety investments and the Preparedness Framework

## Current Capability Assessment

### Reasoning Models Performance (o1/o3 Series)

| Domain | Capability Level | Benchmark Performance | Risk Assessment |
|--------|------------------|----------------------|-----------------|
| **Mathematics** | PhD+ | <F e="openai" f="84af6115">83%</F> on AIME, IMO medal performance | Advanced problem-solving |
| **Programming** | Expert | <F e="openai" f="25d2308f">71.7%</F> on SWE-bench Verified | Code generation/analysis |
| **Scientific Reasoning** | Graduate+ | High performance on PhD-level physics | Research acceleration potential |
| **Strategic Reasoning** | Not well-characterized | Chain-of-thought reasoning partially hidden | <EntityLink id="deceptive-alignment">Deceptive alignment</EntityLink> risks; active interpretability research area |

**Key Technical Developments:**
- Test-time compute scaling enables reasoning capability improvements
- Partially hidden reasoning processes limit interpretability and alignment verification
- Performance approaching human expert level across cognitive domains
- Deliberative alignment methodology integrated into training process

## Economic Impact and Industry Transformation

### Enterprise Adoption and Integration

**Fortune 500 Penetration:**
- 92% of Fortune 500 companies actively using OpenAI products or APIs[^35]
- Primary use cases: customer service automation, content generation, code assistance
- Integration through Microsoft ecosystem (Office 365, Teams, Azure)
- Custom enterprise solutions and fine-tuning services

**Industry Transformation Metrics:**
- Sparked \$100B+ investment across AI industry following ChatGPT launch
- Developer productivity improvements: 10-40% in coding tasks (GitHub Copilot studies)
- Content creation acceleration across marketing, education, professional services
- Job market evolution with AI-augmented roles emerging alongside traditional functions

## International Strategy and Regulatory Engagement

### Government Relations and Policy Influence

| Jurisdiction | Engagement Type | OpenAI Position | Policy Impact |
|--------------|----------------|-----------------|---------------|
| **US Congress** | Altman testimony, lobbying | Self-regulation advocacy | Influenced Senate AI framework |
| **<EntityLink id="eu-ai-act">EU AI Act</EntityLink>** | Compliance preparation | Geographic market access | Foundation model regulations apply |
| **UK AI Safety** | <EntityLink id="uk-aisi">AISI</EntityLink> collaboration | Partnership approach | Safety institute cooperation |
| **China** | No direct engagement | Technology export controls | Limited model access |

### Global Expansion Framework

**Data Sovereignty Approach:**
- OpenAI for Countries program supporting local data centers[^36]
- Partnerships for in-country infrastructure development
- Balance between global access and national security concerns
- Custom deployment models for government and enterprise clients

## Safety Methodology and Alignment Research

### Current Safety Framework (2025)

**Evaluation Processes:**
- **Scalable Evaluations**: Automated testing measuring capability proxies[^37]
- **Deep Dives**: Human red-teaming and third-party assessments[^38]
- **Capability Thresholds**: Predetermined criteria triggering additional safety measures
- **Cross-Lab Collaboration**: Joint safety evaluations with industry partners

**Deliberative Alignment Implementation:**
- Integration of human-written safety specifications into reasoning models[^39]
- Training models to explicitly reason about safety considerations
- Applied to o-series models with ongoing evaluation
- Represents an evolution beyond RLHF toward more explicit safety reasoning in model outputs

### Alignment Research Post-Superalignment

**Current Research Directions:**
- <EntityLink id="scalable-oversight">Scalable oversight</EntityLink> methods for superhuman AI systems
- <EntityLink id="interpretability">Interpretability</EntityLink> research for understanding model reasoning
- Robustness testing across diverse deployment scenarios
- Integration of safety measures into product development cycles

**Resource Allocation:**
- Original 20% compute allocation for safety research; current structure is not publicly confirmed
- Safety research integrated into product teams rather than housed in an independent research division
- External commentators have raised concerns about the sufficiency of dedicated safety resources; OpenAI disputes these characterizations
- Balance between product development velocity and safety thoroughness remains a subject of ongoing public debate

## Expert Perspectives and Current Debates

### Internal Alignment (Current Leadership)

**Sam Altman's Position (2025):**
- AGI development is expected to proceed and OpenAI believes it is better positioned than alternatives to pursue it responsibly
- Commercial success is argued to enable greater safety research investment
- Rapid deployment with iterative safety improvements is preferred over delayed release
- Maintaining technological leadership is framed as necessary given competitive dynamics

**Technical Leadership Perspective:**
- Integration of safety measures into the development process rather than maintaining separate research tracks
- Emphasis on real-world deployment experience as a source of safety learning
- Collaborative industry approach to safety standards and evaluation

### External Safety Community Assessment

**Academic and Safety Researcher Views:**
- <EntityLink id="yoshua-bengio">Yoshua Bengio</EntityLink>: Has publicly expressed concern about commercial mission drift from original safety focus
- <EntityLink id="stuart-russell">Stuart Russell</EntityLink>: Has publicly warned about commercial capture of safety research priorities
- Former OpenAI safety researchers (Leike, Schulman): Cited systematic deprioritization of safety relative to capabilities in public departure statements[^34]

**Policy and Governance Experts:**
- External oversight mechanisms beyond self-regulation have been advocated by multiple governance researchers
- Concentration of AGI development in a single organization raises questions about democratic accountability
- Legal scholars have assessed the PBC structure as a formal improvement over the capped-profit LLC for mission preservation; critics argue it remains an insufficient safeguard without stronger enforcement mechanisms

## Future Trajectories and Critical Decisions

### Timeline Projections (Updated 2025)

| Scenario | Probability Estimate | Timeline | Key Indicators |
|----------|---------------------|----------|----------------|
| **<EntityLink id="agi-development">AGI Development</EntityLink>** | High (per Altman) | 1-3 years | Altman confidence, o3+ performance |
| **Regulatory Intervention** | Medium-High | 1-2 years | Government AI governance initiatives |
| **Safety Breakthrough** | Low-Medium | Unknown | <EntityLink id="scalable-oversight">Scalable alignment</EntityLink> advances |
| **Competitive Disruption** | Medium | 2-3 years | Open source parity, international advances |

### Strategic Decision Points

**Immediate (2025):**
- AGI timeline communications and expectation management
- Response to increasing regulatory scrutiny and safety criticism
- Resource allocation between reasoning model advancement and safety research
- International expansion pace and partnership selection
- Completion of PBC restructuring, pending California AG and legal approvals

**Medium-term (2026-2027):**
- AGI deployment framework and access policies
- Safety standard establishment and industry coordination
- Relationship management with government oversight bodies
- Competitive response to potential capability disruptions

## Key Research Questions

<KeyQuestions questions={[
  "Can OpenAI maintain safety priorities while pursuing aggressive AGI timelines?",
  "Will deliberative alignment scale to superintelligent systems with hidden reasoning?",
  "How will international coordination develop around OpenAI's AGI deployment decisions?",
  "What governance mechanisms could effectively constrain rapid AGI development?",
  "Can the developer ecosystem and API strategy support sustainable business model?",
  "How will competitive dynamics evolve as multiple labs approach AGI capabilities?",
  "How will the PBC restructuring affect the non-profit foundation's ability to enforce mission-aligned constraints?",
  "What role will the California AG's oversight play in shaping the final restructuring terms?"
]} />

## Sources and Resources

### Primary Documents
| Source | Type | Key Content | Link |
|--------|------|-------------|------|
| GPT-4 System Card | Technical report | Risk assessment, red teaming results | [OpenAI GPT-4 System Card](https://cdn.openai.com/papers/gpt-4-system-card.pdf) |
| Preparedness Framework | Policy document | Catastrophic risk evaluation framework | [OpenAI Preparedness](https://cdn.openai.com/openai-preparedness-framework-beta.pdf) |
| Deliberative Alignment | Research paper | Reasoning-based safety methodology | [OpenAI Deliberative Alignment](https://openai.com/index/deliberative-alignment/) |
| OpenAI for Countries | Policy initiative | International partnership framework | [Global Affairs Initiative](https://openai.com/global-affairs/openai-for-countries/) |

### Recent Announcements and Performance
| Source | Type | Key Content | Link |
|--------|------|-------------|------|
| Sora 2 Release | Product announcement | Video generation capabilities | [Sora 2 Launch](https://openai.com/index/sora-2/) |
| o3-mini Launch | Model release | Latest reasoning model availability | [Computerworld Coverage](https://www.computerworld.com/article/4015023/openai-latest-news-and-insights.html) |
| AGI Timeline Interview | Executive statement | Altman's AGI predictions | [TIME Magazine Interview](https://time.com/7205596/sam-altman-superintelligence-agi/) |

### Academic Research
| Paper | Authors | Contribution | Citation |
|-------|---------|-------------|----------|
| Language Models are Few-Shot Learners | Brown et al. | GPT-3 capabilities demonstration | [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) |
| Training language models to follow instructions | Ouyang et al. | InstructGPT/RLHF methodology | [arXiv:2203.02155](https://arxiv.org/abs/2203.02155) |
| <EntityLink id="weak-to-strong">Weak-to-Strong Generalization</EntityLink> | Burns et al. | Superalignment research direction | [arXiv:2312.09390](https://arxiv.org/abs/2312.09390) |
| GPT-4 Technical Report | OpenAI (279 contributors) | Official technical documentation | [arXiv:2303.08774](https://arxiv.org/abs/2303.08774) |

[^1]: [ChatGPT Users Statistics (February 2026) – Growth & Usage Data](https://www.demandsage.com/chatgpt-statistics/)
[^2]: [ChatGPT Users Statistics (February 2026) – Growth & Usage Data](https://www.demandsage.com/chatgpt-statistics/)
[^3]: [OpenAI Latest News and Insights](https://www.computerworld.com/article/4015023/openai-latest-news-and-insights.html)
[^5]: [OpenAI Latest News and Insights](https://www.computerworld.com/article/4015023/openai-latest-news-and-insights.html)
[^6]: [Sora 2 is here](https://openai.com/index/sora-2/)
[^7]: [ChatGPT Users Statistics (February 2026) – Growth & Usage Data](https://www.demandsage.com/chatgpt-statistics/)
[^8]: [OpenAI lost \$5 billion in 2024 (and its losses are increasing)](https://www.lesswrong.com/posts/CCQsQnCMWhJcCFY9x/openai-lost-usd5-billion-in-2024-and-its-losses-are)
[^9]: [OpenAI lost \$5 billion in 2024 (and its losses are increasing)](https://www.lesswrong.com/posts/CCQsQnCMWhJcCFY9x/openai-lost-usd5-billion-in-2024-and-its-losses-are)
[^10]: [OpenAI Statistics 2026: Adoption, Integration & Innovation](https://sqmagazine.co.uk/openai-statistics/)
[^11]: [OpenAI's API Profitability in 2024](https://futuresearch.ai/openai-api-profit/)
[^12]: [OpenAI's API Profitability in 2024](https://futuresearch.ai/openai-api-profit/)
[^13]: [The Era of Tailored Intelligence: Charting the Growth and Market Impact of Custom GPTs](https://originality.ai/blog/gpts-statistics)
[^14]: [OpenAI Statistics 2026: Adoption, Integration & Innovation](https://sqmagazine.co.uk/openai-statistics/)
[^15]: [Introducing OpenAI for Countries](https://openai.com/global-affairs/openai-for-countries/)
[^16]: [Inside OpenAI's Global Business Expansion](https://ff.co/openai-business-expansion/)
[^17]: [Inside OpenAI's Global Business Expansion](https://ff.co/openai-business-expansion/)
[^18]: [How OpenAI's Sam Altman Is Thinking About AGI and Superintelligence in 2025](https://time.com/7205596/sam-altman-superintelligence-agi/)
[^19]: [We know how to build AGI - Sam Altman](https://www.lesswrong.com/posts/T5p9NEAyrHedC2znD/recent-sam-altman-statements-on-agi-and-asi)
[^20]: [Deliberative alignment: reasoning enables safer language models](https://openai.com/index/deliberative-alignment/)
[^21]: [Deliberative alignment: reasoning enables safer language models](https://openai.com/index/deliberative-alignment/)
[^22]: [All the labs AI safety plans: 2025 edition](https://www.lesswrong.com/posts/dwpXvweBrJwErse3L/all-the-lab-s-ai-safety-plans-2025-edition)
[^23]: [All the labs AI safety plans: 2025 edition](https://www.lesswrong.com/posts/dwpXvweBrJwErse3L/all-the-lab-s-ai-safety-plans-2025-edition)
[^27]: [OpenAI's API Profitability in 2024](https://futuresearch.ai/openai-api-profit/)
[^28]: [OpenAI's API Profitability in 2024](https://futuresearch.ai/openai-api-profit/)
[^29]: [OpenAI Statistics 2026: Adoption, Integration & Innovation](https://sqmagazine.co.uk/openai-statistics/)
[^30]: [GPT Store Statistics & Facts: Contains 159.000 of the 3 million created GPTs](https://seo.ai/blog/gpt-store-statistics-facts)
[^31]: [The Era of Tailored Intelligence: Charting the Growth and Market Impact of Custom GPTs](https://originality.ai/blog/gpts-statistics)
[^32]: [OpenAI lost \$5 billion in 2024 (and its losses are increasing)](https://www.lesswrong.com/posts/CCQsQnCMWhJcCFY9x/openai-lost-usd5-billion-in-2024-and-its-losses-are)
[^33]: [OpenAI lost \$5 billion in 2024 (and its losses are increasing)](https://www.lesswrong.com/posts/CCQsQnCMWhJcCFY9x/openai-lost-usd5-billion-in-2024-and-its-losses-are)
[^34]: Jan Leike departure statement on X/Twitter, May 2024
[^35]: [OpenAI Statistics 2026: Adoption, Integration & Innovation](https://sqmagazine.co.uk/openai-statistics/)
[^36]: [Introducing OpenAI for Countries](https://openai.com/global-affairs/openai-for-countries/)
[^37]: [All the labs AI safety plans: 2025 edition](https://www.lesswrong.com/posts/dwpXvweBrJwErse3L/all-the-lab-s-ai-safety-plans-2025-edition)
[^38]: [All the labs AI safety plans: 2025 edition](https://www.lesswrong.com/posts/dwpXvweBrJwErse3L/all-the-lab-s-ai-safety-plans-2025-edition)
[^39]: [Deliberative alignment: reasoning enables safer language models](https://openai.com/index/deliberative-alignment/)
