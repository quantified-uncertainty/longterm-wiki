---
title: Government AI Safety Organizations
description: Overview of government bodies and intergovernmental organizations focused on AI safety, including national AI Safety Institutes and multilateral coordination bodies.
sidebar:
  label: Overview
  order: 0
subcategory: government
---
import {EntityLink} from '@components/wiki';

## Overview

Governments have begun establishing dedicated institutions to address AI safety risks, with AI Safety Institutes (AISIs) emerging as a key organizational model since 2023. These bodies operate with public mandates and budgets that distinguish them from the largely philanthropically-funded nonprofit landscape, though they face different constraints including political cycles and bureaucratic processes.

## National AI Safety Institutes

| Organization | Country | Founded | Focus | Budget |
|---|---|---|---|---|
| **<EntityLink id="E364">UK AI Safety Institute</EntityLink>** | UK | 2023 | Frontier model evaluation, safety research | ≈\$65M |
| **<EntityLink id="E365">US AI Safety Institute</EntityLink>** | US | 2024 | Standards development, model evaluation | ≈\$47.7M requested |
| **<EntityLink id="E424">NIST AI</EntityLink>** | US | Ongoing | AI Risk Management Framework, standards | Part of NIST budget |

The UK AISI was the first national AI Safety Institute, established following the Bletchley Park AI Safety Summit in November 2023. The US AISI was established within NIST in 2024. Both conduct pre-deployment evaluations of frontier AI models.

## Intergovernmental Bodies

- **<EntityLink id="E533">Global Partnership on Artificial Intelligence (GPAI)</EntityLink>**: Multilateral initiative with 29 member countries working on responsible AI development and governance

## International Network of AI Safety Institutes

As of early 2026, 11+ countries have established or announced AI Safety Institutes, forming a growing network for international safety coordination. Members share evaluation methodologies, coordinate on frontier model assessments, and develop common safety benchmarks. Key members include the UK, US, Japan, Canada, France, and South Korea, with India joining in 2026.

## Key Dynamics

**Political vulnerability**: Government AI safety bodies are subject to changes in political leadership and priorities. The US AISI's mandate and funding depend on congressional and executive support, which can shift between administrations.

**Relationship with labs**: AISIs must balance cooperative relationships with frontier labs (needed for access to models) against independent oversight mandates. The UK AISI has voluntary agreements with major labs for pre-deployment access.

**Complementarity with nonprofits**: Government bodies focus on standards, regulation, and institutional evaluation capacity, while nonprofit safety organizations (like <EntityLink id="E201">METR</EntityLink> and <EntityLink id="E24">Apollo Research</EntityLink>) conduct more specialized technical research. There is increasing collaboration between the two.
