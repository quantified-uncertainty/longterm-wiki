---
title: FAR AI
description: AI safety research nonprofit founded in 2022 by Adam Gleave and Karl Berzins, focusing on making AI systems safe through technical research and coordination
sidebar:
  order: 16
quality: 32
llmSummary: FAR AI (FAR.AI) is a 2022-founded AI safety research nonprofit led by CEO Adam Gleave and COO Karl Berzins. The organization focuses on technical AI safety research and coordination to ensure safety techniques are adopted. Their research has been cited in Congress and won best paper awards.
lastEdited: "2026-02-17"
readerImportance: 84.5
researchImportance: 53.5
update_frequency: 21
ratings:
  novelty: 2.5
  rigor: 3
  actionability: 2
  completeness: 5
clusters:
  - ai-safety
  - community
subcategory: safety-orgs
entityType: organization
---
import {DataInfoBox, KeyPeople, KeyQuestions, Section, R, EntityLink, DataExternalLinks} from '@components/wiki';

<DataExternalLinks pageId="far-ai" />

<DataInfoBox entityId="E138" />

## Overview

FAR AI (FAR.AI, standing for Frontier AI Research) is an AI safety research nonprofit founded in July 2022 by Adam Gleave (CEO) and Karl Berzins (COO). Adam Gleave completed his PhD in AI at UC Berkeley, advised by <EntityLink id="stuart-russell">Stuart Russell</EntityLink>. The organization focuses on technical innovation to make AI systems safe and coordination to ensure these safety techniques are adopted.

FAR AI conducts technical research in areas including <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink>, model evaluation, and alignment theory. The organization also operates FAR.Labs, a co-working space for AI safety researchers in Berkeley, and runs grant-making and convening programs. {/* NEEDS CITATION */} FAR AI's work has been published at machine learning conferences including NeurIPS, ICML, and ICLR, and the organization has received funding from sources in the effective altruism ecosystem.

## Key Research Areas

### Adversarial Robustness

| Research Focus | Approach | Safety Connection | Publications |
|----------------|----------|-------------------|--------------|
| <EntityLink id="adversarial-training">Adversarial Training</EntityLink> | Training models to resist adversarial examples | Robust systems prerequisite for alignment | Multiple top-tier venues |
| Certified Defenses | Mathematical guarantees against attacks | Worst-case safety assurances | NeurIPS, ICML papers |
| Robustness Evaluation | Comprehensive testing against adversarial inputs | Identifying failure modes | Benchmark development |
| Distribution Shift | Performance under novel conditions | Real-world deployment safety | ICLR, AISTATS |

FAR AI's research in <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink> examines how to make AI systems resistant to adversarial examples and maintain performance under distribution shift. This work connects to broader AI safety concerns by addressing the reliability of AI systems under challenging or out-of-distribution conditions. The organization has published research on <EntityLink id="adversarial-training">adversarial training</EntityLink> methods, certified defense mechanisms, and robustness evaluation frameworks.

### Research Programs

FAR AI operates through several key programs:

| Program | Purpose | Impact | Details |
|---------|---------|--------|---------|
| FAR.Labs | Co-working space | 40+ members | Berkeley-based AI safety research hub |
| Grant-making | Fund external research | Academic partnerships | Early-stage safety research funding |
| Events & Workshops | Convene stakeholders | 1,000+ attendees | Industry, policy, academic coordination |
| In-house Research | Technical safety work | 30+ papers published | Robustness, interpretability, alignment |

The grant-making program funds external researchers working on AI safety problems, particularly those in academic settings. FAR.Labs provides physical co-working space and community infrastructure for independent researchers and small teams working on <EntityLink id="technical-research">technical AI safety research</EntityLink>. The events program hosts workshops and convenings that bring together researchers from industry, academia, and policy backgrounds.

### Natural Abstractions Research

| Research Question | Hypothesis | Implications | Status |
|-------------------|------------|--------------|--------|
| Universal Concepts | Intelligent systems discover same abstractions | Shared conceptual basis for alignment | Theoretical development |
| Neural Network Learning | Do NNs learn natural abstractions? | Interpretability foundations | Empirical investigation |
| Alignment Verification | Can we verify shared concepts? | Communication with AI systems | Early research |
| Mathematical Universality | Math/physics as natural abstractions | Foundation for value alignment | Ongoing |

FAR AI has theoretical interest in <EntityLink id="natural-abstractions">natural abstractions</EntityLink> research, which explores whether intelligent systems independently converge on similar conceptual representations of the world. This research direction connects to work by <EntityLink id="miri">MIRI</EntityLink> and other organizations investigating whether shared abstractions between human and AI cognition could provide a foundation for alignment approaches.

## Organizational Structure and Operations

### Research Team

FAR AI's research team is led by CEO Adam Gleave, who brings expertise in reinforcement learning, adversarial robustness, and AI safety from his doctoral work at UC Berkeley. Karl Berzins serves as COO, overseeing operations and coordination functions. {/* NEEDS CITATION */} The organization employs researchers with backgrounds in machine learning, AI safety, and related technical fields.

### FAR.Labs Co-working Space

FAR.Labs operates as a Berkeley-based co-working facility for AI safety researchers. The space hosts independent researchers, fellows, and small research teams working on <EntityLink id="safety-research">AI safety</EntityLink> problems. {/* NEEDS CITATION */} The co-working model provides infrastructure, community, and coordination benefits for researchers who may not be affiliated with larger institutions.

The reported membership of 40+ individuals includes both full-time residents and part-time participants. The space facilitates collaboration and knowledge-sharing among researchers working on related technical problems.

## Current State & Trajectory

### 2024 Research Progress

**Publications**: FAR AI researchers have published work at machine learning conferences including NeurIPS, ICML, and ICLR, focusing on adversarial robustness, model evaluation, and related safety topics.

**Team Growth**: The organization has expanded its research staff and increased the capacity of FAR.Labs to host additional researchers.

**Collaborations**: FAR AI maintains partnerships with academic institutions and other organizations working on AI safety, including collaborations on evaluation methodologies and robustness research.

### Current Metrics

| Metric | Status | Context |
|--------|--------|---------|
| Research Papers | 30+ published | Academic venues including NeurIPS, ICML, ICLR |
| FAR.Labs Members | 40+ | Berkeley co-working space participants |
| Events Hosted | 10+ | Workshops and convenings |
| Research Focus | Robustness, interpretability, evaluation, alignment | Active research programs |

These metrics represent organization-reported figures and include publications co-authored by FAR AI researchers, membership in the FAR.Labs co-working space, and events organized or co-organized by the organization.

## Strategic Position Analysis

### Organizational Comparisons

| Organization | Focus | Overlap | Differentiation |
|--------------|-------|---------|----------------|
| <EntityLink id="anthropic">Anthropic</EntityLink> | <EntityLink id="constitutional-ai">Constitutional AI</EntityLink>, scaling | Safety research | Academic publication, no model development |
| <EntityLink id="arc">ARC</EntityLink> | Alignment research | Theoretical alignment | Empirical ML approach |
| <EntityLink id="metr">METR</EntityLink> | Model evaluation | Safety assessment | Robustness specialization |
| Academic Labs | ML research | Technical methods | Safety mission-focused |

FAR AI differs from model-developing organizations like <EntityLink id="anthropic">Anthropic</EntityLink> by focusing on research publication rather than deploying AI systems. Compared to more theoretically-oriented organizations like <EntityLink id="arc">ARC</EntityLink>, FAR AI emphasizes empirical machine learning methods. The organization shares methodological overlap with evaluation-focused groups like <EntityLink id="metr">METR</EntityLink> while maintaining broader research interests.

### Positioning in AI Safety Ecosystem

FAR AI occupies a position between academic AI safety research and industry application. The organization publishes at mainstream machine learning venues, which provides visibility within the broader ML community beyond specialized safety researchers. The grant-making and convening programs position FAR AI as a coordinator within the AI safety ecosystem, connecting researchers across institutional boundaries.

The organization's focus on <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink> and empirical evaluation connects to near-term safety concerns while maintaining theoretical interest in longer-term alignment challenges through work on <EntityLink id="natural-abstractions">natural abstractions</EntityLink> and related concepts.

## Research Impact and Influence

### Academic Publications

FAR AI researchers publish at top-tier machine learning conferences including NeurIPS (Conference on Neural Information Processing Systems), ICML (International Conference on Machine Learning), and ICLR (International Conference on Learning Representations). Publications cover topics in adversarial robustness, model evaluation, and AI safety.

The organization has reported publishing 30+ papers, though specific publication titles, citation counts, and impact metrics require verification. {/* NEEDS CITATION */}

### Policy Engagement

FAR AI's research has been referenced in policy discussions. {/* NEEDS CITATION */} The organization has reported that its work was cited in congressional contexts, though specific hearings, reports, or legislative documents require verification.

The organization participates in policy discussions through workshops and convenings that bring together technical researchers with policymakers and governance-focused participants.

### Community Building

Through FAR.Labs, the organization provides infrastructure for independent researchers and small teams. The reported 40+ members include researchers working on various aspects of <EntityLink id="technical-research">technical AI safety</EntityLink>. The co-working space model aims to reduce barriers for researchers who lack institutional affiliation while maintaining focus on safety-relevant technical problems.

FAR AI's events program reports hosting 10+ workshops and convenings with 1,000+ total attendees, though specific event details require verification. {/* NEEDS CITATION */} These events connect researchers from industry, academia, and policy backgrounds around AI safety topics.

## Research Questions and Uncertainties

### Theoretical Questions

Several theoretical questions shape FAR AI's research direction:

- **Natural Abstractions Validity**: Whether intelligent systems independently converge on similar conceptual representations remains an open empirical question. The <EntityLink id="natural-abstractions">natural abstractions</EntityLink> hypothesis has theoretical appeal but requires extensive empirical validation across diverse AI architectures and training regimes.

- **Robustness-Alignment Connection**: The relationship between <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink> and value alignment is unclear. While robustness may be necessary for aligned systems, the degree to which robustness research directly contributes to solving alignment problems remains debated within the AI safety community.

- **Scaling Dynamics**: Whether current robustness and evaluation approaches will remain relevant as AI systems increase in capability is uncertain. Some safety researchers argue that qualitatively new challenges emerge at higher capability levels that may not be addressed by current methodologies.

### Organizational Uncertainties

- **Research Timeline**: Academic publication timelines typically span months to years, while AI capabilities advance rapidly. Whether this research pace adequately matches the urgency of safety concerns depends on timelines for transformative AI development.

- **Scope Evolution**: FAR AI's research focus may evolve as the field develops. The organization's emphasis on empirical robustness could shift toward other safety approaches depending on which problems prove most tractable or urgent.

- **Policy Engagement**: The extent of FAR AI's involvement in AI governance and policy discussions may expand beyond its current focus on technical research and convening activities.

### Field-Wide Debates

| Debate | FAR AI Position | Alternative Views | Resolution Timeline |
|--------|-----------------|-------------------|-------------------|
| Value of robustness for alignment | Robustness research relevant to safety | Limited connection to core alignment | 2-3 years |
| Natural abstractions importance | Potentially foundational concept | Speculative without strong evidence | 5+ years |
| Academic vs. applied research | Balance needed | Industry focus more impactful | Ongoing |
| Benchmark limitations | Manageable with good design | Fundamental Goodhart problems | 1-2 years |

These debates reflect broader disagreements within the AI safety community about research priorities, timelines, and the relationship between different technical approaches to safety.

## Funding & Sustainability

### Current Funding Model

| Source Type | Estimated % | Characteristics | Notes |
|-------------|-------------|-----------------|-------|
| EA Foundations | 70-80% | Mission-aligned, multi-year support | Includes <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> and related funders |
| Government Grants | 10-15% | Competitive, restricted use | Enhances credibility, bureaucratic requirements |
| Private Donations | 10-15% | Variable amounts and timelines | Individual donors, smaller foundations |

These funding estimates are approximate and based on typical funding patterns for AI safety research organizations in the <EntityLink id="ea">effective altruism</EntityLink> ecosystem. Specific funding amounts and sources require verification from organizational disclosures or funder databases.

### Financial Sustainability

FAR AI's funding profile reflects dependence on sources within the effective altruism ecosystem, particularly <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> and related foundations. This concentration provides mission alignment and relatively stable multi-year funding but creates vulnerability to changes in EA funding priorities or grantmaker assessment of the organization's impact.

Government grants provide credibility through competitive peer review but typically carry restrictions on fund usage and require substantial administrative overhead. Private donations offer flexibility but may be less predictable for long-term planning.

The organization competes with higher-paying industry positions for technical talent, which affects hiring and retention. Academic-style research positions typically offer lower compensation than roles at AI labs, though they provide greater research autonomy and publication freedom.

## Criticisms and Responses

### Academic Pace Concerns

**Criticism**: Academic publication processes operate on timelines of months to years, including peer review, revision, and conference scheduling. Critics argue this pace may be too slow given rapid AI capability advances and the urgency of safety concerns.

**FAR AI Perspective**: Peer-reviewed publication ensures research quality and credibility. The methodology and evaluation frameworks developed through careful research provide lasting value even as specific techniques evolve. Preprint sharing and direct collaboration with AI labs can accelerate impact for time-sensitive findings.

**Context**: This tension between research rigor and speed affects the broader AI safety field. Different organizations make different tradeoffs between publication quality, speed, and direct industry impact.

### Limited Scope Questions

**Criticism**: Research on <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink> and evaluation may not directly address core alignment challenges like <EntityLink id="deceptive-alignment">deceptive alignment</EntityLink>, goal specification, or value learning. Critics question whether robustness research provides sufficient traction on the hardest alignment problems.

**FAR AI Perspective**: Robustness is a necessary foundation for aligned systems. Understanding failure modes through adversarial testing and evaluation provides crucial insights for safety. The organization's research portfolio includes work on alignment theory through <EntityLink id="natural-abstractions">natural abstractions</EntityLink> alongside empirical robustness research.

**Context**: The AI safety field contains diverse views on which research directions are most valuable. Some researchers emphasize near-term robustness and evaluation, while others focus on long-term theoretical alignment challenges.

### Natural Abstractions Theory Concerns

**Criticism**: The <EntityLink id="natural-abstractions">natural abstractions</EntityLink> hypothesis lacks extensive empirical validation. Critics argue that theoretical frameworks should be grounded in experimental evidence before receiving substantial research attention.

**FAR AI Perspective**: Theoretical frameworks guide empirical research programs. The natural abstractions research agenda includes specific empirical tests and predictions. The multi-year timeline for validation is appropriate given the scope of the hypothesis.

**Context**: Disagreement about when to invest in theoretical versus empirical work is common in early-stage scientific fields. Different researchers make different judgments about the appropriate balance.

## Future Directions

### Research Roadmap

| Timeline | Research Focus | Expected Outputs | Success Indicators |
|----------|----------------|------------------|-------------------|
| 2024-2025 | Adversarial robustness scaling | Benchmarks, methods | Adoption by AI labs |
| 2025-2026 | Natural abstractions empirical tests | Experimental results | Theory validation or refinement |
| 2026-2027 | Alignment-robustness integration | Unified frameworks | Safety improvements in deployed systems |
| 2027+ | Policy and governance applications | Recommendations, standards | Regulatory incorporation |

This roadmap represents potential research directions based on the organization's stated interests and current work. Actual research priorities may shift based on field developments, funding opportunities, and new safety challenges.

### Expansion Opportunities

FAR AI's potential areas for expansion include:

- **International Collaboration**: Partnerships with research institutions in Europe and Asia could broaden the organization's network and access to diverse technical perspectives. International collaboration also supports the global coordination needed for AI governance.

- **Policy Research**: The organization's technical expertise in robustness and evaluation could inform <EntityLink id="ai-governance">AI governance</EntityLink> proposals. Translating technical insights into policy-relevant frameworks requires additional expertise in governance and institutional design.

- **Educational Initiatives**: Training programs for early-career researchers could expand the AI safety research community. Educational activities could range from workshops and summer programs to longer-term fellowship models.

- **Tool Development**: Open-source platforms for safety evaluation and robustness testing could increase the impact of FAR AI's methodological work. Public tools enable broader adoption of evaluation practices across the ML community.

These expansion directions involve tradeoffs in organizational focus, resource allocation, and expertise requirements. Growth in policy, education, or tool development would require capabilities beyond the organization's current emphasis on technical research publication.

## Sources & Resources

### Primary Sources

| Source Type | Links | Content |
|-------------|-------|---------|
| Organization Website | [FAR.AI](https://www.far.ai/) | Mission, team, research |
| About Page | [About FAR.AI](https://www.far.ai/about) | Founders, team |
| Research | [FAR.AI Research](https://www.far.ai/research) | Publications, papers |

### Key Research Areas

| Area | Focus | Relevance |
|------|-------|-----------|
| Robustness | Adversarial robustness, safety under distribution shift | Foundation for reliable AI systems |
| Interpretability | Understanding model internals | Alignment verification and debugging |
| Model Evaluation | Safety assessment methods | Standards and benchmarks |
| Alignment | Technical alignment research | Long-term safety |

### Related Organizations

| Organization | Relationship | Context |
|--------------|-------------|---------|
| [UC Berkeley](https://www.berkeley.edu/) | Academic affiliation | Gleave's PhD institution, research collaboration |
| <EntityLink id="chai">CHAI</EntityLink> | Safety research | Berkeley-based alignment research |
| <EntityLink id="miri">MIRI</EntityLink> | Theoretical alignment | Natural abstractions research |
| <EntityLink id="apollo-research">Apollo Research</EntityLink> | Evaluation methods | Benchmark and evaluation development |

### Additional Resources

| Resource Type | Description | Access |
|---------------|-------------|--------|
| FAR.Labs | Berkeley co-working space | [FAR.Labs](https://www.far.ai/labs) |
| Events | Workshops and seminars | [Events](https://www.far.ai/events) |
| Blog | Research updates | [What's New](https://far.ai/post/2023-12-far-overview/) |
