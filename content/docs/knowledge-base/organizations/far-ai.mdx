---
title: FAR AI
description: AI safety research nonprofit founded in 2022 by Adam Gleave and Karl Berzins, focusing on making AI systems safe through technical research and coordination
sidebar:
  order: 16
quality: 32
llmSummary: FAR AI (FAR.AI) is a 2022-founded AI safety research nonprofit led by CEO Adam Gleave and COO Karl Berzins. The organization focuses on technical AI safety research and coordination to ensure safety techniques are adopted.
lastEdited: "2026-02-20"
readerImportance: 84.5
tacticalValue: 68
researchImportance: 53.5
update_frequency: 21
ratings:
  novelty: 2.5
  rigor: 3
  actionability: 2
  completeness: 5
clusters:
  - ai-safety
  - community
subcategory: safety-orgs
entityType: organization
---
import {DataInfoBox, KeyPeople, KeyQuestions, Section, R, EntityLink, DataExternalLinks} from '@components/wiki';

<DataExternalLinks pageId="far-ai" />

<DataInfoBox entityId="E138" />

## Overview

FAR AI ([far.ai](https://www.far.ai/)) is an AI safety research nonprofit founded in July 2022 by Adam Gleave (CEO) and Karl Berzins (COO). Adam Gleave completed his PhD in AI at UC Berkeley, advised by <EntityLink id="stuart-russell">Stuart Russell</EntityLink>. The organization focuses on technical research to make AI systems safer and on coordination to support adoption of safety techniques.

FAR AI conducts technical research in areas including <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink>, model evaluation, and alignment theory. The organization has published work at machine learning conferences including NeurIPS, ICML, and ICLR, and has received funding from sources within the effective altruism ecosystem.

## Key Research Areas

### Adversarial Robustness

| Research Focus | Approach | Safety Connection | Publications |
|----------------|----------|-------------------|--------------|
| <EntityLink id="adversarial-training">Adversarial Training</EntityLink> | Training models to resist adversarial examples | Robust systems as prerequisite for alignment | Multiple top-tier venues |
| Certified Defenses | Mathematical guarantees against attacks | Worst-case safety assurances | NeurIPS, ICML papers |
| Robustness Evaluation | Comprehensive testing against adversarial inputs | Identifying failure modes | Benchmark development |
| Distribution Shift | Performance under novel conditions | Real-world deployment safety | ICLR, AISTATS |

FAR AI's research in <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink> examines how to make AI systems resistant to adversarial examples and maintain performance under distribution shift. This work connects to broader AI safety concerns by addressing the reliability of AI systems under challenging or out-of-distribution conditions. The organization has published research on <EntityLink id="adversarial-training">adversarial training</EntityLink> methods, certified defense mechanisms, and robustness evaluation frameworks.

### Research Programs

FAR AI operates through several key programs:

| Program | Purpose | Details |
|---------|---------|---------|
| FAR.Labs | Co-working space | Berkeley-based AI safety research hub |
| Grant-making | Fund external research | Early-stage safety research funding |
| Events & Workshops | Convene stakeholders | Industry, policy, academic coordination |
| In-house Research | Technical safety work | Robustness, interpretability, alignment |

The grant-making program funds external researchers working on AI safety problems, particularly those in academic settings. FAR.Labs provides physical co-working space and community infrastructure for independent researchers and small teams working on <EntityLink id="technical-research">technical AI safety research</EntityLink>. The events program hosts workshops and convenings that bring together researchers from industry, academia, and policy backgrounds.

### Natural Abstractions Research

| Research Question | Hypothesis | Implications | Status |
|-------------------|------------|--------------|--------|
| Universal Concepts | Intelligent systems discover same abstractions | Shared conceptual basis for alignment | Theoretical development |
| Neural Network Learning | Do NNs learn natural abstractions? | Interpretability foundations | Empirical investigation |
| Alignment Verification | Can we verify shared concepts? | Communication with AI systems | Early research |
| Mathematical Universality | Math/physics as natural abstractions | Foundation for value alignment | Ongoing |

FAR AI has expressed theoretical interest in <EntityLink id="natural-abstractions">natural abstractions</EntityLink> research, which explores whether intelligent systems independently converge on similar conceptual representations of the world. This research direction connects to work by <EntityLink id="miri">MIRI</EntityLink> and other organizations investigating whether shared abstractions between human and AI cognition could provide a foundation for alignment approaches.

## Organizational Structure and Operations

### Research Team

FAR AI's research team is led by CEO Adam Gleave, who brings expertise in reinforcement learning, adversarial robustness, and AI safety from his doctoral work at UC Berkeley. The organization employs researchers with backgrounds in machine learning, AI safety, and related technical fields.

### FAR.Labs Co-working Space

FAR.Labs operates as a Berkeley-based co-working facility for AI safety researchers. The space hosts independent researchers, fellows, and small research teams working on <EntityLink id="safety-research">AI safety</EntityLink> problems. The co-working model provides infrastructure, community, and coordination for researchers working across institutional boundaries.

## Current State & Trajectory

### Research Progress

**Publications**: FAR AI researchers have published work at machine learning conferences including NeurIPS, ICML, and ICLR, focusing on adversarial robustness, model evaluation, and related safety topics.

**Team Growth**: The organization has expanded its research staff and increased the capacity of FAR.Labs to host additional researchers.

**Collaborations**: FAR AI maintains partnerships with academic institutions and other organizations working on AI safety, including collaborations on evaluation methodologies and robustness research.

### Current Metrics

| Metric | Status | Context |
|--------|--------|---------|
| Research Focus | Robustness, interpretability, evaluation, alignment | Active research programs |
| Events Hosted | 10+ | Workshops and convenings |
| Publications | Multiple | Academic venues including NeurIPS, ICML, ICLR |

## Strategic Position Analysis

### Organizational Comparisons

| Organization | Focus | Overlap | Differentiation |
|--------------|-------|---------|----------------|
| <EntityLink id="anthropic">Anthropic</EntityLink> | <EntityLink id="constitutional-ai">Constitutional AI</EntityLink>, scaling | Safety research | Academic publication, no model development |
| <EntityLink id="arc">ARC</EntityLink> | Alignment research | Theoretical alignment | Empirical ML approach |
| <EntityLink id="metr">METR</EntityLink> | Model evaluation | Safety assessment | Robustness specialization |
| Academic Labs | ML research | Technical methods | Organizational safety focus |

FAR AI differs from model-developing organizations like <EntityLink id="anthropic">Anthropic</EntityLink> by focusing on research publication rather than deploying AI systems. Compared to more theoretically-oriented organizations like <EntityLink id="arc">ARC</EntityLink>, FAR AI emphasizes empirical machine learning methods. The organization shares methodological overlap with evaluation-focused groups like <EntityLink id="metr">METR</EntityLink> while maintaining broader research interests.

### Positioning in AI Safety Ecosystem

FAR AI occupies a position between academic AI safety research and industry application. The organization publishes at mainstream machine learning venues, which provides visibility within the broader ML community beyond specialized safety researchers. The grant-making and convening programs connect researchers across institutional boundaries within the AI safety ecosystem.

The organization's focus on <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink> and empirical evaluation connects to near-term safety concerns while maintaining theoretical interest in longer-term alignment challenges through work on <EntityLink id="natural-abstractions">natural abstractions</EntityLink> and related concepts.

## Research Impact and Influence

### Academic Publications

FAR AI researchers publish at top-tier machine learning conferences including NeurIPS (Conference on Neural Information Processing Systems), ICML (International Conference on Machine Learning), and ICLR (International Conference on Learning Representations). Publications cover topics in adversarial robustness, model evaluation, and AI safety.

### Policy Engagement

The organization participates in policy discussions through workshops and convenings that bring together technical researchers with policymakers and governance-focused participants. FAR AI's research on robustness and evaluation methodologies is relevant to ongoing <EntityLink id="ai-governance">AI governance</EntityLink> discussions, though the extent of direct policy influence is not independently documented.

### Community Building

Through FAR.Labs, the organization provides infrastructure for independent researchers and small teams working on various aspects of <EntityLink id="technical-research">technical AI safety</EntityLink>. The co-working space model supports researchers working across institutional boundaries while maintaining focus on safety-relevant technical problems.

FAR AI's events program hosts workshops and convenings connecting researchers from industry, academia, and policy backgrounds around AI safety topics.

## Research Questions and Uncertainties

### Theoretical Questions

Several theoretical questions shape FAR AI's research direction:

- **Natural Abstractions Validity**: Whether intelligent systems independently converge on similar conceptual representations remains an open empirical question. The <EntityLink id="natural-abstractions">natural abstractions</EntityLink> hypothesis has theoretical appeal but requires extensive empirical validation across diverse AI architectures and training regimes.

- **Robustness-Alignment Connection**: The relationship between <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink> and value alignment is not fully understood. While robustness may be necessary for aligned systems, the degree to which robustness research directly contributes to solving alignment problems remains debated within the AI safety community.

- **Scaling Dynamics**: Whether current robustness and evaluation approaches will remain relevant as AI systems increase in capability is uncertain. Some safety researchers argue that qualitatively new challenges emerge at higher capability levels that may not be addressed by current methodologies.

### Organizational Uncertainties

- **Research Timeline**: Academic publication timelines typically span months to years, including peer review, revision, and conference scheduling. Whether this research pace adequately matches the urgency of safety concerns depends on assessments of timelines for transformative AI development.

- **Scope Evolution**: FAR AI's research focus may evolve as the field develops. The organization's emphasis on empirical robustness could shift toward other safety approaches depending on which problems prove most tractable or urgent.

- **Policy Engagement**: The extent of FAR AI's involvement in AI governance and policy discussions may expand beyond its current focus on technical research and convening activities.

### Field-Wide Debates

| Debate | FAR AI Approach | Alternative Views |
|--------|-----------------|-------------------|
| Value of robustness for alignment | Robustness research treated as relevant to safety | Some researchers see limited connection to core alignment |
| Natural abstractions importance | Theoretical interest in the concept | Others view the hypothesis as speculative without strong evidence |
| Academic vs. applied research | Maintains academic publication model | Some argue industry-facing applied research is more impactful |
| Benchmark limitations | Benchmark development as part of research program | Others raise fundamental Goodhart's Law concerns |

These debates reflect broader disagreements within the AI safety community about research priorities, timelines, and the relationship between different technical approaches to safety.

## Funding & Sustainability

### Current Funding Model

FAR AI's funding profile reflects dependence on sources within the effective altruism ecosystem, including <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> and related foundations. This concentration provides mission alignment and relatively stable multi-year funding, while also creating exposure to changes in EA funding priorities or grantmaker assessments of organizational impact.

Government grants, where applicable, typically carry restrictions on fund usage and require administrative overhead, but can provide credibility through competitive peer review. The organization competes with higher-paying industry positions for technical talent, a dynamic common across academic-style AI safety research organizations.

## Criticisms and Responses

### Academic Pace Concerns

**Criticism**: Academic publication processes operate on timelines of months to years, including peer review, revision, and conference scheduling. Critics argue this pace may be too slow given rapid AI capability advances and the urgency of safety concerns.

**Response**: Proponents of the peer-reviewed publication model argue it ensures research quality and credibility, and that methodology and evaluation frameworks developed through careful research provide lasting value even as specific techniques evolve. Preprint sharing and direct collaboration with AI labs can accelerate impact for time-sensitive findings.

**Context**: This tension between research rigor and speed affects the broader AI safety field. Different organizations make different tradeoffs between publication quality, speed, and direct industry impact.

### Limited Scope Questions

**Criticism**: Research on <EntityLink id="adversarial-robustness">adversarial robustness</EntityLink> and evaluation may not directly address core alignment challenges like <EntityLink id="deceptive-alignment">deceptive alignment</EntityLink>, goal specification, or value learning. Critics question whether robustness research provides sufficient traction on harder alignment problems.

**Response**: Proponents argue robustness is a necessary foundation for aligned systems, and that understanding failure modes through adversarial testing and evaluation provides insights for safety more broadly. FAR AI's research portfolio includes theoretical work alongside empirical robustness research.

**Context**: The AI safety field contains diverse views on which research directions are most valuable. Some researchers emphasize near-term robustness and evaluation, while others focus on long-term theoretical alignment challenges.

### Natural Abstractions Theory Concerns

**Criticism**: The <EntityLink id="natural-abstractions">natural abstractions</EntityLink> hypothesis lacks extensive empirical validation. Critics argue that theoretical frameworks should be grounded in experimental evidence before receiving substantial research attention.

**Response**: Proponents argue theoretical frameworks can productively guide empirical research programs, and that the multi-year timeline for validation is appropriate given the scope of the hypothesis.

**Context**: Disagreement about when to invest in theoretical versus empirical work is common in early-stage scientific fields. Different researchers make different judgments about the appropriate balance.

## Sources & Resources

### Primary Sources

| Source Type | Links | Content |
|-------------|-------|---------|
| Organization Website | [FAR.AI](https://www.far.ai/) | Mission, team, research |
| About Page | [About FAR.AI](https://www.far.ai/about) | Founders, team |
| Research | [FAR.AI Research](https://www.far.ai/research) | Publications, papers |

### Key Research Areas

| Area | Focus | Relevance |
|------|-------|-----------|
| Robustness | Adversarial robustness, safety under distribution shift | Foundation for reliable AI systems |
| Interpretability | Understanding model internals | Alignment verification and debugging |
| Model Evaluation | Safety assessment methods | Standards and benchmarks |
| Alignment | Technical alignment research | Long-term safety |

### Related Organizations

| Organization | Relationship | Context |
|--------------|-------------|---------|
| [UC Berkeley](https://www.berkeley.edu/) | Academic affiliation | Gleave's PhD institution, research collaboration |
| <EntityLink id="chai">CHAI</EntityLink> | Safety research | Berkeley-based alignment research |
| <EntityLink id="miri">MIRI</EntityLink> | Theoretical alignment | Natural abstractions research |
| <EntityLink id="apollo-research">Apollo Research</EntityLink> | Evaluation methods | Benchmark and evaluation development |

### Additional Resources

| Resource Type | Description | Access |
|---------------|-------------|--------|
| FAR.Labs | Berkeley co-working space | [FAR.Labs](https://www.far.ai/labs) |
| Events | Workshops and seminars | [Events](https://www.far.ai/events) |
| Blog | Research updates | [What's New](https://far.ai/post/2023-12-far-overview/) |
