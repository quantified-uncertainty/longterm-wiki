---
title: MATS ML Alignment Theory Scholars program
description: A 12-week fellowship program pairing aspiring AI safety researchers with expert mentors in Berkeley and London, training scholars through mentorship, seminars, and independent research projects.
readerImportance: 31.5
researchImportance: 46.5
lastEdited: "2026-02-01"
update_frequency: 21
sidebar:
  order: 50
ratings:
  novelty: 3
  rigor: 6
  actionability: 7
  completeness: 8
quality: 60
llmSummary: MATS is a well-documented 12-week fellowship program that has successfully trained 213 AI safety researchers with strong career outcomes (80% in alignment work) and research impact (160+ publications, 8000+ citations). The program provides comprehensive support ($27k per scholar) and has produced notable alumni who founded organizations like Apollo Research and joined major AI labs.
clusters:
  - community
  - ai-safety
subcategory: safety-orgs
entityType: organization
---
import {EntityLink, KeyPeople, KeyQuestions, Section} from '@components/wiki';

## Quick Assessment

| Aspect | Rating | Notes |
|--------|--------|-------|
| Program Scale | High | 98 scholars and 57 mentors in most recent cohort (MATS 8.0, Summer 2025)[^1] |
| Research Output | Strong | 160+ publications, 8,000+ citations, h-index of 40 over 4 years[^2] |
| Career Impact | Very High | 80% of alumni work in <EntityLink id="alignment">AI alignment</EntityLink>; placements at <EntityLink id="anthropic">Anthropic</EntityLink>, <EntityLink id="openai">OpenAI</EntityLink>, <EntityLink id="deepmind">DeepMind</EntityLink>[^3] |
| Funding per Scholar | \$27k | \$15k stipend + \$12k compute resources, plus housing and meals[^4] |
| Selectivity | Very Competitive | ≈15% acceptance rate; 40+ mentors with independent selection[^5] |

## Key Links

| Source | Link |
|--------|------|
| Official Website | [matsprogram.org](https://matsprogram.org) |
| <EntityLink id="lesswrong">LessWrong</EntityLink> | [lesswrong.com](https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022) |
| EA Forum | [forum.effectivealtruism.org](https://forum.effectivealtruism.org/posts/da8MmRPAB55Fepjjk/my-experience-applying-to-mats-6-0) |

## Overview

The **ML Alignment & Theory Scholars (MATS) Program** is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment, transparency, and security, connecting them with the Berkeley AI safety research community.[^6] Founded in late 2021 and initially run as SERI MATS under the Stanford Existential Risks Initiative, the program later became independent and now operates 12-week in-person cohorts in Berkeley, California and London, United Kingdom.[^7]

MATS pairs scholars with leading researchers in AI safety for approximately 1-2 hours of mentorship per week, supplemented by seminars, workshops, guest lectures, and dedicated research manager support.[^8] The program provides comprehensive support including a \$15,000 living stipend, \$12,000 in compute resources, private housing, catered meals, and office space.[^9] Scholars develop independent research projects that culminate in presentations at a Scholar Symposium, with selected fellows invited to continue for 6-12 month extensions.

Since its founding, MATS has trained over 446 researchers.[^10] The program has generated over 160 research publications with more than 9,000 citations, advancing agendas in <EntityLink id="interpretability">mechanistic interpretability</EntityLink>, sparse feature analysis, activation engineering, and AI safety evaluation.[^11] Alumni have gone on to leading organizations like Anthropic, OpenAI, and Google DeepMind, as well as founded new AI safety organizations like <EntityLink id="apollo-research">Apollo Research</EntityLink>, with 80% of alumni now working in AI alignment, transparency, and security.[^12]

## History

### Founding and Early Development

MATS originated as SERI MATS, an initiative under the Stanford Existential Risks Initiative (SERI) focused on AI safety research training.[^13] The program structure included a 4-week online upskilling phase (10 hours per week), a 2-week research sprint, and an 8-week intensive in-person program in Berkeley, California.[^14] Early mentors included Alex Gray, Beth Barnes, <EntityLink id="evan-hubinger">Evan Hubinger</EntityLink>, John Wentworth, Leo Gao, and Stuart Armstrong.[^15]

The program's core mission from inception was to train talented individuals for AI alignment research by addressing risks from unaligned AI through mentorship, training, logistics, and community access.[^16] The program eventually evolved into an independent organization, maintaining hubs in both Berkeley and London.[^17]

### Program Evolution and Growth

Over its first four years, MATS iterated significantly on its structure and curriculum:

**Summer 2022**: The first cohort produced notable outcomes, including scholars like Johannes Treutlein working under Evan Hubinger, who co-authored papers on predictive models that were later published at the UAI 2023 conference.[^18]

**Summer 2023 (4th Iteration)**: This cohort expanded to 60 scholars and 15 mentors, with 461 applicants (15% acceptance rate for the Training Phase).[^19] The program introduced the Scholar Research Plan (SRP) requiring a threat model, theory of change, and SMART plan, and implemented distinct phases: Training (Alignment 201), Research (Berkeley), and Extension (London/Berkeley).

**Winter 2023-24 (5th Iteration)**: Further growth to 63 scholars and 20 mentors, with a significant curriculum change replacing Alignment 201 with custom curricula due to feedback.[^20] This included <EntityLink id="neel-nanda">Neel Nanda</EntityLink>'s remote mechanistic interpretability curriculum (November 20-December 22) and AI Safety Strategy Discussions.

**MATS 8.0 (Summer 2025)**: The program reached 98 scholars and 57 mentors, concluding with a symposium on August 22, 2025 featuring 10 spotlight talks and a poster session.[^21]

By May 2024, MATS had supported 213 scholars and 47 mentors across five seasonal programs, presenting insights on talent selection and development at the TAIS 2024 conference.[^22]

## Program Structure and Support

### Core Components

MATS operates as a 12-week in-person fellowship with several key elements:

**Mentorship**: Scholars receive approximately 1-2 hours per week of working with their mentor, with more frequent communication via Slack.[^23] Each mentor conducts their own selection process, with some using work tasks and others conducting interviews. Interview topics varied among mentors but commonly included research ideas, career plans, technical machine learning questions, and prior experience, rather than behavioral or mathematical questions.[^24]

**Research Development**: Scholars develop a Research Plan approximately one month into the program, outlining their threat model, theory of change, and specific deliverables.[^25] Dedicated research managers provide support for scoping projects, maintaining progress, and removing obstacles throughout the fellowship.[^26]

**Educational Programming**: The program includes seminars and workshops 2-3 times per week, featuring speakers from organizations like <EntityLink id="redwood-research">Redwood Research</EntityLink>, <EntityLink id="far-ai">FAR AI</EntityLink>, OpenAI, <EntityLink id="chai">CHAI</EntityLink>, and <EntityLink id="govai">GovAI</EntityLink>.[^27] Past speakers have included Buck Shlegeris, Adam Gleave, Neel Nanda, William Saunders, Andrew Critch, Lennart Heim, and Ajeya Cotra.

**Research Tracks**: MATS offers multiple specialization areas including technical governance, empirical research, policy & strategy, theory, and compute governance.[^28]

### Financial and Logistical Support

The program provides comprehensive material support valued at approximately \$35,000 per scholar:[^29]

- \$15,000 stipend for living expenses (provided by AI Safety Support)[^30]
- \$12,000 compute budget for experiments and evaluations[^31]
- Private housing for the full program duration in Berkeley or London[^32]
- Office space access and catered meals[^33]
- Travel reimbursement where applicable

### Extension Opportunities

Selected scholars may continue for an additional 6 or 12 months through extension programs, with London as the main hub, though scholars can also participate from Berkeley, Boston, or Washington D.C. MATS arranges funding to cover monthly stipends and compute resources, and for scholars participating from an AI safety hub, funding also covers housing and office rent.[^34] To be considered, scholars need a strong research project and an endorsement from their mentors, after which an extension selection committee makes final selections.

## Research Impact and Outcomes

### Publications and Citations

Over four years, MATS has produced significant research output, with alumni generating over 160 publications that have received more than 8,000 citations, yielding an organizational h-index of 40.[^36] Notable publications include:

- *Steering Llama 2 via Contrastive Activation Addition* (Outstanding Paper Award at ACL 2024)[^37]
- *Conditioning Predictive Models: Risks and Strategies* (published at UAI 2023)[^38]
- *Incentivizing Honest Performative Predictions with Proper Scoring Rules* (UAI 2023)
- *Neural Networks Learn Statistics of Increasing Complexity*
- *Copy Suppression*, *Inverse Scaling*, *The Reasons That Agents Act*

In a survey of alumni from the first four programs (46% response rate), 78% reported their key publication "possibly" or "probably" would not have happened without MATS, with 10% accelerated by more than 6 months and 14% accelerated by 1-6 months.[^39]

### Research Agendas Developed

MATS scholars have advanced numerous technical agendas in AI safety:

- Sparse auto-encoders for AI interpretability[^40]
- Activation and <EntityLink id="representation-engineering">representation engineering</EntityLink>
- Emergent misalignment detection
- Inoculation prompting techniques
- Developmental interpretability
- Computational mechanics applications
- Glitch token analysis
- <EntityLink id="situational-awareness">Situational awareness</EntityLink> evaluations
- Gradient routing methods
- Externalized reasoning oversight
- Formalizing natural abstractions

These research directions span mechanistic interpretability, sparse feature analysis, and studies of latent representations in AI systems.[^41]

### Career Outcomes

MATS has achieved strong career placement results for alumni:

**Employment**: 49% of surveyed alumni reported working or interning on AI alignment or control, with 29% conducting independent alignment research.[^42] Among earlier cohorts, 39% were hired by research organizations post-MATS, with 50% indicating MATS made them "much more likely" to be hired.[^43] An additional 22% pursued Master's or PhD programs.

**Organizational Placements**: Alumni have joined nearly every major AI safety initiative, including Anthropic, OpenAI, DeepMind, CHAI, and Redwood Research.[^44] Notable examples include:

- Nina (Summer 2023, mentored by Evan Hubinger): Joined Anthropic as a research scientist; won ACL 2024 Outstanding Paper Award; later mentored SPAR and MATS cohorts[^45]
- Marius Hobbhahn (Winter 2022/23, mentored by Evan Hubinger): Founded and became CEO of Apollo Research, a London-based technical alignment organization focused on scheming evaluations and <EntityLink id="ai-control">AI control</EntityLink>[^46]
- Johannes Treutlein (Summer 2022, mentored by Evan Hubinger): Pursued PhD at CHAI; joined Anthropic in 2024 for alignment stress-testing[^47]

**New Organizations**: Alumni have founded new AI safety initiatives including Apollo Research, Cadenza Labs, PRISM Eval, and have organized conferences on singular learning theory and developmental interpretability.[^48]

**Skill Development**: 49% of alumni reported MATS increased their research or technical skills, while 38% gained legible career capital.[^49]

## Key People

### Leadership

**Ryan Kidd** serves as Co-Executive Director of MATS and Co-Founder of the London Initiative for Safe AI (LISA).[^50] He was a scholar in MATS's first iteration (which had only 5 scholars total) and has since become a <EntityLink id="manifund">Manifund</EntityLink> Regrantor and advisor to organizations including Halcyon Futures, Catalyze Impact, AI Safety ANZ, and Pivotal Research.

**Christian Smith** serves as Co-Executive Director and Co-Founder of LISA.[^51] He brings a background in particle physics and pedagogy from Stanford University, having conducted research at CERN and organized educational programs like the Uncommon Sense Seminar.

**Laura Vaughan**, a Thiel Fellow (2017) who studied physics at the University of Waterloo, brings experience in ML model dataset creation and training, management, entrepreneurship, full-stack software engineering, and biomedical research.[^52] She co-founded a stem cell cryogenics startup before joining MATS.

### Notable Mentors

MATS mentors come from leading organizations including Anthropic, Google DeepMind, Redwood Research, OpenAI, <EntityLink id="miri">MIRI</EntityLink>, ARC (<EntityLink id="arc">Alignment Research Center</EntityLink>), CHAI, <EntityLink id="cais">CAIS</EntityLink>, and the Centre on Long-Term Risk.[^53] Selected examples include:

- **Marius Hobbhahn**: CEO of Apollo Research, where he also leads the evals team; Apollo focuses on <EntityLink id="scheming">scheming</EntityLink>, evals, and control; PhD in Bayesian ML; formerly worked on AI forecasting at Epoch[^54]
- **Sam Bowman**: Leads a research group working on AI alignment and welfare at Anthropic, with a particular focus on evaluation; Associate Professor of Computer Science and Data Science at NYU (on leave); has been studying neural network language models since 2012[^55]
- **Joe Benton**: Member of the Alignment Science team at Anthropic, working on <EntityLink id="scalable-oversight">scalable oversight</EntityLink> with interests in control, chain-of-thought monitoring, and alignment evaluations[^56]
- **Arthur Conmy**: Senior Research Engineer at Google DeepMind on the Language Model Interpretability team with Neel Nanda; focus on practically useful interpretability and related AI safety research; previously did early influential work on automating interpretability and finding circuits; formerly at Redwood Research[^57]
- **Evan Hubinger**: Provided mentorship for early SERI MATS trials and multiple cohorts; formerly at MIRI, now at Anthropic[^58]
- **Neel Nanda**: Senior Research Scientist leading the mechanistic interpretability team at Google DeepMind; a returning MATS mentor who has run Training Phases for scholars including live research sessions, lectures on mechanistic interpretability and sparse autoencoders, and reading groups on papers such as *Toy Models of Superposition*; has approximately 50 MATS alumni[^59]

## Funding

MATS receives grants from partner organizations to support its fellowship program.[^60] Financial support for scholars is coordinated through partner organizations rather than directly by MATS:

**Primary Funding Sources**:
- **AI Safety Support**: Provides the \$15,000 stipend for each fellow completing the full program (prorated for partial participation)[^61]
- **MATS-arranged funding**: Covers extension program costs including monthly stipends, compute, housing, and office rent for 6-12 month extensions[^62]
- **<EntityLink id="open-philanthropy">Open Philanthropy</EntityLink>**: Provided grants to support the early SERI MATS trial program, including grants of \$1,008,127 (April 2022), \$1,538,000 (November 2022), and \$428,942 (June 2023)[^63]
- **Other supporters (2024)**: Foresight Institute, <EntityLink id="sff">Survival and Flourishing Fund</EntityLink>, Long-Term Future Fund, Craig Falls, and several donors via Manifund[^64]

**Per-Scholar Investment**: The total cost per scholar is approximately \$35,000 for the full program, based on recent cohorts of 60 scholars and 15 mentors.[^65] This includes the \$15k stipend, compute resources, and costs for housing, meals, office space, and program administration.

**Historical Funding**: In the 2022 SERI MATS program, scholars received \$6,000 after completing the training and research sprint phase and \$16,000 at program completion, with all accommodation, office space, and event expenses covered. Ongoing discretionary funding was also available to promising scholars at the discretion of research mentors.[^66]

## Criticisms and Concerns

While MATS has achieved strong outcomes, program organizers and alumni have identified several concerns and limitations:

### Field Growth Risks

Program organizers acknowledge concerns that MATS's appeal—particularly access to scaling lab mentors—could attract aspiring AI researchers not primarily focused on existential risk reduction, potentially introducing viewpoints that dilute the field's epistemic rigor.[^67] While organizers maintain high selection pressure to prioritize x-risk-motivated scholars, they recognize this tension between growth and field quality as they plan broader advertising.

### Mentorship Dependency and Deference

Critics note that scholars might overly defer to mentors, failing to critically analyze assumptions and reducing independent thinking or new viewpoints in the field.[^68] This concern exists in tension with the opposite problem: insufficient mentorship could lead to excessive peer reliance among inexperienced researchers. MATS rarely accepts scholars without mentors, viewing mentorship as essential for knowledge transfer, which limits scalability and raises barriers since mentors have high entry requirements and capacity constraints.[^69]

### Opportunity Costs for Participants

Alumni feedback highlights specific challenges reported by MATS participants:[^70][^70]

- **Time allocation**: Non-research tasks like writing proposals and preparing talks divert effort from core research
- **Career uncertainty**: One alumnus noted MATS pushed them into technical research with less than 70% confidence it was positive; another preferred their prior ML engineering role for deeper technical challenges
- **Relationship strain**: Some scholars reported impacts on prior commitments, such as strained relationships with PhD supervisors when pausing unrelated work
- **Emotional fit**: Some felt out of place in the AI safety community or experienced slowed involvement
- **Grant stress**: Short-term funding uncertainty led some to doubt their counterfactual impact when applying to AI safety roles

### Selection Challenges

With approximately 15% acceptance rates and 40+ mentors conducting independent selection, even proficient researchers and engineers with AI safety experience frequently receive rejections due to mentor capacity limits rather than candidate quality.[^71] Application processes involve mentor-specific interviews on ML experience, research proposals, conceptual questions, and experiments, with rejections common even after strong interviews.

Alumni feedback indicates that scholars with prior research experience often rate MATS superior to alternatives like independent research or "hub-hopping," though some note they would have preferred later participation after building more ML skills through programs like ARENA.[^72]

## Key Uncertainties

- **Scalability**: Can MATS maintain research quality while expanding beyond current mentor capacity constraints, given the program's emphasis on apprenticeship-style learning?
- **Counterfactual Impact**: What proportion of alumni would have entered AI safety careers through alternative pathways, and how much does MATS accelerate versus redirect talent?
- **Optimal Program Length**: Is the 12-week duration optimal for research skill development, or would longer or shorter programs better serve different scholar populations?
- **Field Dilution Risks**: As MATS expands and advertises more broadly, how can the program maintain epistemic standards while increasing accessibility?
- **Extension Selection**: With ~70% of scholars historically advancing to extensions, what criteria best predict long-term research impact?
- **Mentor-Scholar Matching**: How can the program optimize matching between mentors and scholars to balance deference concerns against knowledge transfer benefits?

## Sources

[^1]: [MATS 8.0 Research Projects (Summer 2025)](https://www.lesswrong.com/posts/3semWY8cZJN3pD66g/mats-8-0-research-projects-summer-2025)
[^2]: [MATS Program Homepage](https://matsprogram.org)
[^3]: [MATS Program Homepage](https://matsprogram.org)
[^4]: [MATS Program Homepage](https://matsprogram.org)
[^5]: [MATS Summer 2023 Retrospective](https://www.lesswrong.com/posts/zwf68YaySvXhWYCdh/mats-summer-2023-retrospective)
[^6]: [MATS Program - LessWrong](https://www.lesswrong.com/w/mats-program)
[^7]: [MATS Summer 2023 Retrospective](https://www.lesswrong.com/posts/zwf68YaySvXhWYCdh/mats-summer-2023-retrospective)
[^8]: [Machine Learning Alignment Theory Scholars - Idealist](https://www.idealist.org/en/nonprofit/4dc1748de64748c581bd62aaa82fb366-machine-learning-alignment-theory-scholars-berkeley)
[^9]: [MATS Program Homepage](https://matsprogram.org)
[^10]: [MATS: A talk on talent selection and development](https://www.youtube.com/watch?v=tA9K8JqyhP4)
[^11]: [MATS Program Homepage](https://matsprogram.org)
[^12]: [MATS Program - Effective Altruism](https://www.effectivealtruism.org/opportunities/recdsVgelkD2qXd3P)
[^13]: [MATS Summer 2023 Retrospective](https://www.lesswrong.com/posts/zwf68YaySvXhWYCdh/mats-summer-2023-retrospective)
[^14]: [SERI ML Alignment Theory Scholars Program 2022](https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022)
[^15]: [SERI ML Alignment Theory Scholars Program 2022](https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022)
[^16]: [Machine Learning Alignment Theory Scholars - Idealist](https://www.idealist.org/en/nonprofit/4dc1748de64748c581bd62aaa82fb366-machine-learning-alignment-theory-scholars-berkeley)
[^17]: [MATS Summer 2023 Retrospective](https://www.lesswrong.com/posts/zwf68YaySvXhWYCdh/mats-summer-2023-retrospective)
[^18]: [MATS Alumni](https://matsprogram.org/alumni)
[^19]: [MATS Summer 2023 Retrospective](https://www.lesswrong.com/posts/zwf68YaySvXhWYCdh/mats-summer-2023-retrospective)
[^20]: [MATS Winter 2023-24 Retrospective](https://forum.effectivealtruism.org/posts/Cz4tE2zwcwwakqBo8/mats-winter-2023-24-retrospective)
[^21]: [MATS 8.0 Research Projects](https://substack.com/home/post/p-171758976)
[^22]: [MATS: A talk on talent selection and development](https://www.youtube.com/watch?v=tA9K8JqyhP4)
[^23]: [Machine Learning Alignment Theory Scholars - Idealist](https://www.idealist.org/en/nonprofit/4dc1748de64748c581bd62aaa82fb366-machine-learning-alignment-theory-scholars-berkeley)
[^24]: [My experience applying to MATS 6.0](https://forum.effectivealtruism.org/posts/da8MmRPAB55Fepjjk/my-experience-applying-to-mats-6-0)
[^25]: [MATS Program Homepage](https://matsprogram.org)
[^26]: [MATS Program Homepage](https://matsprogram.org)
[^27]: [Machine Learning Alignment Theory Scholars - Idealist](https://www.idealist.org/en/nonprofit/4dc1748de64748c581bd62aaa82fb366-machine-learning-alignment-theory-scholars-berkeley)
[^28]: [MATS Summer 2026 Program](https://matsprogram.org/program/summer-2026)
[^29]: [MATS Funding - Manifund](https://manifund.org/projects/mats-funding)
[^30]: [MATS Program Homepage](https://matsprogram.org)
[^31]: [MATS Program Homepage](https://matsprogram.org)
[^32]: [MATS Program Homepage](https://matsprogram.org)
[^33]: [MATS Program Homepage](https://matsprogram.org)
[^34]: [MATS FAQ](https://matsprogram.org/faq)
[^35]: [MATS Summer 2026 Program](https://matsprogram.org/program/summer-2026)
[^36]: [MATS Program Homepage](https://matsprogram.org)
[^37]: [MATS Alumni Impact Analysis](https://www.lesswrong.com/posts/jeBkx6agMuBCQW94C/mats-alumni-impact-analysis)
[^38]: [MATS Alumni Impact Analysis](https://www.lesswrong.com/posts/jeBkx6agMuBCQW94C/mats-alumni-impact-analysis)
[^39]: [MATS Alumni Impact Analysis](https://www.lesswrong.com/posts/jeBkx6agMuBCQW94C/mats-alumni-impact-analysis)
[^40]: [MATS Program Homepage](https://matsprogram.org)
[^41]: [MATS Program Homepage](https://matsprogram.org)
[^42]: [MATS Alumni Impact Analysis](https://www.lesswrong.com/posts/jeBkx6agMuBCQW94C/mats-alumni-impact-analysis)
[^43]: [MATS: A talk on talent selection and development](https://www.youtube.com/watch?v=tA9K8JqyhP4)
[^44]: [MATS: A talk on talent selection and development](https://www.youtube.com/watch?v=tA9K8JqyhP4)
[^45]: [MATS Alumni](https://matsprogram.org/alumni)
[^46]: [MATS Alumni](https://matsprogram.org/alumni)
[^47]: [MATS Alumni](https://matsprogram.org/alumni)
[^48]: [MATS Alumni Impact Analysis](https://www.lesswrong.com/posts/jeBkx6agMuBCQW94C/mats-alumni-impact-analysis)
[^49]: [MATS Alumni Impact Analysis](https://www.lesswrong.com/posts/jeBkx6agMuBCQW94C/mats-alumni-impact-analysis)
[^50]: [Ryan Kidd - TAIS 2024](https://tais2024.cc/agenda/ryan-kidd/)
[^51]: [MATS Team](https://matsprogram.org/team)
[^52]: [MATS Winter 2023-24 Retrospective](https://forum.effectivealtruism.org/posts/Cz4tE2zwcwwakqBo8/mats-winter-2023-24-retrospective)
[^53]: [MATS Mentors](https://matsprogram.org/mentors)
[^54]: [MATS Mentors](https://matsprogram.org/mentors)
[^55]: [MATS Mentors](https://matsprogram.org/mentors)
[^56]: [MATS Mentors](https://matsprogram.org/mentors)
[^57]: [MATS Mentors](https://matsprogram.org/mentors)
[^58]: [ML Alignment Theory Program under Evan Hubinger](https://www.alignmentforum.org/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger)
[^59]: [MATS Winter 2023-24 Retrospective](https://www.lesswrong.com/posts/Z87fSrxQb4yLXKcTk/mats-winter-2023-24-retrospective)
[^60]: [MATS Funding - Extruct](https://www.extruct.ai/hub/matsprogram-org-funding/)
[^61]: [MATS Program Homepage](https://matsprogram.org)
[^62]: [MATS FAQ](https://matsprogram.org/faq)
[^63]: [ML Alignment Theory Program under Evan Hubinger](https://www.alignmentforum.org/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger)
[^64]: [MATS Team](https://matsprogram.org/team)
[^65]: [MATS Funding - Manifund](https://manifund.org/projects/mats-funding)
[^66]: [SERI ML Alignment Theory Scholars Program 2022](https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022)
[^67]: [How MATS addresses mass movement building concerns](https://www.lesswrong.com/posts/iD6tYjLLFFn4LXgnt/how-mats-addresses-mass-movement-building-concerns)
[^68]: [How MATS addresses mass movement building concerns](https://www.lesswrong.com/posts/iD6tYjLLFFn4LXgnt/how-mats-addresses-mass-movement-building-concerns)
[^69]: [How MATS addresses mass movement building concerns](https://www.lesswrong.com/posts/iD6tYjLLFFn4LXgnt/how-mats-addresses-mass-movement-building-concerns)
[^70]: [MATS Alumni Impact Analysis - EA Forum](https://forum.effectivealtruism.org/posts/kJA9q3SGycx6TXjcF/mats-alumni-impact-analysis)
[^71]: [My experience applying to MATS 6.0](https://forum.effectivealtruism.org/posts/da8MmRPAB55Fepjjk/my-experience-applying-to-mats-6-0)
[^72]: [MATS Alumni Impact Analysis - EA Forum](https://forum.effectivealtruism.org/posts/kJA9q3SGycx6TXjcF/mats-alumni-impact-analysis)

