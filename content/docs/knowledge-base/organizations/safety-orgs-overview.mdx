---
numericId: "E821"
title: "AI Safety Organizations (Overview)"
description: "Overview of organizations focused on AI safety research, policy, and advocacy—from dedicated alignment labs to think tanks and field-building institutions working to reduce catastrophic and existential risks from advanced AI systems."
sidebar:
  label: "Overview"
  order: 0
subcategory: "safety-orgs"
entityType: "overview"
readerImportance: 52.3
tacticalValue: 78
quality: 48
llmSummary: "A well-organized reference overview of ~20 AI safety organizations categorized by function (alignment research, policy, field-building), with a comparative budget/headcount table showing estimated annual budgets of $3-10M and cost-per-researcher of $143K-$400K across nine major orgs, all primarily funded by Open Philanthropy. The page is a competent compilation with useful quantitative estimates but offers little original analysis beyond organizing publicly available information."
ratings:
  focus: 8.5
  novelty: 3.5
  rigor: 4.5
  completeness: 6
  concreteness: 6.5
  actionability: 4.5
  objectivity: 6.5
clusters:
  - ai-safety
  - community
---
import {EntityLink} from '@components/wiki';

## Overview

The AI safety organizational landscape spans dedicated alignment research labs, policy think tanks, advocacy groups, and field-building institutions. These organizations aim to reduce catastrophic and existential risks from advanced AI systems through technical research, governance advocacy, talent development, and public engagement.

Funding is heavily concentrated through a small number of major funders, most prominently <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink>, which has provided grants to the majority of organizations listed on this page. This concentration produces a relatively coordinated funding environment, with most grantees sharing compatible research agendas and norms, while also reducing diversification of funding sources across the field.

*Note: The AI safety organizational landscape evolves rapidly. Headcount, budget, and focus area descriptions reflect available information as of mid-2025 and may not capture recent changes. Check individual entity pages for the most current details.*

## Alignment Research Labs

Dedicated organizations conducting technical AI safety research:

- **<EntityLink id="arc">ARC (Alignment Research Center)</EntityLink>**: Founded by <EntityLink id="paul-christiano">Paul Christiano</EntityLink>; focuses on alignment evaluation and theoretical alignment research
- **<EntityLink id="metr">METR</EntityLink>**: Evaluates dangerous capabilities in frontier AI models; spun out of ARC
- **<EntityLink id="apollo-research">Apollo Research</EntityLink>**: Focuses on detecting and understanding deceptive AI behavior, including <EntityLink id="scheming">scheming</EntityLink> evaluations
- **<EntityLink id="redwood-research">Redwood Research</EntityLink>**: Alignment research lab working on <EntityLink id="interpretability">interpretability</EntityLink>, <EntityLink id="adversarial-training">adversarial training</EntityLink>, and <EntityLink id="ai-control">AI control</EntityLink>
- **<EntityLink id="conjecture">Conjecture</EntityLink>**: Alignment research and product company based in London
- **<EntityLink id="far-ai">FAR AI</EntityLink>**: Researches robustness, adversarial attacks, and alignment failures in AI systems
- **<EntityLink id="palisade-research">Palisade Research</EntityLink>**: Focuses on practical AI safety evaluation and red-teaming
- **<EntityLink id="seldon-lab">Seldon Lab</EntityLink>**: Works on alignment approaches and safety evaluations
- **<EntityLink id="goodfire">Goodfire</EntityLink>**: Interpretability-focused startup building tools for understanding neural networks
- **<EntityLink id="miri">MIRI (Machine Intelligence Research Institute)</EntityLink>**: Pioneer in AI <EntityLink id="alignment">alignment</EntityLink> theory; founded 2000

## Policy and Governance Organizations

Think tanks and research centers focused on AI governance and policy:

- **<EntityLink id="govai">GovAI</EntityLink>**: Research center focused on <EntityLink id="tmc-ai-governance">AI governance</EntityLink> based at Oxford
- **<EntityLink id="cset">CSET (Center for Security and Emerging Technology)</EntityLink>**: Georgetown think tank producing policy-relevant research on AI and emerging technologies
- **<EntityLink id="cser">CSER (Centre for the Study of Existential Risk)</EntityLink>**: Cambridge-based research center studying existential risks including from AI
- **<EntityLink id="secure-ai-project">Secure AI Project</EntityLink>**: Advocacy organization focused on AI safety policy
- **<EntityLink id="controlai">ControlAI</EntityLink>**: Advocacy organization pushing for stronger AI regulation and safety standards
- **<EntityLink id="pause-ai">Pause AI</EntityLink>**: Grassroots advocacy movement calling for a pause on frontier AI development
- **<EntityLink id="frontier-model-forum">Frontier Model Forum</EntityLink>**: Industry-led consortium for frontier AI safety, founded by <EntityLink id="anthropic">Anthropic</EntityLink>, <EntityLink id="deepmind">Google DeepMind</EntityLink>, Microsoft, and <EntityLink id="openai">OpenAI</EntityLink>. The forum's stated mission centers on safety research and best-practice sharing; observers differ on the extent to which it functions as a coordination body versus an industry advocacy vehicle

## Field-Building and Talent Development

Organizations supporting the growth of the AI safety field:

- **<EntityLink id="80000-hours">80,000 Hours</EntityLink>**: Career advisory organization directing talent toward high-impact careers including AI safety
- **<EntityLink id="mats">MATS (ML Alignment Theory Scholars)</EntityLink>**: Training program connecting aspiring alignment researchers with mentors
- **<EntityLink id="lightning-rod-labs">Lightning Rod Labs</EntityLink>**: Works on AI safety infrastructure and tooling
- **<EntityLink id="ai-futures-project">AI Futures Project</EntityLink>**: Research and analysis on AI development trajectories and safety considerations

## Research and Analysis

Organizations focused on understanding AI progress and risks:

- **<EntityLink id="epoch-ai">Epoch AI</EntityLink>**: Tracks AI compute trends, model capabilities, and training data
- **<EntityLink id="cais">CAIS (Center for AI Safety)</EntityLink>**: Conducts safety research and field-building for AI safety; hosts a compute cluster for safety research
- **<EntityLink id="chai">CHAI (Center for Human-Compatible AI)</EntityLink>**: UC Berkeley research center founded by <EntityLink id="stuart-russell">Stuart Russell</EntityLink> focusing on human-compatible AI

## Budget and Headcount Comparison

For funders and researchers evaluating organizational capacity and capital efficiency, comparative budget and headcount data can help identify where additional resources may be most impactful and how different organizations structure their research operations. The table below aggregates publicly available estimates across nine prominent independent AI safety organizations.

*All figures are estimates derived from IRS Form 990 filings (via ProPublica Nonprofit Explorer), Open Philanthropy grant disclosures, LinkedIn headcount data, and news reports. Figures are approximate, may lag actual values by one to two years, and should be treated as indicative rather than authoritative. The "Est. Budget per Staff Member/year" column is calculated using the midpoint of the headcount range and counts all staff, not researchers only.*

| Organization | Annual Budget (Est.) | Headcount (Est.) | Est. Budget per Staff Member/year (Est.) | Primary Funder | Focus Area |
|---|---|---|---|---|---|
| <EntityLink id="miri">MIRI</EntityLink> | ≈\$5M | 10–15 | ≈\$400K | <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> | Alignment theory |
| <EntityLink id="arc">ARC</EntityLink> | ≈\$8M | 20–30 | ≈\$320K | <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> | Alignment research & evaluation |
| <EntityLink id="metr">METR</EntityLink> | ≈\$5M | 20–30 | ≈\$200K | <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> | Dangerous capability evaluation |
| <EntityLink id="cais">CAIS</EntityLink> | ≈\$5M | 15–20 | ≈\$286K | <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> | Research & field-building |
| <EntityLink id="redwood-research">Redwood Research</EntityLink> | ≈\$10M | 30–40 | ≈\$286K | <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> | Interpretability & AI control |
| <EntityLink id="apollo-research">Apollo Research</EntityLink> | ≈\$4M | 15–20 | ≈\$229K | <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> | Deceptive alignment & scheming |
| <EntityLink id="conjecture">Conjecture</EntityLink> | ≈\$5M | 30–40 | ≈\$143K | Mixed (VC + grants) | Alignment research & products |
| <EntityLink id="far-ai">FAR AI</EntityLink> | ≈\$3M | 10–15 | ≈\$240K | <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> | Robustness & adversarial ML |
| <EntityLink id="govai">GovAI</EntityLink> | ≈\$5M | 20–30 | ≈\$200K | <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> | AI governance & policy |

The budget-per-staff figures reflect meaningful variation in organizational structure. Organizations with lower ratios (e.g., Conjecture) typically employ a higher proportion of non-researcher staff or operate hybrid research-product models, whereas those with higher ratios (e.g., MIRI) tend toward smaller, senior-heavy research teams. These figures should not be interpreted as proxies for research quality or output volume.

## Key Patterns

**Specialization trend**: The field has moved from generalist safety organizations—such as <EntityLink id="miri">MIRI</EntityLink> and the <EntityLink id="fhi">Future of Humanity Institute</EntityLink> (FHI, which closed in 2024)—toward more specialized roles: dedicated evaluation labs (<EntityLink id="metr">METR</EntityLink>, <EntityLink id="apollo-research">Apollo Research</EntityLink>), interpretability startups (<EntityLink id="goodfire">Goodfire</EntityLink>), policy research centers (<EntityLink id="controlai">ControlAI</EntityLink>, <EntityLink id="govai">GovAI</EntityLink>), and talent pipelines (<EntityLink id="mats">MATS</EntityLink>, <EntityLink id="80000-hours">80,000 Hours</EntityLink>).

**Industry-adjacent positioning**: Organizations in this landscape occupy a range of positions relative to frontier AI developers. Some—such as the <EntityLink id="frontier-model-forum">Frontier Model Forum</EntityLink>, <EntityLink id="redwood-research">Redwood Research</EntityLink>, and <EntityLink id="apollo-research">Apollo Research</EntityLink>—maintain active collaborative relationships with frontier labs. Others, including <EntityLink id="pause-ai">Pause AI</EntityLink> and <EntityLink id="controlai">ControlAI</EntityLink>, advocate for regulatory constraints on AI development and position themselves independently of industry partnerships. Proponents of each approach offer different accounts of how safety outcomes are best achieved.

**Funding concentration**: As illustrated in the budget table above, most organizations in this cluster report <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> as their primary funder. This pattern is visible across alignment research, governance research, and field-building organizations alike.
