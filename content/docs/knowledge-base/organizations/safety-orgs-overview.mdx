---
numericId: E687
title: AI Safety Organizations
description: Overview of organizations focused on AI safety research, policy, and advocacy—from dedicated alignment labs to think tanks and field-building institutions working to reduce catastrophic and existential risks from advanced AI systems.
sidebar:
  label: Overview
  order: 0
subcategory: safety-orgs
---
import {EntityLink} from '@components/wiki';

## Overview

The AI safety organizational landscape spans dedicated alignment research labs, policy think tanks, advocacy groups, and field-building institutions. These organizations collectively work to reduce catastrophic and existential risks from advanced AI systems through technical research, governance advocacy, talent development, and public engagement.

The field has grown rapidly since 2020, with several organizations founded specifically to address frontier AI risks. Funding is heavily concentrated through <EntityLink id="E552">Coefficient Giving</EntityLink> (formerly Open Philanthropy) and a small number of other longtermist-aligned funders.

## Alignment Research Labs

Dedicated organizations conducting technical AI safety research:

- **<EntityLink id="E25">ARC (Alignment Research Center)</EntityLink>**: Founded by Paul Christiano; focuses on alignment evaluation and theoretical alignment research
- **<EntityLink id="E201">METR</EntityLink>**: Evaluates dangerous capabilities in frontier AI models; spun out of ARC
- **<EntityLink id="E24">Apollo Research</EntityLink>**: Focuses on detecting and understanding deceptive AI behavior, including scheming evaluations
- **<EntityLink id="E557">Redwood Research</EntityLink>**: Alignment research lab working on interpretability, adversarial training, and AI control
- **<EntityLink id="E70">Conjecture</EntityLink>**: Alignment research and product company based in London
- **<EntityLink id="E138">FAR AI</EntityLink>**: Researches robustness, adversarial attacks, and alignment failures in AI systems
- **<EntityLink id="E428">Palisade Research</EntityLink>**: Focuses on practical AI safety evaluation and red-teaming
- **<EntityLink id="E565">Seldon Lab</EntityLink>**: Works on alignment approaches and safety evaluations
- **<EntityLink id="E430">Goodfire</EntityLink>**: Interpretability-focused startup building tools for understanding neural networks
- **<EntityLink id="E202">MIRI (Machine Intelligence Research Institute)</EntityLink>**: Pioneer in AI alignment theory; founded 2000

## Policy and Governance Organizations

Think tanks and research centers focused on AI governance and policy:

- **<EntityLink id="E153">GovAI</EntityLink>**: Research center focused on AI governance based at Oxford
- **<EntityLink id="E524">CSET (Center for Security and Emerging Technology)</EntityLink>**: Georgetown think tank producing policy-relevant research on AI and emerging technologies
- **<EntityLink id="E523">CSER (Centre for the Study of Existential Risk)</EntityLink>**: Cambridge-based research center studying existential risks including from AI
- **<EntityLink id="E562">Secure AI Project</EntityLink>**: Advocacy organization focused on AI safety policy
- **<EntityLink id="E426">ControlAI</EntityLink>**: Advocacy organization pushing for stronger AI regulation and safety standards
- **<EntityLink id="E553">Pause AI</EntityLink>**: Grassroots advocacy movement calling for a pause on frontier AI development
- **<EntityLink id="E427">Frontier Model Forum</EntityLink>**: Industry consortium for frontier AI safety (founded by Anthropic, Google, Microsoft, OpenAI)

## Field-Building and Talent Development

Organizations supporting the growth of the AI safety field:

- **<EntityLink id="E510">80,000 Hours</EntityLink>**: Career advisory organization directing talent toward high-impact careers including AI safety
- **<EntityLink id="E548">MATS (ML Alignment Theory Scholars)</EntityLink>**: Training program connecting aspiring alignment researchers with mentors
- **<EntityLink id="E540">Lightning Rod Labs</EntityLink>**: Works on AI safety infrastructure and tooling
- **<EntityLink id="E511">AI Futures Project</EntityLink>**: Research and analysis on AI development trajectories and safety considerations

## Research and Analysis

Organizations focused on understanding AI progress and risks:

- **<EntityLink id="E125">Epoch AI</EntityLink>**: Tracks AI compute trends, model capabilities, and training data
- **<EntityLink id="E47">CAIS (Center for AI Safety)</EntityLink>**: Research and field-building for AI safety; hosts compute cluster for safety research
- **<EntityLink id="E57">CHAI (Center for Human-Compatible AI)</EntityLink>**: UC Berkeley research center founded by Stuart Russell focusing on human-compatible AI

## Key Patterns

**Specialization trend**: The field has moved from generalist safety organizations (MIRI, FHI) toward more specialized roles—dedicated evaluation labs (METR, Apollo), interpretability startups (Goodfire), policy advocates (ControlAI, Pause AI), and talent pipelines (MATS, 80,000 Hours).

**Industry-adjacent positioning**: Several organizations (Frontier Model Forum, Redwood Research, Apollo Research) work closely with frontier AI labs, while others (Pause AI, ControlAI) adopt more adversarial stances toward the AI industry.

**Funding concentration**: Most organizations in this cluster depend heavily on a small number of funders, particularly <EntityLink id="E552">Coefficient Giving</EntityLink>, creating both coordination benefits and single-point-of-failure risks.
