---
numericId: E874
title: Longtermism's Philosophical Credibility After FTX
description: An examination of how the FTX collapse affected longtermism's
  standing as a philosophical movement, including reputational damage, funding
  disruptions, and ongoing debates about the ideology's core claims.
importance: 58
lastEdited: "2026-02-20"
subcategory: ea-history
sidebar:
  order: 50
ratings:
  novelty: 4
  rigor: 6
  actionability: 3
  completeness: 7
readerImportance: 58
tacticalValue: 52
quality: 50
llmSummary: "This article examines the dual impact of the FTX collapse on
  longtermism: severe reputational and funding damage (including $160M in lost
  Future Fund commitments and FHI's 2024 closure), alongside pre-existing and
  continuing philosophical critiques about probability cluelessness,
  ends-justify-means reasoning, and power concentration that remain unresolved
  regardless of SBF's misconduct. The article concludes that both the
  reputational and philosophical questions remain genuinely open, with defenders
  maintaining the fraud reflects individual failure rather than ideological
  corruption."
balanceFlags:
  - missing-primary-sources
entityType: historical
---
import {EntityLink, R, F, Calc, DataInfoBox, DataExternalLinks} from '@components/wiki';

## Quick Assessment

| Dimension | Assessment |
|-----------|-----------|
| **Philosophical Status** | Contested; core claims remain defended but face renewed scrutiny |
| **Reputational Damage** | Severe in late 2022–2023; partial recovery ongoing |
| **Funding Impact** | \$160M in FTX Future Fund commitments lost; Open Philanthropy paused then resumed longtermist grantmaking in early 2023 |
| **Institutional Status** | Mixed; Future of Humanity Institute closed April 2024; Global Priorities Institute continues with Open Philanthropy support |
| **Ongoing Credibility** | Defended by MacAskill and others; challenged on independent philosophical grounds in peer-reviewed literature |

## Key Links

| Source | Link |
|--------|------|
| Radical Philosophy critique (Crary, 2023) | [radicalphilosophy.com](https://www.radicalphilosophy.com/commentary/the-toxic-ideology-of-longtermism) |
| Wikipedia (Effective Altruism) | [en.wikipedia.org](https://en.wikipedia.org/wiki/Effective_altruism) |
| Wikipedia (Longtermism) | [en.wikipedia.org](https://en.wikipedia.org/wiki/Longtermism) |
| 80,000 Hours introduction | [80000hours.org](https://80000hours.org/articles/future-generations/) |
| Torres, Aeon critique (Oct 2021) | [aeon.co](https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo) |
| MacAskill, EA Forum statement (Nov 2022) | [forum.effectivealtruism.org](https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx) |
| Open Philanthropy 2023 progress report | [openphilanthropy.org](https://www.openphilanthropy.org/research/our-progress-in-2023-and-plans-for-2024/) |
| FHI Final Report (Apr 2024) | [forum.effectivealtruism.org](https://forum.effectivealtruism.org/posts/uK27pds7J36asqJPt/future-of-humanity-institute-2005-2024-final-report) |

## Overview

Longtermism is the ethical view that positively influencing the long-term future of humanity constitutes a key moral priority of our time. Developed most prominently by philosophers William MacAskill and <EntityLink id="toby-ord">Toby Ord</EntityLink> within the <EntityLink id="cea">Centre for Effective Altruism</EntityLink> ecosystem, the philosophy holds that future generations deserve equal moral consideration to those alive today, that the potential scale of future human lives dwarfs the present population, and that actions reducing existential risks—such as AI misalignment, engineered pandemics, or nuclear conflict—carry substantial expected value as a result.[^1] Its institutional expression included the <EntityLink id="fhi">Future of Humanity Institute</EntityLink> at Oxford, the Global Priorities Institute, and a network of researchers and grantmakers funded substantially through <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> and, critically, through <EntityLink id="sam-bankman-fried">Sam Bankman-Fried</EntityLink>'s <EntityLink id="ftx-future-fund">FTX Future Fund</EntityLink>.[^2]

The collapse of <EntityLink id="ftx">FTX</EntityLink> in November 2022 and Bankman-Fried's subsequent conviction on fraud charges precipitated a significant crisis for the longtermist movement. Bankman-Fried had been one of longtermism's most prominent public funders, pledging the bulk of his fortune—which peaked at approximately \$16 billion—to effective altruist and longtermist causes.[^3] The <EntityLink id="ftx-future-fund">FTX Future Fund</EntityLink> had committed \$160 million to longtermist researchers and organizations, funding that became unrecoverable when the exchange filed for bankruptcy.[^4] The scandal prompted intense public debate about whether longtermism's philosophical framework had, in some sense, licensed or enabled the misconduct—or whether the ideology simply suffered guilt by association with a fraudulent donor.

The question of philosophical credibility after FTX splits along two axes. The first concerns reputational damage: the movement's public standing, institutional health, and ability to attract funding and talent. The second is more fundamental: whether critics had identified genuine theoretical weaknesses in longtermism that the scandal brought into sharper relief, or whether the philosophy's core arguments remain intact regardless of who funds them. Both debates remain unresolved, and academic scrutiny of longtermism's foundations—including a Leverhulme Trust–funded research project at the University of Bristol announced in 2024 and multiple peer-reviewed articles published in *Ethics*, *Philosophy and Public Affairs*, and *Synthese*—continues.[^5]

## History and Background

### Origins in Effective Altruism

Longtermism emerged from the effective altruism (EA) movement, which applies utilitarian-influenced reasoning to maximize the good achievable through philanthropy and career choices. Peter Singer's work on the moral obligations of affluent individuals toward distant strangers provided the philosophical foundation. During the 2010s, EA expanded its focus to incorporate longer-horizon concerns alongside near-term interventions such as global health and poverty alleviation.[^6] MacAskill is credited with coining the term "longtermism" in 2017, and the movement crystallized around two major texts: <EntityLink id="toby-ord">Toby Ord</EntityLink>'s *The Precipice: Existential Risk and the Future of Humanity* (2020) and MacAskill's *What We Owe the Future* (2022).[^7]

The institutional infrastructure that developed around this philosophical shift was substantial. By 2021, the EA movement had accumulated an estimated \$46 billion in dedicated funding, much of it directed toward existential risk research.[^8] The <EntityLink id="fhi">Future of Humanity Institute</EntityLink> and the Global Priorities Institute (GPI) at Oxford served as academic homes for longtermist research, while <EntityLink id="80000-hours">80,000 Hours</EntityLink> steered early-career professionals toward longtermist cause areas including AI safety and biosecurity.

### Sam Bankman-Fried and FTX's Role

Bankman-Fried's relationship with EA began in 2012 when MacAskill advised him to pursue "earning to give"—a strategy of maximizing income in order to donate at scale—as an effective altruistic path.[^9] Bankman-Fried went on to found the cryptocurrency exchange FTX and became one of the movement's most prominent funders. He pledged to donate the overwhelming majority of his fortune to EA and longtermist causes and established the <EntityLink id="ftx-future-fund">FTX Future Fund</EntityLink> as the vehicle for this giving.

Bankman-Fried described himself primarily as a consequentialist utilitarian rather than a longtermist specifically. By his own account, he was "a total, act, hedonistic/one level (as opposed to high and low pleasure), classical (as opposed to negative) utilitarian."[^34] The <EntityLink id="ftx-future-fund">FTX Future Fund</EntityLink>, however, was explicitly organized around longtermist grant priorities: it set out to make grants based on longtermist ideas, with humanity's overriding ethical aim framed as protecting future generations.[^35] By 2022, approximately 40% of EA's funding was directed toward longtermist causes, much of it toward AI safety.[^35] This distinction matters for evaluating how directly Bankman-Fried's conduct implicates longtermism specifically versus EA's broader utilitarian framework—critics and defenders have drawn different conclusions from the same facts.

The FTX Future Fund committed \$160 million to longtermist projects before the exchange's collapse.[^4] MacAskill served on the Future Fund's advisory board and had directed \$36.5 million to organizations he co-founded.[^36] When FTX filed for bankruptcy in November 2022 amid revelations that customer funds had been misappropriated—allegedly to support affiliated trading firm <EntityLink id="ftx">FTX</EntityLink>-linked Alameda Research—the Future Fund team, including its director <EntityLink id="nick-beckstead">Nick Beckstead</EntityLink>, resigned. FTX's bankruptcy CEO John J. Ray III compared the collapse to Enron, citing failures of corporate controls.[^11] Bankman-Fried was subsequently convicted on multiple counts of wire fraud and securities fraud.

## The Reputational Fallout

The immediate consequences for longtermism's public standing were substantial. On November 12, 2022, MacAskill published a statement on the EA Forum acknowledging the damage directly. He wrote that he did not know which emotion was stronger: his "utter rage at Sam (and others?) for causing such harm to so many people," or his "sadness and self-hatred for falling for this deception." He also tweeted "I cannot in words convey how strongly I condemn what they did," then went largely silent on the platform until June 2023.[^12] MacAskill argued in his statement that *What We Owe the Future* explicitly opposed "ends justify the means" reasoning, citing relevant passages in his own defense.[^12]

Critics contested this framing, arguing it "admits that they were duped by an unethical huckster, but denies that there is any serious flaw in the movement itself," treating SBF's conduct as "simply an unhappy coincidence that Samuel Bankman-Fried was associated with them."[^37] <EntityLink id="80000-hours">80,000 Hours</EntityLink> expressed regret at having placed trust in Bankman-Fried and acknowledged the organization was grappling with the lessons of the collapse.[^13]

Critics argued the scandal revealed something structural rather than merely incidental. Longtermism's movement had grown to depend on a small number of extremely wealthy donors—a concentration of philanthropic power that some argued left it vulnerable to the ethical failures of individual actors and insulated from the democratic accountability that might otherwise have constrained them.[^14] The movement's funding apparatus, critics noted, channeled substantial resources into EA institution-building and movement growth; Émile Torres observed that the EA movement had "\$46.1 billion in committed funding" before the FTX collapse, with billions remaining dedicated to longtermist efforts even after.[^15]

A TIME Magazine investigation reported in March 2023 that some EA leaders had received warnings about concerns regarding Bankman-Fried's conduct years before the collapse.[^16] EA community responses disputed the significance of those warnings, arguing they were not specific enough to warrant action; the degree of prior knowledge and its implications for institutional accountability remain contested.[^16]

Peter Singer, whose philosophical work had inspired EA's founding generation, acknowledged that the reputational damage from the FTX collapse was substantial and expressed uncertainty about the movement's near-term recovery prospects.[^17]

### Institutional Consequences

The <EntityLink id="fhi">Future of Humanity Institute</EntityLink> closed on April 16, 2024. The closure resulted primarily from decisions by Oxford's Faculty of Philosophy, which in 2020 imposed a freeze on FHI's fundraising and hiring. That freeze led to the loss of lead researchers and a promising cohort of junior researchers. In late 2023, the Faculty announced that the contracts of remaining FHI staff would not be renewed. Anders Sandberg, a senior FHI researcher, described the process as "a gradual suffocation by Faculty bureaucracy."[^38] The institute's largest team, the Governance of AI Program, had already spun out of the university in 2021 to escape bureaucratic restrictions and became an independent organization.[^39] Staff also suspected that an interdepartmental transfer plan—intended to move FHI out of the Faculty of Philosophy into a more hospitable administrative home—had been blocked internally.[^39] Additional contributing factors included controversies surrounding FHI figures: a 1996 email with racist content by Nick Bostrom that resurfaced and prompted a university investigation, and separate allegations of misconduct involving FHI-adjacent individuals.[^40] Open Philanthropy had been FHI's most important funder, making grants of £1.6 million in 2017 and £13.3 million in 2018; a significant portion of the latter remained unspent at the time of closure due to the hiring freeze.[^39] While the FTX collapse disrupted broader EA funding flows, the proximate causes of FHI's closure were Oxford administrative decisions rather than the loss of FTX funding specifically.

<EntityLink id="nick-beckstead">Nick Beckstead</EntityLink>, who had served as CEO of the FTX Foundation after joining in November 2021, co-signed the Future Fund team's resignation statement on November 11, 2022, expressing that the team was "shocked and immensely saddened" by the events at FTX and concerned for "thousands of customers whose finances may have been jeopardized."[^41] By August 2023, Beckstead had stepped down from the boards of Effective Ventures UK and Effective Ventures US, with the board noting that his ongoing recusal from all FTX-related matters had made it difficult for him to contribute effectively.[^42] No further public philosophical statements by Beckstead defending or critiquing longtermism have been published since his resignation.

The Global Priorities Institute at Oxford has continued to operate. <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> recommended a grant of approximately \$3.3 million to GPI for general support in the 2023–2024 period, and GPI produced active research output including working papers on population ethics and epistemic challenges to longtermism.[^43]

### Funding Disruption and Recovery

<EntityLink id="open-philanthropy">Open Philanthropy</EntityLink> paused most new longtermist funding commitments in November 2022 following the FTX collapse. That pause was lifted in late January 2023, after the organization conducted an internal review and established new grant assessment guidance.[^44] In 2023, Open Philanthropy directed over \$750 million in grants across its portfolios.[^45] Notably, Open Philanthropy renamed its "Longtermism" portfolio to "Global Catastrophic Risks (GCR)" in 2023–2024, stating that AI risk and biorisk are not only long-term concerns and could threaten many lives in the near future; the rebranding was also intended to provide better symmetry with the "Global Health and Wellbeing" portfolio.[^45]

The FTX collapse caused a reduction in expected longtermist funding estimated in the hundreds of millions of dollars annually. Other funders partially filled the gap: the Survival and Flourishing Fund increased giving in 2023, and Longview Philanthropy moved over \$55 million since its founding in 2018, with further increases expected in 2024.[^46] The Long-Term Future Fund paid out approximately \$5.36 million in grants between May 2023 and March 2024.[^46] On the commercial side, Google invested \$500 million in <EntityLink id="anthropic">Anthropic</EntityLink> in October 2023, committing up to \$1.5 billion total—replacing FTX's earlier \$500 million investment in the company.[^46]

### The Near-Term vs. Long-Term Rift Within EA

The FTX collapse also intensified pre-existing tensions within EA between advocates focused on global health and poverty alleviation and those prioritizing longtermist cause areas. Critics from the global health side of EA argued that longtermism's dominance of EA funding had distorted priorities and that the FTX scandal validated concerns about speculative cause prioritization. The Devex reporting from late 2022 noted that global health and development organizations funded by EA were uncertain whether their funding pipelines would survive the collapse.[^47] Through 2022, roughly 70% of Open Philanthropy's total funding had gone toward global health and wellbeing, with approximately 30% toward longtermist areas; the subsequent rebranding to "Global Catastrophic Risks" further blurred the near-term/long-term distinction at the organizational level.[^44]

## Philosophical Critiques Independent of FTX

It is important to distinguish between reputational damage arising from association with fraud and substantive philosophical critiques—some of which predate FTX and continue to develop on independent grounds.

### The Prioritization Problem

A central objection to longtermism concerns its treatment of present versus future welfare. Because longtermist calculations assign equal moral weight to future individuals, and because the potential future human population vastly outnumbers those alive today—potentially by ratios of thousands to one if humanity persists for millions of years—longtermist reasoning systematically tends to direct resources toward speculative future risks rather than concrete contemporary suffering.[^6] Critics argue that when this logic is applied consistently, it renders nearly every immediate problem less important than existential risk mitigation, effectively licensing the de-prioritization of poverty, climate change, and other present injustices.[^19]

Philosopher Alice Crary, writing in *Radical Philosophy* in 2023, argued that longtermists give existential threats such weight that they deprioritize actual suffering in the world we live in, and that the FTX collapse brought this structural feature of the ideology into public view.[^23] David Thorstad, in a 2023 article in *Philosophy and Public Affairs*, developed a related argument: that longtermists simultaneously hold that humanity faces high existential risk and that existential risk mitigation has astronomical value, but that existential risk pessimism actually reduces the expected value of existential risk mitigation—threatening what he calls the "astronomical value thesis" at the heart of longtermist prioritization.[^48]

### The "Ends Justify Means" Concern

Several critics have argued that longtermism's consequentialist architecture—its insistence that actions be evaluated by their expected long-term outcomes—creates insufficient ethical guardrails against harmful behavior in the present. Because Bankman-Fried described himself as a committed consequentialist, some observers raised the question of whether his fraudulent conduct could be rationalized within a framework that prizes outcome maximization.[^21] This is not to say that longtermism endorses fraud; MacAskill explicitly denied that it does, citing passages in *What We Owe the Future* opposing "ends justify the means" reasoning.[^12] Critics contend, however, that a philosophy oriented around scale-sensitive expected value calculations may systematically underweight deontological constraints—honesty, informed consent, fiduciary duty—that are not easily quantified.[^22]

Philosopher Alice Crary argued in *Radical Philosophy* that longtermism's corruption is inseparable from the way its core ideas are put into practice, suggesting the problem is not merely one of bad individual actors but of structural features of the ideology.[^23]

### Epistemic Challenges and Probability Concerns

Beyond the FTX controversy, critics have identified several internal philosophical vulnerabilities. Émile Torres, in an essay published in *Aeon* on October 19, 2021—more than a year before the FTX collapse—argued that longtermism's emphasis on maximizing the long-run potential of the human species could justify extreme technological acceleration and concentration of power in the present, and that it risks treating current individuals as instrumentalities for future value rather than as beings with intrinsic moral worth.[^24] Torres described himself as "a former longtermist" who had "come to see this worldview as quite possibly the most dangerous secular belief system in the world today."[^24] Torres has subsequently published in academic venues including *Synthese*, *Inquiry*, *Bioethics*, and *Metaphilosophy*, and in 2024 co-authored with Timnit Gebru a peer-reviewed article in *First Monday* arguing that the normative framework motivating AGI development is rooted in the Anglo-American eugenics tradition.[^49]

The probability mathematics underlying longtermist prioritization have also attracted sustained academic criticism. Longtermist arguments often hold that even very small reductions in extinction probability carry more expected value than saving large numbers of current lives, given the astronomical scale of potential future generations.[^25] Christian Tarsney, in a 2023 article in *Synthese*, argued that the effects of present actions on the very long-run future may be nearly impossible to predict, and developed models showing mixed conclusions about whether the case for longtermism is robust under this "epistemic cluelessness."[^50] Thorstad's 2024 article in *Ethics* further critiqued the moral mathematics underlying longtermist arguments for prioritizing existential risk mitigation, and was cited in a 2025 symposium introduction in *Moral Philosophy and Politics* as a major recent contribution to the critical literature.[^51]

A 2025 symposium in *Moral Philosophy and Politics* surveyed the major objections: epistemic cluelessness about the far future, questionable assumptions about demographic development, reliance on "fanaticist decision theory," excessive demandingness, insufficient attention to current problems, and threats to the integrity of meaningful human lives.[^51]

### Internal Incoherence

Some critics have pointed to a potential self-undermining quality in longtermism's demands. If present generations are required to make substantial sacrifices in the service of future flourishing, and if this pattern persists across time, the result may be a history of continuous present-day privation whose benefits accrue only to a final generation—a structure that critics argue undermines the philosophy's own goal of ensuring humanity's long-run welfare. This "perpetual sacrifice" objection has been raised in the population ethics literature and in critical forums.[^27] Defenders respond that longtermist recommendations do not require impoverishing present generations, pointing to interventions such as AI safety research and pandemic preparedness that are argued to produce near-term co-benefits.

## Defenders' Responses

Proponents of longtermism have consistently argued that the FTX scandal reflects the moral failures of an individual, not the validity of a philosophical framework. MacAskill's and <EntityLink id="toby-ord">Toby Ord</EntityLink>'s arguments rest on three premises—that future people matter morally, that future populations could be enormous in scale, and that current actions can reliably influence existential outcomes—none of which is logically affected by what any particular donor did with cryptocurrency exchange funds.[^28]

MacAskill's immediate post-FTX response invoked the content of *What We Owe the Future* directly, arguing the book explicitly opposed "ends justify the means" reasoning and that the fraud was a violation of, not an expression of, longtermist ethics.[^12] He acknowledged the need for serious reflection on how the EA community had extended trust to Bankman-Fried and what institutional changes were warranted.

Defenders also note that longtermism encompasses a range of positions, from MacAskill's more modest claim that long-term impact is *a* key moral priority, to stronger versions holding that it is *the* overwhelming priority. Hilary Greaves and Christian Tarsney's 2025 Oxford University Press volume distinguishes between "minimal" and "expansive" longtermism, representing a pro-longtermism philosophical development in direct dialogue with the critical literature.[^52] The more cautious formulations explicitly recommend robust actions under uncertainty, including option-preservation, avoidance of irreversible harms, and epistemic humility—recommendations not obviously in tension with conventional ethical constraints.[^29]

On the question of present neglect, defenders argue that longtermism is compatible with significant near-term intervention and that critics construct a false opposition. Pandemic preparedness, for instance, serves both near-term public health and long-run existential risk reduction; AI safety research is relevant both to current harms from deployed systems and to hypothetical future catastrophe.[^30]

The Global Priorities Institute has continued to produce research engaging with the critical literature, including working papers that take seriously the epistemic challenges raised by Tarsney and Thorstad.[^43] <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink>, the largest funder of longtermist-adjacent work, resumed grantmaking after its brief pause and directed over \$750 million across its portfolios in 2023.[^45]

## Criticisms and Concerns

### Association with Undemocratic Power Concentration

One of the most persistent criticisms concerns longtermism's relationship to concentrated private wealth. The movement's philanthropic infrastructure has historically depended on a small number of extremely wealthy technology entrepreneurs who exercise significant influence over research agendas and institutional priorities without meaningful democratic oversight.[^14] Critics characterize this as privately held power operating in the public sphere with minimal civic accountability, and argue that the FTX scandal illustrated the risks inherent in such concentration. This critique, developed by Crary in *Radical Philosophy* and by Torres in multiple venues, treats the dependency on billionaire philanthropists not as a contingent feature but as a structural one arising from longtermism's compatibility with—and appeal to—those who have already accumulated exceptional resources.[^23]

### Alleged "Ethics Washing"

Critics have alleged that longtermist and EA identity functioned as what they term "ethics washing"—presenting a philanthropic persona that provided social legitimacy and political access while underlying financial practices were problematic. Bankman-Fried's congressional testimony emphasized his philanthropic commitments even as his exchange allegedly misused customer funds.[^32] Proponents dispute this characterization, arguing that the vast majority of EA-identified donors and organizations were themselves victims of fraud and that characterizing the entire movement's identity as performative overgeneralizes from a single case. Whether the alleged pattern represents a systemic feature of longtermist culture or an individual's exploitation of genuine community norms remains contested among analysts of the collapse.

### The Absence of Tractable Interventions

A practical critique that has gained traction concerns the difficulty of identifying concrete, tractable interventions that reliably reduce existential risk by measurable amounts. Critics note that identifying clearly promising longtermist interventions that meet rigorous evidence-based standards has proven challenging, raising questions about whether the framework generates reliable action-guidance.[^33] Defenders contest this directly, pointing to AI safety research, pandemic preparedness infrastructure, and biosecurity policy work as interventions with both near-term value and longtermist rationale. The disagreement partly reflects differing standards of evidence: critics apply standards developed in global health cost-effectiveness analysis, while defenders argue those standards are not appropriate for novel risk categories where experimental evidence is unavailable.

## Key Uncertainties

Several significant questions remain open:

- **Philosophical independence from scandal**: Whether the core philosophical arguments of longtermism can be evaluated cleanly apart from their institutional context and funding history is disputed. Critics like Crary argue the ideas and their implementation are inseparable; defenders maintain the opposite.
- **Probability and cluelessness**: The degree to which existential risk estimates are tractable versus speculative remains unresolved. Tarsney's 2023 *Synthese* article and Thorstad's 2024 *Ethics* article both raise formal challenges to the probability reasoning underlying longtermist prioritization; defenders have yet to fully respond in the peer-reviewed literature.[^50][^51]
- **Institutional recovery**: Whether EA and longtermist institutions will recover funding and public credibility comparable to their pre-2022 levels is unclear. The closure of the <EntityLink id="fhi">Future of Humanity Institute</EntityLink> and the loss of FTX funding represent significant institutional changes; the rebranding of Open Philanthropy's portfolio and the continued operation of GPI represent countervailing signals.
- **Philosophical development**: How longtermism's academic defenders will respond to the growing body of philosophical criticism—including the Bristol Foundations of Longtermism project and the 2025 *Moral Philosophy and Politics* symposium—remains to be seen. The Greaves–Tarsney distinction between minimal and expansive longtermism may represent one such response.[^52]

## Sources

[^1]: [Longtermism — Wikipedia](https://en.wikipedia.org/wiki/Longtermism)
[^2]: [Longtermism: A Philosophy to Last a Lifetime or Two — Oxford Political Review](https://www.oxjournal.org/longtermism-a-philosophy-to-last-a-lifetime-or-two/)
[^3]: [OK, WTF Is Longtermism? — VICE (November 2022)](https://www.vice.com/en/article/ok-wtf-is-longtermism-the-tech-elite-ideology-that-led-to-the-ftx-collapse/)
[^4]: [Effective Altruism and Longtermism: The Elite Tech Ideologies Damaged by FTX — The Week](https://theweek.com/cryptocurrencies/958633/effective-altruism-and-longtermism-the-elite-tech-ideologies-damaged-by)
[^5]: [Who Else Belongs to My Moral Circle? The Foundations of Longtermism — University of Bristol Arts Matter Blog (November 2024)](https://artsmatter.blogs.bristol.ac.uk/2024/11/21/who-else-belongs-to-my-moral-circle-the-foundations-of-longtermism/)
[^6]: [Long-termism: An Ethical Trojan Horse — Carnegie Council](https://www.carnegiecouncil.org/media/article/long-termism-ethical-trojan-horse)
[^7]: [Centre for Effective Altruism — Longtermism](https://www.centreforeffectivealtruism.org/longtermism)
[^8]: [Longtermism: A Philosophy to Last a Lifetime or Two — Oxford Political Review](https://www.oxjournal.org/longtermism-a-philosophy-to-last-a-lifetime-or-two/)
[^9]: [Sam Bankman-Fried, Effective Altruism, and Alameda — TIME (March 2023)](https://time.com/6262810/sam-bankman-fried-effective-altruism-alameda-ftx/)
[^10]: [OK, WTF Is Longtermism? — VICE (November 2022)](https://www.vice.com/en/article/ok-wtf-is-longtermism-the-tech-elite-ideology-that-led-to-the-ftx-collapse/)
[^11]: [What the FTX Collapse Teaches Us About Ethics — Principia Advisory (March 2023)](https://www.principia-advisory.com/2023/03/24/what-the-ftx-collapse-teaches-us-about-ethics/)
[^12]: [A Personal Statement on FTX — William MacAskill, EA Forum (November 12, 2022)](https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx)
[^13]: [Wrong Lessons from the FTX Catastrophe — EA Forum](https://forum.effectivealtruism.org/posts/DB9ggzc5u9RMBosoz/wrong-lessons-from-the-ftx-catastrophe)
[^14]: [The Toxic Ideology of Longtermism — Alice Crary, Radical Philosophy (2023)](https://www.radicalphilosophy.com/commentary/the-toxic-ideology-of-longtermism)
[^15]: [Why Effective Altruism and Longtermism Are Toxic Ideologies — Current Affairs (interview with Torres, May 2023)](https://www.currentaffairs.org/news/2023/05/why-effective-altruism-and-longtermism-are-toxic-ideologies)
[^16]: [Exclusive: Effective Altruist Leaders Were Warned About Sam Bankman-Fried Years Before FTX Collapsed — TIME (March 15, 2023)](https://time.com/6262810/sam-bankman-fried-effective-altruism-alameda-ftx/)
[^17]: [The Toxic Ideology of Longtermism — Alice Crary, Radical Philosophy (2023)](https://www.radicalphilosophy.com/commentary/the-toxic-ideology-of-longtermism)
[^19]: [Why Longtermism Is the World's Most Dangerous Secular Credo — Émile Torres, Aeon (October 19, 2021)](https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo)
[^21]: [FTX, EA Principles, and the Longtermist EA Community — EA Forum](https://forum.effectivealtruism.org/posts/st59vLvsorvQhqvBr/ftx-ea-principles-and-the-longtermist-ea-community)
[^22]: [Back to Virtue: Effective Altruism After FTX — MercatorNet](https://www.mercatornet.com/back-to-virtue-effective-altruism-after-ftx)
[^23]: [The Toxic Ideology of Longtermism — Alice Crary, Radical Philosophy (2023)](https://www.radicalphilosophy.com/commentary/the-toxic-ideology-of-longtermism)
[^24]: [Why Longtermism Is the World's Most Dangerous Secular Credo — Émile Torres, Aeon (October 19, 2021)](https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo)
[^25]: [Longtermism — Wikipedia](https://en.wikipedia.org/wiki/Longtermism)
[^27]: [On the Fundamental Incoherence of Longtermism — Mind Your Metaphysics (Substack)](https://mindyourmetaphysics.substack.com/p/on-the-fundamental-incoherence-of)
[^28]: [Centre for Effective Altruism — Longtermism](https://www.centreforeffectivealtruism.org/longtermism)
[^29]: [Longtermism — 80,000 Hours](https://80000hours.org/articles/future-generations/)
[^30]: [What Will FTX's Collapse Mean for Global Health and Development? — Devex](https://www.devex.com/news/what-will-ftx-s-collapse-mean-for-global-health-and-development-104480)
[^31]: [Effective Altruism and Longtermism: The Elite Tech Ideologies Damaged by FTX — The Week](https://theweek.com/cryptocurrencies/958633/effective-altruism-and-longtermism-the-elite-tech-ideologies-damaged-by)
[^32]: [What the FTX Collapse Teaches Us About Ethics — Principia Advisory (March 2023)](https://www.principia-advisory.com/2023/03/24/what-the-ftx-collapse-teaches-us-about-ethics/)
[^33]: [Why Haven't We Seen a Promising Longtermist Intervention Yet? — EA Forum](https://forum.effectivealtruism.org/posts/LSfZKPFHzwJgdhF5f/why-haven-t-we-seen-a-promising-longtermist-intervention-yet)
[^34]: [Contemporary Utilitarians: Sam Bankman-Fried — Utilitarianism.com](https://www.utilitarianism.com/sam-bankman-fried.html)
[^35]: [Sam Bankman-Fried and the Effective Altruism Delusion — New Statesman (November 7, 2023)](https://www.newstatesman.com/long-reads/2023/11/sam-bankman-fried-crypto-king-effective-altruism)
[^36]: [What We Owe the Past: William MacAskill, Effective Altruism and the Wrong Life — Logos Journal (October 2023)](https://logosjournal.com/article/what-we-owe-the-past-william-macaskill-effective-altruism-and-the-wrong-life/)
[^37]: [Effective Altruism, Longtermism, and the Problem of Arbitrary Power — The Philosopher 1923 (2023)](https://www.thephilosopher1923.org/post/a-mirror-for-tech-bros)
[^38]: [The End of the Future of Humanity Institute — Daily Nous (April 18, 2024)](https://dailynous.com/2024/04/18/end-future-of-humanity-institute/)
[^39]: [Future of Humanity Institute 2005–2024: Final Report — EA Forum (April 17, 2024)](https://forum.effectivealtruism.org/posts/uK27pds7J36asqJPt/future-of-humanity-institute-2005-2024-final-report)
[^40]: [The Future of Humanity Institute Closes — Bioethics Observatory (June 2024)](https://bioethicsobservatory.org/2024/05/the-university-of-oxford-has-closed-the-future-of-humanity-institute/46366/)
[^41]: [The FTX Future Fund Team Has Resigned — Nick Beckstead and Future Fund team, EA Forum (November 11, 2022)](https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1)
[^42]: [Nick Beckstead Is Leaving the Effective Ventures Boards — Eli Rose, EA Forum (September 6, 2023)](https://forum.effectivealtruism.org/posts/Defu3jkejb7pmLjeN/nick-beckstead-is-leaving-the-effective-ventures-boards)
[^43]: [Global Priorities Institute — General Support Grant — Open Philanthropy (2023–2024)](https://www.openphilanthropy.org/grants/global-priorities-institute-general-support/)
[^44]: [We're No Longer 'Pausing Most New Longtermist Funding Commitments' — Holden Karnofsky, Open Philanthropy EA Forum post (January 30, 2023)](https://forum.effectivealtruism.org/posts/FHJMKSwrwdTogYLGF/we-re-no-longer-pausing-most-new-longtermist-funding)
[^45]: [Our Progress in 2023 and Plans for 2024 — Open Philanthropy (March 27, 2024)](https://www.openphilanthropy.org/research/our-progress-in-2023-and-plans-for-2024/)
[^46]: [Observations on the Funding Landscape of EA and AI Safety — EA Forum (October 2, 2023)](https://forum.effectivealtruism.org/posts/RueHqBuBKQBtSYkzp/observations-on-the-funding-landscape-of-ea-and-ai-safety)
[^47]: [What Will FTX's Collapse Mean for Global Health and Development? — Devex](https://www.devex.com/news/what-will-ftx-s-collapse-mean-for-global-health-and-development-104480)
[^48]: [High Risk, Low Reward: A Challenge to the Astronomical Value of Existential Risk Mitigation — David Thorstad, Philosophy and Public Affairs 51(4): 373–412 (Fall 2023)](https://philpapers.org/rec/THOHRL)
[^49]: [The TESCREAL Bundle: Eugenics and the Promise of Utopia through Artificial General Intelligence — Émile P. Torres and Timnit Gebru, First Monday (April 2024)](https://firstmonday.org/ojs/index.php/fm/article/view/13636)
[^50]: [The Epistemic Challenge to Longtermism — Christian Tarsney, Synthese (2023)](https://link.springer.com/article/10.1007/s11229-023-04153-y)
[^51]: [Philosophy for the Long Run: Introduction to the Symposium on Longtermism — Moral Philosophy and Politics, De Gruyter (2025)](https://www.degruyterbrill.com/document/doi/10.1515/mopp-2025-0012/html); see also: [Mistakes in the Moral Mathematics of Existential Risk — David Thorstad, Ethics 135(1): 122–150 (2024)](https://www.journals.uchicago.edu/doi/10.1086/730283)
[^52]: [Minimal and Expansive Longtermism — Hilary Greaves and Christian Tarsney, Oxford University Press (2025)](https://philpapers.org/browse/longtermism)
