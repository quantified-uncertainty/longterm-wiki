---
title: EA and Longtermist Wins and Losses
description: A comprehensive impact ledger of the effective altruism and longtermist movements' major successes and setbacks, organized by year and topic, including cumulative statistics, policy record, funding milestones, global health impact, the FTX collapse, community debates, and criticisms of longtermist epistemology.
importance: 50
lastEdited: "2026-02-20"
clusters:
  - community
  - ai-safety
subcategory: ea-history
numericId: E868
sidebar:
  order: 50
ratings:
  novelty: 4
  rigor: 6
  actionability: 4
  completeness: 7
quality: 53
llmSummary: A comprehensive impact ledger of EA/longtermism's track record organized by year and topic, covering verified wins (GiveWell's $1B+ directed, ~100,000 lives saved through AMF, 10K GWWC pledges) and significant setbacks (FTX collapse, FHI closure, funding decline, internal fragmentation), with cumulative statistics, year-by-year highlights, a policy record table, and honest counterfactual attribution notes throughout.
entityType: historical
---
import {EntityLink} from '@components/wiki';

## Quick Assessment

| Dimension | Assessment |
|-----------|-----------|
| **Scope** | Tracks outcomes of EA and longtermist interventions across global health, existential risk, AI safety, animal welfare, and community building |
| **Key Wins** | GiveWell directing \$1.45B+ to effective charities; Against Malaria Foundation saving ≈100,000 lives; 10,000 GWWC pledges; AI safety policy wins in California and New York; 92% cage-free commitment fulfillment |
| **Key Losses** | FTX collapse damaging credibility and funding; closure of <EntityLink id="E140">Future of Humanity Institute</EntityLink> in 2024; internal tensions between neartermist and longtermist factions; epistemological critiques of longtermist modeling; declining total grantmaking since 2022 peak |
| **Movement Status** | <EntityLink id="E517">CEA</EntityLink> reports 20–25% engagement growth in 2025, reversing moderate declines in 2023–2024; forum usage and EAG attendance had not returned to 2022 peak levels as of end-2024; funding landscape stabilizing but below 2022 peak |
| **Primary Debate** | Whether longtermist priorities (AI safety, existential risk) crowd out or complement neartermist work (global health, animal welfare) |

## Key Links

| Source | Link |
|--------|------|
| Wikipedia | [Longtermism](https://en.wikipedia.org/wiki/Longtermism) |
| EA Forum | [Celebrating Wins Discussion Thread](https://forum.effectivealtruism.org/posts/KN7BTMJHAQkLb6Bac/celebrating-wins-discussion-thread) |
| Centre for Effective Altruism | [centreforeffectivealtruism.org](https://www.centreforeffectivealtruism.org) |
| EA Forum (Funding Data) | [Historical EA Funding Data: 2025 Update](https://forum.effectivealtruism.org/posts/NWHb4nsnXRxDDFGLy/historical-ea-funding-data-2025-update) |

## Overview

Effective altruism (EA) is a philosophical and social movement that uses evidence and reason to identify the most effective ways to benefit others. Within EA, longtermism emphasizes positively influencing the long-term future — particularly by reducing existential risks from advanced AI, engineered pandemics, and other catastrophic threats.[^1] Tracking the movement's successes and failures matters both for internal strategic learning and for external evaluation of whether EA's approach to philanthropy and cause prioritization delivers on its ambitious claims.

The movement has achieved substantial concrete wins: GiveWell has directed over \$1.45 billion to effective charities,[^28] the Against Malaria Foundation — consistently one of GiveWell's top-recommended organizations — has protected over 400 million people and is estimated to have saved approximately 100,000 lives,[^3] and <EntityLink id="E517">CEA</EntityLink> reports strong engagement growth in 2025 after post-FTX declines.[^4] On the longtermist side, hundreds of millions of dollars have flowed to existential risk research, AI safety work has gained mainstream policy traction, and California has signed bills directly regulating AI risk.[^5]

However, the movement has also suffered significant setbacks. The collapse of FTX and fraud conviction of <EntityLink id="E857">Sam Bankman-Fried</EntityLink> — who had publicly framed his financial activities in longtermist terms — severely damaged EA's credibility and contributed to a decline in total grantmaking.[^6] The closure of the <EntityLink id="E140">Future of Humanity Institute</EntityLink> at Oxford in April 2024 — the field's founding research institution — marked the largest single institutional loss in the longtermist research ecosystem.[^43] Internal tensions between neartermist and longtermist factions persist, with some global health advocates reporting that the longtermist turn has harmed their reputations and diverted resources.[^7] Critics from multiple directions challenge longtermism's epistemological foundations, arguing that its calculations are highly sensitive to speculative assumptions about the far future.[^8]

EA grantmaking reached its peak around 2022 and has contracted since, though the 80,000 Hours 2021 estimate of approximately \$46 billion committed to EA causes (growing at approximately 37% annually since 2015) reflects the scale of capital that had been pledged rather than deployed.[^45] Open Philanthropy, the largest single EA-aligned funder, has directed more than \$4 billion in total grants since its founding in 2017.[^46]

---

## Cumulative Statistics

The table below aggregates self-reported impact figures from major EA-aligned organizations. Figures are drawn from organizational reports, EA Forum posts, and external analyses. Counterfactual attribution — the question of how much of this impact would have occurred without EA — is addressed in the notes column and in the dedicated attribution section below.

| Organization | Key Metric | Figures | Source / Notes |
|---|---|---|---|
| **GiveWell** | Total directed to top charities (since 2009) | \$1.45B+ directed; est. 340,000 lives saved total; 74,000 in 2024 alone from \$96.3M directed to AMF | [GiveWell 2024 metrics](https://blog.givewell.org/2025/08/13/givewells-2024-metrics-and-impact/); self-reported; methodology documented publicly. Counterfactual: fraction of donors would have given comparably without GiveWell recommendations — contested. A specific "50–80% counterfactual" figure has circulated in EA discussions but does not appear in GiveWell's published methodology; treat as an illustration, not a published estimate. |
| **<EntityLink id="E552">Open Philanthropy</EntityLink>** | Total giving; farm animal welfare; AI safety; global health | Total giving exceeds \$4B since founding in 2017 (as of June 2025); AI safety spending approximately \$336M (≈12% of total) through 2023; 3B+ farm animals' lives improved via corporate commitments; 100,000+ lives saved via global health grants | [Open Phil grants database](https://www.openphilanthropy.org/grants/); [Open Philanthropy Wikipedia](https://en.wikipedia.org/wiki/Open_Philanthropy_(organization)); [LessWrong AI safety funding overview (2023)](https://www.lesswrong.com/posts/WGpFFJo2uFe5ssgEb/an-overview-of-the-ai-safety-funding-situation). "3B animals" aggregates corporate commitment reach, not verified individual welfare improvements. Through 2022, roughly 70% of Open Phil total funding went to Global Health and Wellbeing, 30% to longtermist portfolio. |
| **<EntityLink id="E863">Giving What We Can</EntityLink>** | Pledges and estimated donation flow | 10,000+ members with 10% pledge; \$2.5B+ in pledges made; \$40M+ donated by members | [GWWC 10,000 pledge milestone](https://www.givingwhatwecan.org/blog/10000-pledge); [Oxford press release (2022)](https://www.ox.ac.uk/news/2022-03-01-oxford-based-charity-receives-more-25-billion-pledges-community-effective-givers). Pledge totals are lifetime commitments, not yet-donated amounts. GWWC uses two complementary methodologies: a Lifetime Giving Method and a Realised Giving Method, both documented in its 2023–2024 impact evaluation. |
| **Founders Pledge** | Entrepreneurs pledged and donated | ≈1,900 entrepreneurs pledged ≈\$10B; \$1.1B donated as of April 2024 | [Founders Pledge 2024 impact report](https://founderspledge.com/stories/our-impact); pledges contingent on exit events; donated figure is more reliable than pledge total. |
| **<EntityLink id="E510">80,000 Hours</EntityLink>** | Career changes and hours redirected | 3,000+ significant career changes; ≈80M more hours on important problems | [80K impact page](https://80000hours.org/about/impact/); self-reported; "significant career change" definition is methodologically contested. |
| **Charity Entrepreneurship** | Charities incubated and reach | 50 charities incubated in 6 years; \$38M raised; 75M people and 1B animals served | [Charity Entrepreneurship impact page](https://www.charityentrepreneurship.com/our-impact); "served" figures aggregate program reach, not verified outcome counts. |
| **Evidence Action / Deworm the World** | Deworming treatments delivered | 198M children reached in 2024 alone (record); 2B+ treatments delivered since 2012; &lt;\$0.50 cost per child per treatment | [GFDW update (May 2025)](https://www.gfdw.eu/en/blog/deworm-the-world); [Evidence Action Tanzania expansion (Jan 2026)](https://www.evidenceaction.org/newsroom/evidence-action-expands-to-tanzania); GiveWell-recommended \$4.4M renewal grant secured programs through 2026. |

**Counterfactual note on cumulative statistics:** These figures represent gross reach and estimated impact, not net counterfactual impact. GiveWell's methodology attempts to estimate counterfactual value (would a donor have given elsewhere?), but for most metrics the counterfactual fraction is unknown. The EA Forum's [2025 funding data post](https://forum.effectivealtruism.org/posts/NWHb4nsnXRxDDFGLy/historical-ea-funding-data-2025-update) notes that self-reported organizational metrics are difficult to compare across cause areas.[^11]

---

## Year-by-Year Highlights

This section provides a scannable chronological record of major events in the EA and longtermist movements. Type classifications: **Win** = outcome broadly favorable to EA/longtermist goals; **Loss** = setback or failure; **Mixed** = ambiguous or contested outcome. Attribution confidence reflects how directly EA action caused the outcome versus broader trends.

| Year | Cause Area | Event | Type | Notes |
|---|---|---|---|---|
| 2006 | Infrastructure | GiveWell founded by Holden Karnofsky and Elie Hassenfeld | Win | First systematic charity evaluator using empirical evidence; catalyzed evidence-based philanthropy field. |
| 2009 | Infrastructure | <EntityLink id="E863">Giving What We Can</EntityLink> founded by <EntityLink id="E355">Toby Ord</EntityLink>; GiveWell begins directing significant funds | Win | Institutionalized the 10% pledge norm; early GiveWell grants began redirecting millions toward top charities. By January 2013, GWWC had welcomed its 300th member. |
| 2011 | Infrastructure | <EntityLink id="E510">80,000 Hours</EntityLink> and <EntityLink id="E517">CEA</EntityLink> founded; term "effective altruism" coined | Win | Career-focused EA organization and coordination hub for EA movement; hosted EA Global conferences. |
| 2015 | AI Safety | <EntityLink id="E552">Open Philanthropy</EntityLink> begins major AI safety funding | Win | Credited with jumpstarting AI safety as a funded research field; seeded organizations including <EntityLink id="E202">MIRI</EntityLink> and later <EntityLink id="E557">Redwood Research</EntityLink> and <EntityLink id="E25">ARC</EntityLink>. *Counterfactual: AI safety was growing independently via DeepMind and academic channels; Open Phil funding likely accelerated rather than originated the field.* |
| 2019 | Animal Welfare | Target, Aramark, and Compass Group make cage-free commitments | Win | Part of broader corporate campaign wave; EA-funded groups (Open Wing Alliance) played documented role. *Counterfactual: Broad consumer pressure and EU regulatory trajectory were also driving corporate policy shifts; EA-funded campaigns' distinctively attributable effect is stronger in emerging markets with less independent consumer pressure.* |
| 2019 | Infrastructure | Approximately \$416M donated to effective charities identified by EA movement (37% annual growth rate since 2015) | Win | [Wikipedia / EA growth data](https://en.wikipedia.org/wiki/Effective_altruism). Reflects movement scaling through the late 2010s. |
| 2020 | Global Health | COVID-19 pandemic highlights gaps in biosecurity infrastructure | Mixed/Loss | Demonstrated relevance of EA biosecurity work; also revealed that EA-adjacent pandemic preparedness funding had not prevented the pandemic. <EntityLink id="E423">Johns Hopkins Center for Health Security</EntityLink> (Open Phil–funded) provided early warnings. |
| 2020 | Infrastructure | GiveWell exceeds \$300M directed in a single year | Win | First time GiveWell surpassed this threshold; driven partly by COVID-era philanthropy surge. Open Philanthropy supported \$100M in GiveWell recommendations in 2020, rising to \$300M in 2021.[^26] |
| 2021 | AI Safety | <EntityLink id="E552">Open Philanthropy</EntityLink> launches major AI safety push; funds <EntityLink id="E557">Redwood Research</EntityLink> and <EntityLink id="E25">ARC</EntityLink> | Win | Substantially expanded AI safety research capacity. Redwood focused on adversarial training; ARC on evaluations. Open Phil recommended over \$400M in grants in 2021 total.[^26] *Counterfactual: Both organizations were founded by researchers with pre-existing safety interest; Open Phil funding enabled scaling.* |
| 2021 | Animal Welfare | Successful corporate campaigns reach billions of animals via cage-free pledges | Win | Open Wing Alliance and allied campaigns documented commitments covering estimated 2B+ hens globally. Fulfillment rates remained uncertain at time of commitment. |
| 2021 | AI Safety | <EntityLink id="E22">Anthropic</EntityLink> founded by <EntityLink id="E91">Dario Amodei</EntityLink> and team departing <EntityLink id="E218">OpenAI</EntityLink> | Win | Created a major safety-focused AI lab. EA/longtermist network connections played a role in early fundraising; <EntityLink id="E552">Open Philanthropy</EntityLink> invested. *Counterfactual: Anthropic's founders had independent safety motivations; the lab likely would have been founded without EA support, though perhaps with less early capital.* |
| 2022 | Community | FTX collapse; <EntityLink id="E857">Sam Bankman-Fried</EntityLink> arrested for fraud | Loss | Most damaging single event for EA credibility. <EntityLink id="E855">FTX Future Fund</EntityLink> had committed approximately \$160M in total grantee commitments before collapse; those commitments were voided. (Approximately \$100M in grants had already been disbursed; the remaining commitments were cancelled. See FTX Future Fund section below.) Forum usage, EAG attendance, EA Funds donations, and virtual programming all declined in subsequent years.[^16] |
| 2022 | Animal Welfare | Multiple EU cage-free policy wins; Spain, France, Germany commit to phase-outs | Win | Regulatory rather than corporate wins; EA animal welfare funders had supported European campaign infrastructure. |
| 2022 | Infrastructure | <EntityLink id="E862">Will MacAskill</EntityLink>'s *What We Owe the Future* reaches bestseller status | Mixed | Brought longtermism to mainstream audience; also increased scrutiny and criticism, coinciding with FTX collapse three months after publication. |
| 2023 | AI Safety | <EntityLink id="E469">Bletchley Declaration</EntityLink> signed at UK AI Safety Summit; 28 nations commit to AI risk cooperation | Win | First multilateral government statement on frontier AI risks; UK government convened the summit under PM Rishi Sunak; agreed to support a "State of the Science" report led by Yoshua Bengio and announced the world's first AI Safety Institute. EA-aligned researchers contributed to technical agenda-setting. *Counterfactual: Post-ChatGPT AI anxiety was mainstream by late 2023; governments were independently motivated to act. EA's marginal contribution to summit outcomes has not been documented with named individuals or government acknowledgments.* |
| 2024 | AI Safety | <EntityLink id="E202">MIRI</EntityLink> announces significant staff cuts and strategic pivot | Loss | MIRI scaled back its technical alignment research program, concluding in a January 2024 strategy update that it believes the alignment field is "very unlikely" to make progress quickly enough to prevent human extinction. The organization pivoted toward policy and public communications, with technical research "no longer our top priority, at least for the foreseeable future." MIRI's annual spending ranged from \$5.4M–\$7.7M (2019–2023).[^38] |
| 2023 | Global Health | WHO approves R21/Matrix-M malaria vaccine; <EntityLink id="E552">Open Philanthropy</EntityLink> co-funded development | Win | R21 is a second malaria vaccine (alongside RTS,S) with high efficacy. Open Phil provided funding to the Jenner Institute's development work. *Counterfactual: R21 development was primarily funded by Wellcome Trust and Serum Institute of India; Open Phil's role was contributory but not foundational.* |
| 2023 | Policy | US Supreme Court upholds California Prop 12 animal welfare law in *National Pork Producers v. Ross* (May 2023) | Win | Upheld California's right to enforce space standards for farm animals sold in California. EA animal welfare groups supported the legal defense. Sets national precedent for state animal welfare laws. |
| 2023 | AI Safety | GPT-4 release makes AI safety concerns mainstream | Mixed | Public concern about AI capabilities validates EA's long-standing warnings; however, mainstream attention also means AI safety is no longer primarily an EA-driven field, reducing EA's comparative influence going forward. |
| 2023 | Cultivated Meat | USDA approves cultivated chicken for sale by UPSIDE Foods and Good Meat | Win | First US approval for commercially sold cultivated meat; Good Food Institute (EA-supported) contributed to the regulatory pathway. |
| 2024 | AI Safety | California SB 1047 vetoed by Governor Newsom | Loss | SB 1047 would have imposed safety requirements on large AI model developers. EA-aligned advocates supported the bill. Anthropic's position evolved: the company initially opposed the bill in its original form (July 2024), proposed amendments that were partially adopted, then offered cautious support for the amended version (August 2024) before Newsom's veto.[^39] <EntityLink id="E218">OpenAI</EntityLink> and Google opposed throughout. Newsom cited concerns about innovation impact. |
| 2024 | AI Safety | <EntityLink id="E364">UK AI Safety Institute</EntityLink> and <EntityLink id="E365">US AI Safety Institute</EntityLink> formally established | Win | Both institutes conduct frontier AI evaluations; UK AISI had EA-connected staff in founding roles. *Counterfactual: Government AI safety interest was driven substantially by post-ChatGPT political pressure independent of EA advocacy.* |
| 2024 | AI Safety | Seoul AI Summit; Seoul Declaration signed | Win | Follow-up to Bletchley; 16 additional nations joined commitments; focused on AI safety testing and information sharing among frontier labs. EA influence in technical agenda-setting noted by participants, though primary drivers were government-led. |
| 2024 | Institutional | <EntityLink id="E140">Future of Humanity Institute</EntityLink> closes after 19 years | Loss | Oxford's Faculty of Philosophy closed FHI on April 16, 2024, citing "increasing administrative headwinds." Founded by Nick Bostrom in 2005, FHI had coined much of the field's core terminology ("existential risk," "information hazard," "unilateralist's curse") and served as the primary academic home for longtermist research. The Faculty had imposed a freeze on FHI fundraising and hiring beginning in 2020; in late 2023 it announced contracts of remaining staff would not be renewed. Senior staff attempted multiple rescue options including restructuring, college affiliation, spinning out, and interdepartmental transfer — none succeeded.[^43] FHI's final report noted that "FHI's mission has replicated and spread and diversified" into dozens of successor organizations, though the loss of the Oxford institutional anchor was significant. |
| 2024 | Animal Welfare | California and Washington state pass octopus farming bans | Win | First US legislation banning a specific type of cephalopod aquaculture; animal welfare advocates (some EA-aligned) supported the bills; cephalopod sentience research partly EA-funded. |
| 2024 | Global Health | Lead Exposure Elimination Project (LEEP) and allied funders announce \$100M+ committed to global lead reduction | Win | Lead exposure is a GiveWell-recommended cause area; LEEP is a Charity Entrepreneurship incubatee with documented direct policy wins in Malawi, Madagascar, and other countries. Open Philanthropy's 2024 grants included the Lead Exposure Action Fund under global health.[^36] |
| 2024 | Global Health | GiveWell directs \$96.3M to Against Malaria Foundation — largest single grant in GiveWell history | Win | Expected to distribute 17M+ nets across Chad, DRC, Nigeria, and Zambia; estimated to avert 20,000+ deaths.[^5] GiveWell raised approximately \$415M total in 2024, up from approximately \$355M in 2023.[^28] |
| 2024 | Community | EA Forum engagement metrics increase after post-FTX lows | Mixed | Forum activity showed signs of recovery but did not return to 2022 peak levels as of end-2024; CEA's own 2025 strategy report noted "forum usage metrics have been on a steady decline since FTX's collapse in late 2022."[^16] |
| 2025 | AI Safety | California <EntityLink id="E457">SB 53</EntityLink> signed into law | Win | SB 53 requires AI developers to maintain safety and security protocols; narrower than SB 1047 but represented a legislative foothold for AI safety regulation after the SB 1047 veto. EA-aligned advocates supported passage. |
| 2025 | Community | <EntityLink id="E863">Giving What We Can</EntityLink> hits 10,000 pledges | Win | Milestone reported in GWWC communications; each pledge estimated to generate \$15,000 in counterfactual donations to high-impact charities over a lifetime, per GWWC's documented Lifetime Giving Method.[^47] |
| 2025 | Community | EA Global London reaches approximately 1,600 attendees | Win | Indicator of community recovery post-FTX; attendance figures from <EntityLink id="E517">CEA</EntityLink> strategy reports.[^16] |
| 2025 | Animal Welfare | 92% of corporate cage-free commitments with 2024 or earlier deadlines fulfilled | Win | Tracked by Open Wing Alliance; reported in EA Forum wins thread.[^5] Represents a shift in hen welfare conditions across the food industry relative to prior years. Note: this figure tracks formal policy transitions, not verified supply chain welfare outcomes at the farm level. |
| 2025 | Animal Welfare | Lewis Bollard appearance on Dwarkesh Podcast raises \$2M+ for animal charities, estimated to help ≈4M animals | Win | Demonstrates EA media strategy effectiveness for cause area fundraising.[^5] |
| 2025 | Community | <EntityLink id="E517">CEA</EntityLink> reports 20–25% growth in engagement across all tiers | Win | Substantially exceeds 7.5–10% growth target set in CEA's 2025–26 strategy; reverses moderate declines in 2023–2024 without increasing spending. Growth is measured year-over-year from 2024 baselines, not from 2022 peak levels.[^4] |

---

## Notable Policy Record

This table covers major policy outcomes with documented EA involvement. "EA Attribution" rates how much of the outcome is plausibly attributable to EA advocacy versus independent forces, using a qualitative scale: **High** = EA was a leading driver; **Moderate** = EA was a significant but non-decisive contributor; **Low** = EA was a minor participant in a broader movement; **Contested** = attribution is disputed.

| Policy | Year | Outcome | EA Attribution | Notes |
|---|---|---|---|---|
| California Prop 12 (Farm Animal Confinement) | 2018 (passed), 2023 (upheld) | **Win** — US Supreme Court upholds in *Nat'l Pork Producers v. Ross* (2023) | Moderate | Humane Society led the ballot initiative; EA-aligned animal welfare funders supported defense. Sets national precedent for state animal welfare laws. |
| California SB 1047 (Safe and Secure Innovation for Frontier AI Models Act) | 2024 | **Loss** — Vetoed by Governor Newsom, September 2024 | Moderate | EA-aligned advocates including Legal Priorities Project supported passage. Anthropic's position shifted from opposition (July 2024) to cautious support for the amended bill (August 2024) before the veto.[^39] OpenAI and Google opposed throughout. Newsom cited chilling effect on AI innovation. |
| California <EntityLink id="E457">SB 53</EntityLink> (AI Safety Protocols) | 2025 | **Win** — Signed into law | Moderate | Narrower successor to SB 1047; requires documented safety protocols for frontier AI developers. EA-aligned advocates supported. |
| <EntityLink id="E471">New York RAISE Act</EntityLink> (Responsible AI Safety and Education Act) | 2024–2025 | **Partial / Pending** | Low–Moderate | Bill introduced with EA-adjacent support; had not passed as of early 2026. Highlights limits of state-level AI safety legislation outside California. |
| <EntityLink id="E469">Bletchley Declaration</EntityLink> (UK AI Safety Summit) | November 2023 | **Win** — 28 nations sign | Moderate | UK government led under PM Rishi Sunak; EA-aligned researchers contributed to technical agenda-setting. Post-ChatGPT political momentum was the primary driver. No named individuals or government acknowledgments documenting EA's specific causal contributions have been published. |
| Seoul Declaration (Seoul AI Summit) | May 2024 | **Win** — 16 additional nations join | Moderate | Follow-up to Bletchley; advanced commitments on safety testing. EA influence in technical agenda-setting noted by participants; primary drivers were government-led processes. |
| <EntityLink id="E127">EU AI Act</EntityLink> | 2024 (passed) | **Mixed** | Low | Primarily driven by EU internal politics and civil society groups; EA-aligned groups contributed technical input on frontier model provisions. Final text modified frontier model provisions relative to EA advocates' preferences. |
| <EntityLink id="E216">NIST AI Risk Management Framework (AI RMF)</EntityLink> | 2023 | **Win** — Adopted as voluntary US standard | Low–Moderate | EA-adjacent researchers contributed comments during public consultation. Industry adoption is voluntary; enforcement limited. |
| WHO R21/Matrix-M Malaria Vaccine Approval | October 2023 | **Win** — WHO prequalification granted | Low–Moderate | Open Philanthropy co-funded Jenner Institute development; primary funders were Wellcome Trust and Serum Institute of India. |
| Lead Paint and Gasoline Regulation Progress (low-income countries) | 2022–2025 | **Win (partial)** — Multiple countries adopt stricter lead standards | Moderate | LEEP (Charity Entrepreneurship incubatee) documented direct policy wins in Malawi, Madagascar, and other countries. Open Philanthropy's 2024 grants included the Lead Exposure Action Fund.[^36] |
| California/Washington Octopus Farming Bans | 2024 | **Win** — Signed into law in both states | Moderate | EA-aligned animal welfare advocates supported; cephalopod sentience research (partly EA-funded) informed legislative framing. |
| US USDA Cultivated Meat Approvals (UPSIDE Foods, Good Meat) | June 2023 | **Win** — First US commercial approvals | Low–Moderate | Good Food Institute (EA-supported) contributed to regulatory pathway; primary drivers were the companies themselves and USDA process. |

**Policy record notes:** The absence of biosecurity and pandemic preparedness policy entries reflects a genuine gap in the EA attribution record. EA-adjacent organizations including <EntityLink id="E423">Johns Hopkins Center for Health Security</EntityLink> contributed to pandemic preparedness frameworks, and Open Philanthropy has funded biosecurity work, but tracking specific policy outcomes attributable to EA advocacy in this domain requires further documentation. Similarly, nuclear risk reduction receives Open Philanthropy funding (grants to organizations including the Federation of American Scientists and Ploughshares Fund) but no specific policy wins have been tracked here due to attribution difficulty.

---

## History and Movement Trajectory

EA emerged from proto-communities in the late 2000s and early 2010s, coalescing around organizations like GiveWell (founded 2006), <EntityLink id="E863">Giving What We Can</EntityLink> (founded 2009 at Oxford by <EntityLink id="E355">Toby Ord</EntityLink>), and <EntityLink id="E510">80,000 Hours</EntityLink> (founded 2011).[^1] CEA was founded in 2011 as an umbrella organization for GWWC and 80,000 Hours, and the term "effective altruism" was coined that year.[^44] By January 2013, GWWC had welcomed its 300th member; by 2022, more than 7,000 people from 95 countries had taken a pledge, collectively representing over \$2.5 billion in pledged lifetime donations.[^44b] An estimated \$416 million was donated to effective charities identified by the movement in 2019, representing approximately 37% annual growth since 2015.[^1]

By 2021, an estimated \$46 billion in funding had been committed to EA causes — growing approximately 37% per year since 2015, with much of the growth concentrated in 2020–2021 — with around \$420 million deployed annually (roughly 1% of committed capital).[^45] In early 2020, roughly 60% of EA annual deployment flowed through Open Philanthropy, 20% from GiveWell, and 20% from other sources.[^45]

The longtermist turn within EA accelerated in the late 2010s and early 2020s. Key markers of this shift include <EntityLink id="E552">Open Philanthropy</EntityLink>'s first major AI safety grants in 2015; the growing influence of the <EntityLink id="E140">Future of Humanity Institute</EntityLink> at Oxford (founded 2005 by Nick Bostrom), which coined core longtermist terminology and hosted researchers including Anders Sandberg and <EntityLink id="E355">Toby Ord</EntityLink>; and <EntityLink id="E355">Toby Ord</EntityLink>'s 2020 book *The Precipice*, which articulated a systematic framework for existential risk reduction. <EntityLink id="E862">Will MacAskill</EntityLink>'s 2022 book *What We Owe the Future* brought longtermism to mainstream audiences, achieving bestseller status.[^10] That same year, the <EntityLink id="E855">FTX Future Fund</EntityLink> committed approximately \$160 million in total grantee commitments (with approximately \$100 million already disbursed) within months of launching.[^10][^50]

<EntityLink id="E552">Open Philanthropy</EntityLink> recommended over \$400 million in grants in 2021, including \$300 million in support for GiveWell's recommendations (up from \$100 million in 2020).[^26] Through 2022, roughly 70% of Open Philanthropy's total funding went toward areas in its Global Health and Wellbeing portfolio, and 30% went toward areas in its longtermist portfolio — making the "EA = longtermism" characterization inaccurate as a description of funding distribution.[^27] Following the FTX collapse in late 2022, Open Philanthropy paused most new longtermist funding commitments pending further review.[^27]

EA grantmaking has been on a downward trend since the 2022 peak, though GiveWell has maintained funding levels at approximately \$300–415 million annually, with total funds raised in 2024 reaching approximately \$415 million.[^28] Open Philanthropy expected to recommend over \$700 million in grants in 2023; as of June 2025, it had directed more than \$4 billion in total grants since founding.[^46] No single public source provides a comprehensive aggregate of all EA-aligned giving for 2023 or 2024 across all funders, which limits precise quantification of the post-2022 decline.

By 2025, <EntityLink id="E517">CEA</EntityLink> reported a turnaround, with 20–25% year-over-year engagement increases across all tiers — substantially exceeding its 7.5–10% growth target and reversing moderate declines in 2023–2024.[^4] This growth metric refers to year-over-year change from 2024 baselines, not recovery to 2022 peak levels; CEA's own strategy documents explicitly noted that forum usage, EAG attendance, EA Funds donations, and virtual program participation had all declined year-over-year during 2023–2024 before beginning to recover.[^16]

The FHI closed in April 2024 after the Faculty of Philosophy declined to renew staff contracts, following years of administrative friction and a hiring and fundraising freeze imposed in 2020.[^44c] The specific reasons the Faculty imposed constraints on FHI have not been made fully public; factors cited in reporting include bureaucratic disputes over FHI's operating style, personnel issues, and controversy over a resurfaced 1996 email from Bostrom. Senior researchers attempted multiple rescue options — none succeeded.

**Community size and demographics:** As of January 2023, there were 362 known active EA groups worldwide — up from 233 in late 2020 (approximately 55% growth over two years), with 403 groups in CEA's database by May 2023.[^29] CEA supports EA groups in more than 40 countries and has held EAGx conferences in locations including Singapore, India, and Mexico.[^30] In 2023–2024, 189 organizers from 34 countries participated in CEA's Organizer Support Programme.[^31] The 2024 EA Survey (conducted by <EntityLink id="E558">Rethink Priorities</EntityLink>) found that the USA (34.4%) and UK (13.5%) remained the countries with the largest proportions of EA respondents; 32% came from Europe (excluding the UK) and 20% from the rest of the world.[^48] Local group membership rates varied substantially by country: US respondents (21.9%) and UK respondents (25.8%) had among the lowest rates of local group membership, while many countries showed 40–80% local group membership rates among survey respondents.

---

## Documented Wins

### Global Health and Development

EA's most verifiable successes lie in global health. GiveWell has transferred over \$1.45 billion to effective charities,[^28] and the Against Malaria Foundation — consistently one of GiveWell's top-recommended organizations — has distributed enough insecticide-treated nets to protect over 400 million people, with an estimated 100,000 lives saved.[^3] GiveWell announced its largest single grant ever of \$96.3 million to the Against Malaria Foundation, expected to distribute over 17 million nets across Chad, DRC, Nigeria, and Zambia and avert over 20,000 deaths.[^5]

Other concrete outcomes include significant reductions in parasitic disease through deworming programs. Evidence Action's Deworm the World program launched in Tanzania in January 2026, targeting more than 10 million children currently at risk of or infected with soil-transmitted helminths and/or schistosomiasis, partnering with the government to provide targeted district-level deworming after USAID's NTD funding ended in early 2025.[^32] Globally, Deworm the World reached 198 million children in India, Kenya, Nigeria, Pakistan, and Malawi in 2024 alone — a record number — and has delivered more than 2 billion treatments since 2012 at an estimated cost of less than \$0.50 per child per treatment, supported by a GiveWell-recommended \$4.4 million renewal grant.[^33]

**Counterfactual note:** GiveWell's own methodology acknowledges that counterfactual attribution is difficult — some fraction of donors to top charities would have donated similarly without GiveWell's recommendations, and some fraction of lives saved via malaria nets would have occurred via alternative programs. GiveWell applies a "leverage and funging adjustment" to cost-effectiveness estimates based on the probability that other counterfactual funding scenarios would occur in the absence of their charities' philanthropic spending, and applies this methodology to specific grantees.[^34] GiveWell revised its estimate of the counterfactual value of Gavi's spending downward by more than half (from 0.0167 to 0.007 units of value per dollar) upon further investigation — illustrating how counterfactual estimates can shift substantially with new analysis.[^34] A specific "50–80% counterfactual" figure has circulated in EA discussions but does not appear in GiveWell's published methodology pages and should not be treated as an official published estimate.

No comprehensive independent peer-reviewed evaluations of GiveWell's aggregate impact claims have been published. GiveWell's methodology is documented publicly, and interventions like insecticide-treated nets and deworming have been studied via randomized controlled trials (e.g., through J-PAL-affiliated researchers), but external assessments of whether GiveWell-recommended programs achieve their stated impact in aggregate have not appeared in major peer-reviewed outlets.

### Animal Welfare

Animal welfare has emerged as an increasingly prominent EA cause area. According to a 2025 EA Forum post tracking wins, 92% of corporate cage-free egg commitments with 2024 or earlier deadlines have been fulfilled.[^5] Lewis Bollard's appearance on the Dwarkesh Podcast raised over \$2 million for effective animal welfare charities, estimated to help approximately 4 million animals.[^5] The 2024 EA Survey showed animal welfare joining the top tier of cause prioritization among community members.[^14] Open Wing Alliance secured 141 new cage-free commitments in 2024 alone, per Open Philanthropy's 2024 progress report.[^36]

**Counterfactual note:** Corporate cage-free commitments began accelerating in 2015–2016, partly driven by Humane Society campaigns and EU regulatory trends that predated significant EA funding of animal welfare campaigns. Open Wing Alliance (the main EA-funded vehicle) estimates it played a more decisive role in accelerating commitments in Southeast Asia and Latin America specifically, where independent consumer pressure was lower — a more plausible counterfactual claim than for US/EU markets.[^5] No published study or comparative campaign analysis has been identified that quantitatively estimates the geographic differential in EA-attributed impact versus broader societal trends.

**Quality of fulfillment data:** The 92% cage-free fulfillment figure tracks whether companies made formal policy transitions, not verified supply chain welfare outcomes at the farm level. The gap between corporate policy adoption and verified welfare improvement at the farm level remains an open measurement challenge.

### AI Safety and Existential Risk Policy

The longtermist wing can point to growing policy influence. California and New York have signed bills directly related to AI risk regulation, following advocacy efforts supported by EA-aligned organizations.[^5] The Existential Risk Persuasion Tournament (XPT), which ran June–October 2022 (approximately 50 domain experts, approximately 50 superforecasters), found that approximately 42% of expert existential risk forecasters reported having attended an EA meetup, compared to 9% of superforecasters — providing the closest published proxy for EA community overlap with the x-risk researcher pool.[^35] This measures conference attendance, not career origin, and does not establish that EA was the primary cause of participants' entry into the field. AI safety has expanded significantly within AI/ML research communities, with historical ties to rationalist communities like <EntityLink id="E538">LessWrong</EntityLink> playing a role in building early researcher networks.[^15]

A 2022 survey of AI safety researchers at organizations including OpenAI, DeepMind, FAR AI, Open Philanthropy, Rethink Priorities, MIRI, Redwood, and GovAI found that all respondents had at least familiarity with EA and/or rationalist communities, with most being actively involved in at least one — and that *Superintelligence* and 80,000 Hours writing were each mentioned by three people as influential on their decision to work on AI safety.[^15] This is consistent with substantial EA influence on early researcher recruitment but does not establish the proportion of current AI safety researchers at major labs who entered the field through EA channels; no such systematic survey has been published.

**Counterfactual note on AI safety wins:** The central attribution challenge is temporal. EA began funding AI safety in 2015 when it was a niche concern; by 2023–2024, AI safety was mainstream following ChatGPT's release. Policy wins like Bletchley and Seoul were substantially driven by post-ChatGPT political momentum rather than EA-specific advocacy. The field would have grown substantially with the commercial AI trajectory regardless of EA funding. EA's contribution is more plausibly attributed to earlier field-building (2015–2021) than to post-2022 policy outcomes. A "commonly cited estimate" that EA meaningfully accelerated AI safety policy timelines has circulated in community discussions, but no named source, published model, or methodology has been identified — this claim should be treated as a qualitative community judgment rather than a quantitative estimate.

### Community and Funding Infrastructure

<EntityLink id="E863">Giving What We Can</EntityLink> reached 10,000 members taking the 10% pledge, with each pledge estimated to generate \$15,000 in counterfactual donations to high-impact charities over a lifetime, per GWWC's Lifetime Giving Method.[^47] Partnership pledge drives at EA Global and EAGx events generated 203 new pledges during Q2–Q4 2024, estimated to produce \$9.8 million in lifetime donations.[^16] This \$9.8M figure is derived from GWWC's lifetime donation methodology applied to new pledges; the per-pledge lifetime estimate uses empirical GWWC retention data, external reference classes, and temporal discounts including inflation and global catastrophic risk uncertainty.[^47] <EntityLink id="E552">Open Philanthropy</EntityLink> directed \$87 million to GiveWell-recommended charities in 2024.[^36]

---

## Documented Losses and Setbacks

### The FTX Collapse

The collapse of FTX and the fraud conviction of <EntityLink id="E857">Sam Bankman-Fried</EntityLink>, who had publicly framed his financial activities using longtermist reasoning about maximizing utility, damaged EA's credibility and raised concerns about ends-justify-the-means thinking within utilitarian frameworks.[^6] Total EA grantmaking has been on a downward trend since 2022.[^11]

The <EntityLink id="E855">FTX Future Fund</EntityLink> launched in February 2022 and shut down in November 2022. During its existence, it made grants worth approximately \$100 million and committed to \$160 million in total grantee commitments as of September 2022.[^50] The fund's team resigned en masse, stating they were "unable to perform our work or process grants" and had "fundamental questions about the legitimacy and integrity of the business operations" funding the Future Fund.[^51] All unfulfilled commitments were voided when FTX entered bankruptcy.

Among affected organizations: SBF's biggest grants went to pandemic prevention and EA institutions including <EntityLink id="E517">CEA</EntityLink>, the Long Term Future Fund, Lightcone Infrastructure, The Atlas Fellowship, and Constellation. University-affiliated research initiatives received more than \$13 million in total; twenty academics at institutions including Cornell, Princeton, Brown, and Cambridge received individual grants exceeding \$100,000 each.[^52] Other affected grantees included HelixNano and Our World in Data.[^53] The full organizational landscape — which entities closed versus found alternative funding — has not been comprehensively documented in any single public source.

CEA's own reports and the 2025 EA Forum funding data post document the subsequent multi-year decline in forum engagement, EAG attendance, EA Funds donations, and virtual programming participation that followed the collapse.[^11][^16]

### The Closure of the Future of Humanity Institute

The closure of the <EntityLink id="E140">Future of Humanity Institute</EntityLink> (FHI) at Oxford on April 16, 2024 represents the largest single institutional loss in the longtermist research ecosystem.[^43] Founded in 2005 as part of the Faculty of Philosophy and the Oxford Martin School, FHI had Nick Bostrom as director and staff including Anders Sandberg and <EntityLink id="E355">Toby Ord</EntityLink>. The institute coined much of the field's foundational terminology — "existential risk," "existential hope," "information hazard," "unilateralist's curse" — and hosted research that shaped global biosecurity and AI safety policy debates.

Beginning in 2020, the Faculty of Philosophy imposed a freeze on FHI fundraising and hiring. In late 2023, the Faculty announced it would not renew contracts of remaining FHI staff. Senior researchers attempted multiple rescue options — leadership restructuring, joining a college, spinning out of the university entirely, and an interdepartmental transfer to Physics — none succeeded. The University of Oxford's closing statement cited "increasing administrative headwinds within the Faculty of Philosophy."[^43] The specific reasons the Faculty imposed constraints on FHI have not been made fully public; factors cited in reporting include bureaucratic disputes over FHI's fast-moving, externally-networked operating style, personnel issues, and the controversy over a resurfaced 1996 email from Bostrom.[^44c]

FHI's final report, authored by Anders Sandberg, stated that "FHI's mission has replicated and spread and diversified" into dozens of organizations and thousands of individuals — noting successor institutions including the <EntityLink id="E364">UK AI Safety Institute</EntityLink>, the Global Priorities Institute (also at Oxford), and the Institute for Ethics in AI.[^43] Oxford retains other EA-adjacent research institutions, limiting but not eliminating the institutional loss.

### Internal Tensions

The shift toward longtermism has generated friction within the EA community. Some EA advocates focused on global health and animal welfare have reported that longtermism and x-risk work has made their lives harder, worsened their reputations, and occupied valued community niches.[^7] The 2024 EA Survey revealed diverging priorities: highly engaged respondents rated AI risks, animal welfare, and EA movement building more highly, while less engaged members emphasized climate change, global health, and poverty.[^14] An EA Forum post on the relationship between the EA community and AI safety noted that AI safety dominance has alienated global health and development EAs, creating perceptions of exclusion at events, and that community members sometimes perceive EA as an AI/longtermism-only movement — a perception that risks talent loss in other cause areas.[^18]

### Funding Decline

While GiveWell has maintained relatively stable and growing funding from non-Open Philanthropy donors (approximately \$415M raised in 2024), the broader EA funding landscape contracted since the FTX collapse.[^11][^28] Open Philanthropy's available assets fell by roughly half over the course of 2022, though they had since recovered about half of the total losses by 2023; this led Open Philanthropy to raise the cost-effectiveness bar for Global Health and Wellbeing grants by roughly a factor of two.[^37] Open Philanthropy does not publish audited balance sheets publicly; the "recovered half" figure appears in a Coefficient Giving summary of Open Philanthropy's own communications rather than in an independently audited source.[^37]

<EntityLink id="E202">MIRI</EntityLink>'s strategic pivot away from technical alignment research (announced January 2024) was one visible indicator of funding pressure and strategic reorientation at longtermist-focused organizations. MIRI's annual spending ranged from \$5.4M–\$7.7M during 2019–2023, with a high in 2020 and a low in 2022; projected 2024 spending was \$5.6M. MIRI's own framing of the change was a strategic scaling back rather than a traditional layoff.[^38]

### Policy Setbacks

California SB 1047's veto by Governor Newsom in September 2024 was the most visible EA-supported AI safety policy loss. The bill would have imposed safety requirements on large AI model developers. Anthropic's position on the bill evolved: the company initially stated it did not support SB 1047 in its original form in July 2024, proposed specific amendments (including removing pre-harm civil penalty authority for the attorney general and shifting the legal standard from "reasonable assurance" to "reasonable care"), saw those amendments partially adopted, then offered cautious support for the amended version in August 2024 — while noting "there are still some aspects of the bill which seem concerning or ambiguous."[^39] At least 113 current and former AI company employees, including some at Anthropic, signed a letter to Newsom supporting the bill before his veto.[^39b] Newsom vetoed the bill citing economic competitiveness concerns.

---

## Counterfactual Attribution Analysis

Evaluating EA's impact requires distinguishing between outcomes EA caused and outcomes EA coincided with. The following framework summarizes attribution confidence across cause areas.

**High-confidence EA attribution:**
- GiveWell's redirection of individual donor funds to specific charities: GiveWell's recommendation methodology is the proximate cause of most directed funds; without GiveWell, many donors would have given to lower-impact charities or not at all. GiveWell's counterfactual methodology is documented publicly, though specific estimates vary by grantee and funding round.[^34]
- Charity Entrepreneurship incubatees: organizations including LEEP that would not exist without CE's program and which have documented policy wins in multiple countries.
- <EntityLink id="E863">Giving What We Can</EntityLink> pledges: the pledge mechanism directly influenced donation behavior for 10,000+ members, with methodology documented in GWWC's 2023–2024 impact evaluation.[^47]

**Moderate-confidence EA attribution:**
- AI safety researcher pipeline (pre-2022): EA significantly influenced career entry for many researchers now at AI safety organizations. Evidence includes the XPT data (42% of expert x-risk forecasters attended EA meetups)[^35] and qualitative surveys showing universal familiarity with EA among early AI safety researchers.[^49] EA's marginal contribution to the field's current size is harder to isolate given the post-2022 commercial AI boom.
- Corporate animal welfare commitments in emerging markets: EA-funded campaigns have a more plausible counterfactual impact in markets with less independent consumer pressure (Southeast Asia, Latin America) than in US/EU markets where broader societal trends were also driving change. No published comparative analysis quantifying this geographic differential has been identified.
- Open Philanthropy's R21 co-funding: contributory but not decisive; Wellcome Trust and Serum Institute of India were primary funders.

**Low-confidence EA attribution:**
- Multilateral AI safety declarations (Bletchley, Seoul): post-ChatGPT political momentum was the primary driver; EA-aligned researchers contributed technical input but no named individuals or government acknowledgments documenting EA's specific causal role have been published.
- <EntityLink id="E127">EU AI Act</EntityLink> provisions: primarily driven by EU parliamentary process and civil society groups; EA input was marginal.
- US AI safety mainstream adoption post-2022: the field scaled with AI capabilities, not primarily with EA funding.

**Active attribution disputes:**
- Whether longtermist cause prioritization crowded out neartermist funding. Global health advocates argue yes; longtermists argue the donor pools are largely separate. Open Phil's own published allocation data (70% global health / 30% longtermist through 2022) partially addresses this for financial capital, but the question of talent and attention allocation is harder to resolve.[^27]
- Whether EA's early AI safety focus shaped <EntityLink id="E218">OpenAI</EntityLink>'s and <EntityLink id="E22">Anthropic</EntityLink>'s safety cultures, versus those organizations developing safety norms independently.

---

## Criticisms and Limitations

### Longtermist Modeling Vulnerabilities

Research published on the EA Forum has identified critical vulnerabilities in longtermist calculations: small changes in baseline existential risk assumptions can dominate tenfold differences in period risk reduction estimates, and the value of existential risk mitigation remains highly sensitive to estimated future risk levels.[^8] The "time of perils hypothesis" — used to defend both the claim that existential risks are high and that reducing them is especially valuable — requires speculative claims about how risks will diminish after a certain period.[^8]

Philosopher Steven Pinker has critiqued longtermism for potentially prioritizing any scenario, no matter how improbable, as long as it can be framed as having arbitrarily large effects far in the future.[^2] A related concern is that longtermism's predictive confidence should be very low given the exponential branching of possible futures.[^2]

### Systemic and Philosophical Objections

Critics argue that EA focuses on symptoms via charity rather than addressing root causes like institutional reform, debt, and power inequality. Philosopher Amia Srinivasan has argued that "Effective Altruism doesn't try to understand how power works, except to better align itself with it. In this sense it leaves everything just as it is."[^40] In academic formulations, Srinivasan contends that by insisting on quantifying the value of actions in terms of potential effects and probabilities of success, EA "effectively endorses the prevailing global capitalist institutional order — precisely the order that must be changed to address global poverty meaningfully."[^41] Nathan Robinson published a widely discussed critique in *Current Affairs* in 2023 making related institutional arguments.[^42] Brian Berkey's peer-reviewed "Institutional Critique of Effective Altruism" (*Utilitas*, Cambridge, 2017) formalizes these claims in academic philosophy.[^41]

EA proponents have responded to institutional critiques by noting that 80,000 Hours' stated priorities include "mitigating great power conflict," "global governance," and "space governance" — pointing to engagement with structural questions that go beyond individual charity.[^42b]

### Evidence Standards and Epistemic Concerns

GiveWell itself has acknowledged that seeking strong evidence and a straightforward, documented case for impact can be in tension with maximizing impact — reflecting a deep epistemic split within EA between evidence-based giving and longtermism's higher-risk, higher-reward approach.[^6] Jacob Steinhardt has argued that EA has a history of making overconfident claims with insufficient research, citing Peter Singer's 2009 claim that a life could be saved for \$200 — a figure substantially revised by 2011.[^22]

Ben Kuhn has argued that EA exhibits "epistemic inertia," where community consensus becomes less responsive to new evidence and arguments, partly because group norms fail to adequately guard against motivated cognition.[^23]

---

## Current Strategic Direction

<EntityLink id="E517">CEA</EntityLink> is pursuing what it describes as a growth-focused stewardship strategy for 2025–2026, aiming to build sustainable momentum. The organization acknowledges that meaningful results may not materialize until the end of 2026, as it requires building new infrastructure and iterating on strategy.[^16]

<EntityLink id="E862">Will MacAskill</EntityLink> has argued in a widely discussed October 2025 post that EA should expand beyond traditional cause areas to address the transition to a post-AGI society, rejecting both "legacy movement" and "refocus on classic causes" framings. MacAskill specified that in terms of what people do, perhaps 15% of EA effort (scaling to 30% or more over time) should be primarily working on cause areas beyond the "classic" four (AI safety, biorisk, animal welfare, global health) — including AI welfare, AI character, AI persuasion and epistemic disruption, human power concentration, and space governance. He further proposed that in terms of curriculum content, perhaps 30% or more should cover non-classic cause areas.[^24] These figures are from MacAskill's own October 2025 post, based on a memo written for the Meta Coordination Forum, and are not derived from an independent survey or community mandate.

GiveWell continues recommending charities in mental health, malnutrition, and lead exposure while broadening its work beyond charity evaluation to identify new grantees and explore underexplored cause areas.[^13] Longtermist projects represented less than one-third of total EA funding as of August 2022, with the majority of EA work falling outside longtermist frameworks.[^25]

---

## Key Uncertainties

- **Post-FTX funding trajectory**: Whether EA grantmaking stabilizes at current levels, continues declining, or recovers remains unclear. <EntityLink id="E552">Open Philanthropy</EntityLink> has begun efforts to diversify its donor base beyond Good Ventures.[^36] No comprehensive public breakdown of total EA-aligned giving for 2023 or 2024 across all funders has been published, making quantification of the post-2022 decline difficult.
- **AI safety policy impact**: While California and New York have passed AI safety legislation, the actual effect of these bills on reducing catastrophic AI risk is untested and contested.
- **Longtermist epistemology**: The sensitivity of longtermist calculations to baseline assumptions about existential risk levels means that the case for longtermist interventions could strengthen or weaken substantially with new evidence.[^8]
- **Community cohesion**: Whether EA can maintain unity across its neartermist and longtermist wings, or whether increasing specialization leads to effective fragmentation, remains an open question.[^7][^14]
- **Reputational recovery**: The extent to which the FTX scandal permanently affected EA's ability to attract talent and funding versus representing a temporary setback is still unfolding.[^6]
- **AI safety attribution**: As AI safety becomes mainstream post-ChatGPT, EA's distinctive comparative advantage in the field may diminish even as absolute activity increases — a strategic question the community has not fully resolved.[^18]
- **Cage-free commitment quality**: The 92% fulfillment figure for commitments with 2024 deadlines tracks whether companies made formal policy transitions, not verified supply chain welfare outcomes at the farm level. The gap between corporate policy adoption and verified welfare improvement remains an open measurement challenge.
- **FHI institutional legacy**: Whether the closure of FHI represents a net setback for longtermist research — or whether successor organizations (UK AISI, Global Priorities Institute, etc.) have absorbed and expanded FHI's research agenda — is contested within the community.[^43]
- **Mental health as EA cause area**: GiveWell recommends StrongMinds as a top charity focused on mental health, but no aggregate impact metrics for mental health interventions comparable to the lives-saved estimates for malaria or deworming programs have been published in this page's tracked data.

---

## Sources

[^1]: [Effective Altruism - Wikipedia](https://en.wikipedia.org/wiki/Effective_altruism)
[^2]: [How Effective Altruism Lost Its Way - Quillette](https://quillette.com/2023/12/21/how-effective-altruism-lost-its-way/)
[^3]: [Against Malaria Foundation — Impact Metrics](https://www.againstmalaria.com/Impact.aspx); [GiveWell — Against Malaria Foundation Review](https://www.givewell.org/charities/amf)
[^4]: [CEA Is Growing Again: 25% More People Engaged - EA Forum](https://forum.effectivealtruism.org/posts/NKrpEdg7wAgcoHNRG/cea-is-growing-again-25-more-people-engaged-with-our-1); [Building Sustainable Momentum: Progress Report on CEA's 2025–26 Strategy - EA Forum (February 2026)](https://forum.effectivealtruism.org/posts/Dy4iGHbAkKAQ4t2Dw/building-sustainable-momentum-progress-report-on-cea-s-2025)
[^5]: [Celebrating Wins Discussion Thread - EA Forum](https://forum.effectivealtruism.org/posts/KN7BTMJHAQkLb6Bac/celebrating-wins-discussion-thread)
[^6]: [How Effective Altruism Lost Its Way - Quillette](https://quillette.com/2023/12/21/how-effective-altruism-lost-its-way/)
[^7]: [EA and Longtermism: Not a Crux for Saving the World - EA Forum](https://forum.effectivealtruism.org/posts/cP7gkDFxgJqHDGdfJ/ea-and-longtermism-not-a-crux-for-saving-the-world)
[^8]: [Sensitive Assumptions in Longtermist Modeling - EA Forum](https://forum.effectivealtruism.org/posts/2eYxQDbPbqHypFihe/sensitive-assumptions-in-longtermist-modeling)
[^10]: [Effective Altruism, Longtermism, and William MacAskill Interview - TIME](https://time.com/6204627/effective-altruism-longtermism-william-macaskill-interview/)
[^11]: [Historical EA Funding Data: 2025 Update - EA Forum](https://forum.effectivealtruism.org/posts/NWHb4nsnXRxDDFGLy/historical-ea-funding-data-2025-update)
[^13]: [EA Organization Updates Thread: February 2026 - EA Forum](https://forum.effectivealtruism.org/posts/HetCMJE49JCXLPGYW/ea-organization-updates-thread-february-2026)
[^14]: [EA Community Cause Prioritization - Rethink Priorities Research Digest](https://rpresearchdigest.substack.com/p/effective-altruism-community-cause-prioritization)
[^15]: [AI Safety and Neighboring Communities: A Quick Start Guide - Alignment Forum](https://www.alignmentforum.org/posts/EFpQcBmfm2bFfM4zM/ai-safety-and-neighboring-communities-a-quick-start-guide-as)
[^16]: [Stewardship: CEA's 2025-26 Strategy - EA Forum](https://forum.effectivealtruism.org/posts/QkE35oTsasMn37o7R/stewardship-cea-s-2025-26-strategy-to-reach-and-raise-ea-s-1)
[^18]: [Relationship Between EA Community and AI Safety - EA Forum](https://forum.effectivealtruism.org/posts/opCxiPwxFcaaayyMB/relationship-between-ea-community-and-ai-safety)
[^22]: [Another Critique of Effective Altruism - LessWrong](https://www.lesswrong.com/posts/CZmkPvzkMdQJxXy54/another-critique-of-effective-altruism)
[^23]: [EA Critique - Ben Kuhn](https://www.benkuhn.net/ea-critique/)
[^24]: [Effective Altruism in the Age of AGI - Will MacAskill Substack](https://willmacaskill.substack.com/p/effective-altruism-in-the-age-of-agi)
[^25]: [Longtermism - Wikipedia](https://en.wikipedia.org/wiki/Longtermism)
[^26]: [Open Philanthropy — Our Grants](https://www.openphilanthropy.org/grants/)
[^27]: [Open Philanthropy Wikipedia](https://en.wikipedia.org/wiki/Open_Philanthropy_(organization))
[^28]: [GiveWell 2024 Metrics and Impact](https://blog.givewell.org/2025/08/13/givewells-2024-metrics-and-impact/)
[^29]: [Growth and Engagement in EA Groups: 2022 Groups Census Results - EA Forum](https://forum.effectivealtruism.org/posts/aetatCMGqcAPsNbLs/growth-and-engagement-in-ea-groups-2022-groups-census)
[^30]: [Centre for Effective Altruism Global Reach](https://www.centreforeffectivealtruism.org/about)
[^31]: [What Has the CEA Uni Groups Team Been Up To? – Our 2023/2024 Review - EA Forum](https://forum.effectivealtruism.org/posts/FTbTfpR7txzzb9akm/what-has-the-cea-uni-groups-team-been-up-to-our-2023-2024)
[^32]: [Evidence Action Expands to Tanzania to Eliminate Parasitic Worms as a Public Health Problem - Evidence Action](https://www.evidenceaction.org/newsroom/evidence-action-expands-to-tanzania-to-eliminate-parasitic-worms-as-a-public-health-problem)
[^33]: [Deworm the World: Update 2025 - GFDW](https://www.gfdw.eu/en/blog/deworm-the-world)
[^34]: [2023 Cost-Effectiveness Analysis Changelog - GiveWell](https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models/changelog-2023)
[^35]: [Announcing 'Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament' - EA Forum](https://forum.effectivealtruism.org/posts/un42vaZgyX7ch2kaj/announcing-forecasting-existential-risks-evidence-from-a)
[^36]: [Our Progress in 2024 and Plans for 2025 - Open Philanthropy](https://www.openphilanthropy.org/research/our-progress-in-2024-and-plans-for-2025/)
[^37]: [Our Progress in 2023 and Plans for 2024 - Open Philanthropy (via Coefficient Giving)](https://coefficientgiving.org/research/our-progress-in-2023-and-plans-for-2024/)
[^38]: [MIRI's 2024 End-of-Year Update - intelligence.org](https://intelligence.org/2024/12/02/miris-2024-end-of-year-update/)
[^39]: [Anthropic CEO Backs New California AI Legislation, with Some Reservations - Pure AI (August 23, 2024)](https://pureai.com/Articles/2024/08/23/Anthropic-CEO-Backs-SB-1047.aspx); [Anthropic does not support California AI bill SB 1047 - Axios (July 25, 2024)](https://www.axios.com/2024/07/25/exclusive-anthropic-weighs-in-on-california-ai-bill)
[^39b]: [Over 100 AI Employees Wrote to Gov. Newsom Urging Him to Sign SB 1047 - Wired (September 2024)](https://www.wired.com/story/ai-employees-letter-sb-1047-newsom/)
[^40]: [Why Am I Not an Effective Altruist? - Why Philanthropy Matters (citing Srinivasan)](https://whyphilanthropymatters.com/article/why-am-i-not-an-effective-altruist/)
[^41]: [The Institutional Critique of Effective Altruism - Brian Berkey, Utilitas (2017)](https://www.cambridge.org/core/journals/utilitas/article/abs/institutional-critique-of-effective-altruism/91A0449E2F030BAE417A09E52599E605)
[^42]: [Why Effective Altruism and Longtermism Are Toxic Ideologies - Current Affairs](https://www.currentaffairs.org/news/2023/05/why-effective-altruism-and-longtermism-are-toxic-ideologies)
[^42b]: [80,000 Hours Problem Profiles](https://80000hours.org/problem-profiles/)
[^43]: [Future of Humanity Institute - Wikipedia](https://en.wikipedia.org/wiki/Future_of_Humanity_Institute); [FHI Closing Statement - fhi.ox.ac.uk](https://www.fhi.ox.ac.uk/2024/04/fhi-is-closing/)
[^44]: [Centre for Effective Altruism - Wikipedia](https://en.wikipedia.org/wiki/Centre_for_Effective_Altruism)
[^44b]: [Giving What We Can — More than 7,000 pledges (Oxford press release, 2022)](https://www.ox.ac.uk/news/2022-03-01-oxford-based-charity-receives-more-25-billion-pledges-community-effective-givers)
[^44c]: [FHI Is Closing — Discussion Thread (LessWrong, April 2024)](https://www.lesswrong.com/posts/gkRpV3TzsPBTerKdQ/fhi-is-closing)
[^45]: [Effective Altruism Is Growing Fast (80,000 Hours, 2021)](https://80000hours.org/2021/08/effective-altruism-growing/)
[^46]: [Open Philanthropy — Our Grants Database](https://www.openphilanthropy.org/grants/)
[^47]: [Giving What We Can Impact Evaluation 2023–2024](https://www.givingwhatwecan.org/blog/how-we-evaluate-our-impact)
[^48]: [The 2024 EA Survey — Cause Prioritization (Rethink Priorities)](https://rethinkpriorities.org/publications/the-2024-ea-survey-cause-prioritization)
[^50]: [FTX Future Fund — About (archived)](https://web.archive.org/web/20221020000000*/ftxfuturefund.org)
[^51]: [The FTX Future Fund Team Has Resigned - EA Forum](https://forum.effectivealtruism.org/posts/xafpj3on76uRWSAoK/the-ftx-future-fund-team-has-resigned)
[^52]: [Sam Bankman-Fried's Charitable Empire Included Hundreds of Millions to EA Institutions — Inside Philanthropy (2023)](https://www.insidephilanthropy.com/home/2023/4/7/sam-bankman-frieds-charitable-empire-included-hundreds-of-millions-to-ea-institutions)
[^53]: [Grantees Affected by FTX Collapse Including Our World in Data (BBC, 2022)](https://www.bbc.com/news/technology-63601025)
