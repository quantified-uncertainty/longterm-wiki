---
numericId: E838
title: History
description: Timeline of AI safety as a field
sidebar:
  label: Overview
  order: 0
entityType: overview
clusters:
  - community
  - ai-safety
---
import {EntityLink} from '@components/wiki';


## Overview

This section traces the development of AI safety as a field, from early theoretical concerns to the current mainstream recognition of AI risks. Understanding this history helps contextualize current debates and institutional structures.

## Historical Eras

### <EntityLink id="miri-era">MIRI Era (2000-2015)</EntityLink>
The field's founding period, dominated by the <EntityLink id="miri">Machine Intelligence Research Institute</EntityLink>:
- <EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink>'s early writings on AI risk
- Founding of SIAI (later MIRI) in 2000
- Development of foundational concepts (orthogonality thesis, <EntityLink id="instrumental-convergence">instrumental convergence</EntityLink>)
- *Superintelligence* (2014) brings ideas to academic attention

### <EntityLink id="deep-learning-era">Deep Learning Era (2015-2022)</EntityLink>
Deep learning breakthroughs reshape the landscape:
- AlphaGo (2016) demonstrates superhuman capability
- GPT-2 (2019) shows language model potential
- <EntityLink id="anthropic">Anthropic</EntityLink> founded (2021) by former <EntityLink id="openai">OpenAI</EntityLink> safety team
- Growing recognition in ML community

### FTX/EA Crisis (2022)
The collapse of <EntityLink id="E856" name="ftx">FTX</EntityLink> exposed major fissures in EA-funded AI safety:
- <EntityLink id="E873" name="longtermism-credibility-after-ftx">FTX collapse and EA's public credibility</EntityLink> — November 2022 bankruptcy and reputational fallout
- <EntityLink id="E869" name="ea-epistemic-failures-in-the-ftx-era">EA epistemic failures in the FTX era</EntityLink> — governance, donor vetting, and cultural critiques
- <EntityLink id="E870" name="ea-institutions-response-to-the-ftx-collapse">EA institutions' response</EntityLink> — community surveys, trust damage, funding gaps
- <EntityLink id="E855" name="ftx-future-fund">FTX Future Fund</EntityLink> — \$132M in grants dissolved overnight
- <EntityLink id="longtermism-credibility-after-ftx">Longtermism's credibility after FTX</EntityLink> — philosophical and reputational questions

### <EntityLink id="early-warnings">Early Warnings (2022-2023)</EntityLink>
AI safety enters public consciousness:
- ChatGPT (Nov 2022) captures public attention
- Pause letter (March 2023) signed by prominent researchers
- <EntityLink id="geoffrey-hinton">Geoffrey Hinton</EntityLink> leaves Google to speak freely about risks
- Congressional hearings on AI safety

### <EntityLink id="mainstream-era">Mainstream Era (2023-Present)</EntityLink>
AI safety becomes a policy priority:
- Biden Executive Order on AI (Oct 2023)
- Bletchley Park AI Safety Summit (Nov 2023)
- <EntityLink id="ai-safety-institutes">AI Safety Institutes</EntityLink> established globally
- Major labs adopt <EntityLink id="rsp">responsible scaling policies</EntityLink>

## Key Milestones

| Year | Event | Significance |
|------|-------|--------------|
| 2000 | SIAI founded | First AI safety organization |
| 2014 | *Superintelligence* published | Brought ideas to academia |
| 2017 | Asilomar Principles | Early multi-stakeholder agreement |
| 2022 | FTX collapse | \$132M in EA/AI safety funding dissolved; major community reckoning |
| 2022 | ChatGPT released | Public awareness breakthrough |
| 2023 | UK AI Safety Summit | First major government summit |
| 2024 | <EntityLink id="eu-ai-act">EU AI Act</EntityLink> enacted | First comprehensive AI regulation |
