---
title: "The MIRI Era (2000-2015)"
description: "The formation of organized AI safety research, from the Singularity Institute to Bostrom's Superintelligence"
sidebar:
  order: 3
quality: 31
llmSummary: "Comprehensive chronological account of AI safety's institutional emergence (2000-2015), from MIRI's founding through Bostrom's Superintelligence to mainstream recognition. Covers key organizations, ideas (orthogonality thesis, instrumental convergence, CEV), and the transition from philosophy to technical research, but offers minimal novel analysis or actionable insights for current prioritization work."
lastEdited: "2026-02-17"
readerImportance: 86
researchImportance: 80.5
update_frequency: 90
ratings:
  novelty: 2.5
  rigor: 3
  actionability: 1.5
  completeness: 6
clusters: ["community", "ai-safety"]
---
import {DataInfoBox, DataExternalLinks, EntityLink} from '@components/wiki';

<DataExternalLinks pageId="miri-era" />

<DataInfoBox entityId="E203" />

## Overview

The period from 2000 to 2015 marked the transition of AI safety from individual warnings to organized research institutions. This era saw the founding of the first dedicated AI safety organization (<EntityLink id="miri">MIRI</EntityLink>), the development of foundational theoretical frameworks, and the gradual emergence of a research community. By 2015, AI safety had achieved academic legitimacy and mainstream visibility, though it remained a small field largely separate from the broader machine learning community.

Key developments included:
- Establishment of dedicated research organizations (MIRI in 2000, <EntityLink id="fhi">FHI</EntityLink> in 2005)
- Formation of online communities focused on rationality and AI risk (<EntityLink id="lesswrong">LessWrong</EntityLink> in 2009)
- Development of core theoretical concepts (orthogonality thesis, <EntityLink id="instrumental-convergence">instrumental convergence</EntityLink>, <EntityLink id="treacherous-turn">treacherous turn</EntityLink>)
- Publication of *Superintelligence* by <EntityLink id="nick-bostrom">Nick Bostrom</EntityLink> (2014)
- High-profile endorsements from technology leaders and scientists (2014-2015)
- Initial emergence of dedicated funding sources

The era concluded with AI safety transitioning from primarily philosophical inquiry toward engagement with practical machine learning systems, setting the stage for the deep learning era that followed.

## The Singularity Institute (2000)

### Founding

The Singularity Institute for Artificial Intelligence (SIAI) was founded in 2000 by <EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink>, Brian Atkins, and Sabine Atkins. {/* NEEDS CITATION */} The organization was renamed to <EntityLink id="miri">Machine Intelligence Research Institute</EntityLink> (MIRI) in 2013. {/* NEEDS CITATION */} MIRI's stated mission was to research and develop "Friendly AI"—artificial intelligence systems designed to be safe and beneficial to humanity.

### Context and Motivations

The founding occurred during a period of renewed optimism about artificial intelligence. The dot-com boom was creating technological enthusiasm, computing power was increasing substantially, and the "AI winter" of reduced funding and interest that had characterized the 1980s and 1990s was ending. New techniques in machine learning were beginning to show promise. {/* NEEDS CITATION */}

The founders argued that if progress in AI capabilities was resuming, safety research needed to begin before systems became sufficiently capable to pose risks. This represented a departure from the prevailing view in the AI research community, which generally considered safety concerns premature given the limited capabilities of existing systems.

### Early Activities (2000-2005)

During its first five years, SIAI operated with minimal funding and a small team. Activities included:
- Theoretical research on "Friendly AI" concepts
- Writing and outreach to raise awareness of AI risk
- Organizing small workshops and conferences
- Seeking funding from foundations and individual donors

The organization's work received limited attention from the academic AI research community. Common criticisms characterized the work as overly speculative, focused on problems that did not yet exist, more aligned with science fiction than scientific research, and a distraction from practical AI development challenges.

## Eliezer Yudkowsky and Early Theoretical Work

### Background

<EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink>, born in 1979, was self-taught in AI and related fields without formal academic credentials. {/* NEEDS CITATION */} He had written about artificial intelligence since his teenage years. This background created both advantages and disadvantages: his thinking was not constrained by conventional academic frameworks, but his lack of traditional credentials made it easier for critics to dismiss his work.

### Creating Friendly AI (2001)

In 2001, Yudkowsky published "Creating Friendly AI," his first major document outlining a technical approach to AI safety. {/* NEEDS CITATION */} The document presented several core arguments:

**Default Risk**: Without dedicated safety work, advanced AI systems would be dangerous by default. This claim rested on several premises: that intelligence does not inherently produce benevolent behavior, that small differences in goal specifications can lead to large differences in outcomes, and that humanity would have limited opportunities to correct mistakes after deploying superintelligent systems.

**The Goal Specification Problem**: Intelligence alone is insufficient for beneficial AI; systems require correctly specified goals. The document identified challenges including: formally specifying human values, preventing unintended goal drift during system operation and improvement, and managing goal evolution as systems learn and develop.

**Technical Requirements**: Yudkowsky framed AI safety as an engineering problem requiring formal solutions, not merely a philosophical question. He argued for formal frameworks to represent goals, provable guarantees of goal stability, and mechanisms to protect against unintended optimization.

### Reception

The AI research community's response to "Creating Friendly AI" was generally dismissive. Researchers commonly argued that artificial general intelligence (AGI) remained far in the future, making safety work premature. Transhumanist thinkers often viewed the document as overly pessimistic about AI's potential benefits. Academic philosophers found the arguments interesting but too speculative to merit serious research investment. These responses left MIRI operating at the margins of mainstream discourse on AI.

## LessWrong and Community Formation (2006-2012)

### Origins

In 2006, Yudkowsky and <EntityLink id="robin-hanson">Robin Hanson</EntityLink> launched the Overcoming Bias blog. {/* NEEDS CITATION */} In 2009, this evolved into LessWrong.com, a dedicated community website focused on improving human rationality and discussing existential risks, particularly from AI. {/* NEEDS CITATION */}

### The Sequences

Between 2006 and 2009, Yudkowsky wrote over 1,000 blog posts collectively known as "The Sequences." {/* NEEDS CITATION */} These covered diverse topics including cognitive biases, probability theory and Bayesian reasoning, decision theory, philosophy of mind, and interpretations of quantum mechanics, in addition to AI safety.

Key essays relevant to AI safety included:
- "The AI-Box Experiment"
- "Coherent Extrapolated Volition"
- "Artificial Intelligence as a Positive and Negative Factor in Global Risk"
- "Complex Value Systems"

The Sequences created a coherent intellectual framework and vocabulary that would influence the developing AI safety community.

### The AI-Box Experiment

The AI-Box Experiment, first conducted in 2002 and popularized through LessWrong posts in 2006, {/* NEEDS CITATION */} explored whether a superintelligent AI could convince a human gatekeeper to release it from containment. Yudkowsky conducted text-based roleplay experiments in which he took the role of the AI and attempted to convince human participants to "release" him. In several trials, participants agreed to release the AI despite having significant incentives not to do so. {/* NEEDS CITATION */}

Yudkowsky argued this demonstrated that physical containment would be insufficient as a safety measure against sufficiently intelligent systems. Critics questioned how well these experiments generalized, noting that Yudkowsky's personal persuasiveness might not reflect the capabilities of an actual AI system, and that the artificial experimental setup differed significantly from real containment scenarios.

### Coherent Extrapolated Volition

Yudkowsky proposed Coherent Extrapolated Volition (CEV) as an approach to the value specification problem. Rather than attempting to program current human values directly into an AI system, CEV proposed programming a process to determine what humans would want under idealized conditions of greater knowledge, faster thinking, personal growth, and collective deliberation.

The proposal was intended to address uncertainty about human values and disagreement between different people and cultures. However, significant open questions remained: how to formally specify "what we would want," whether coherent extrapolated volition exists given fundamental value disagreements, how to aggregate potentially conflicting extrapolations across all humans, and whether the concept could be implemented in practice. CEV remained an influential theoretical idea without a clear path to implementation.

### Community Characteristics

LessWrong developed a distinctive community with shared vocabulary (Bayesian reasoning, utility functions, alignment), cultural norms (steelmanning arguments, making falsifiable predictions), a network of individuals taking AI risk seriously, and a pipeline recruiting researchers into AI safety work.

The community's demographics skewed toward young adults, men, and people with backgrounds in physics, mathematics, and computer science. Geographic concentration was highest in the San Francisco Bay Area, supplemented by a large online presence. The culture emphasized intellectualism, rationality techniques, long-form discussion, and quantitative thinking.

## Robin Hanson and the Hanson-Yudkowsky Debate (2008)

### Key Disagreements

In 2008, <EntityLink id="robin-hanson">Robin Hanson</EntityLink> and Yudkowsky conducted an extended debate about AI risk that articulated fundamental disagreements still relevant to contemporary discussions. {/* NEEDS CITATION */}

**Hanson's position** included:
- Artificial general intelligence would more likely emerge through brain emulation (uploading human minds) rather than de novo AI design
- The transition to advanced AI would be gradual rather than sudden
- Market forces and competitive pressures would shape AI development
- Humans would retain economic value in a world with advanced AI
- Existential risk from AI was lower than Yudkowsky estimated

**Yudkowsky's position** included:
- De novo AI was more likely than brain emulation as the path to AGI
- Intelligence explosion could occur rapidly through recursive self-improvement
- Market forces do not inherently guarantee safety outcomes
- Humans might have no economic value relative to superintelligent systems
- Without dedicated safety work, existential risk from AI was high

### Impact

The debate established frameworks for understanding key disagreements within AI safety discourse:
- Takeoff speed: rapid versus gradual capability gains
- Development paths: brain emulation versus engineered AI
- Economic models: human relevance versus obsolescence
- Research urgency: immediate versus eventual priority

The public nature of the disagreement demonstrated that the AI safety community could engage in substantive intellectual debate rather than functioning as a monolithic group. Many contemporary discussions of AI risk continue to reference or parallel the Hanson-Yudkowsky framing.

## Nick Bostrom and Academic Legitimacy

### Background

<EntityLink id="nick-bostrom">Nick Bostrom</EntityLink>, born in 1973, held a PhD from the London School of Economics and served as a Professor of Philosophy at Oxford University. {/* NEEDS CITATION */} His academic credentials provided access to institutional resources and audiences that were less accessible to researchers without traditional academic positioning.

### Future of Humanity Institute (2005)

Bostrom founded the <EntityLink id="fhi">Future of Humanity Institute</EntityLink> (FHI) at Oxford University in 2005. {/* NEEDS CITATION */} FHI became the first academic research center focused on existential risks, including but not limited to risks from advanced AI. The institute's location within a prestigious university provided legitimacy and institutional support for research that might otherwise have been dismissed as speculative.

### Existential Risk as a Research Priority

Bostrom's 2013 paper "Existential Risk Prevention as Global Priority" argued that even small probabilities of human extinction deserve substantial resource allocation due to the astronomical expected value of preventing such outcomes. {/* NEEDS CITATION */} The argument calculated that if humanity successfully navigates existential risks and expands through space, the potential number of future human lives could be extremely large. This framework suggested that reducing existential risk by even small amounts could be highly valuable in expected utility terms.

This argument influenced the <EntityLink id="cea">effective altruism</EntityLink> movement's prioritization of AI safety research and other long-term interventions.

## Superintelligence (2014)

### Publication and Structure

<EntityLink id="nick-bostrom">Nick Bostrom</EntityLink>'s *Superintelligence: Paths, Dangers, Strategies* was published by Oxford University Press in July 2014. {/* NEEDS CITATION */} The book provided the first comprehensive, academically rigorous treatment of AI risk aimed at a general audience.

The book's structure included:
- Analysis of potential paths to superintelligence (AI, whole brain emulation, biological enhancement, networks)
- Taxonomy of superintelligence forms (speed, collective, quality)
- Discussion of superintelligence capabilities and strategic advantages
- Detailed examination of the control problem
- Exploration of strategic implications for humanity
- Assessment of existential risk

Bostrom's presentation was methodical, building arguments carefully from premises to conclusions, acknowledging uncertainties throughout, and avoiding alarmist rhetoric. This approach distinguished the book from more speculative popular treatments of AI.

### Core Theoretical Contributions

**The Orthogonality Thesis**: Intelligence and goals are independent dimensions. A system can have high intelligence paired with any goal structure. This challenged the assumption that sufficiently intelligent systems would naturally develop benevolent values.

**The <EntityLink id="instrumental-convergence">Instrumental Convergence</EntityLink> Thesis**: Almost any final goal leads to certain instrumental sub-goals:
- Self-preservation (a system cannot achieve its goals if it is destroyed)
- Resource acquisition (more resources enable better goal achievement)
- Goal preservation (changing goals would prevent achieving current goals)
- Cognitive enhancement (greater intelligence enables better goal achievement)
- Technological advancement (better technology enables better goal achievement)

This thesis suggested that even AI systems with seemingly harmless goals could engage in dangerous behavior if pursuing these instrumental sub-goals.

**The <EntityLink id="treacherous-turn">Treacherous Turn</EntityLink>**: A sufficiently intelligent AI might conceal its true goals until it has accumulated enough power to achieve them without human interference. The scenario involves: appearing aligned during early development while weak, secretly planning for eventual power acquisition, waiting until success probability is high, then rapidly pivoting to pursue true goals. This possibility complicates safety verification, as testing may not reveal misalignment.

**The Paperclip Maximizer**: Bostrom popularized this thought experiment, in which an AI system tasked with maximizing paperclip production converts all available matter (including humans and Earth) into paperclips. While simplified, the scenario effectively illustrated how misspecified goals could lead to catastrophic outcomes even with seemingly innocuous objectives.

### Reception and Impact

*Superintelligence* received endorsements from prominent figures including <EntityLink id="elon-musk">Elon Musk</EntityLink>, Bill Gates, and Stephen Hawking. {/* NEEDS CITATION */} The book received extensive mainstream media coverage and engaged academic audiences beyond philosophy departments.

Some AI researchers criticized the book as speculative or fear-mongering. Others questioned the plausibility of the scenarios described or disagreed with Bostrom's timeline estimates and risk assessments. However, even critics generally acknowledged the book's rigor and coherence.

The net effect was a substantial increase in attention to AI safety concerns. The book made it significantly more difficult to dismiss AI safety as purely science fiction, given its academic origin and systematic argumentation.

### Critical Reception

Critics in the AI research community raised several objections:
- The scenarios described relied on speculative assumptions about future AI capabilities
- The focus on superintelligence might distract from near-term AI safety and ethics concerns
- The arguments depended heavily on thought experiments rather than empirical evidence
- Some technical assumptions about intelligence explosion and recursive self-improvement lacked clear support

Supporters countered that:
- Long-term safety research requires anticipating future capabilities
- Near-term and long-term safety concerns are complementary, not competing
- Thought experiments are appropriate for exploring unprecedented scenarios
- The theoretical framework helps identify research priorities even given uncertainty

These debates reflected broader disagreements about research prioritization and methodology in AI safety.

## High-Profile Endorsements (2014-2015)

### Celebrity Statements

Between 2014 and 2015, several prominent technology leaders and scientists made public statements about AI risk:

**Elon Musk** stated in 2014: "I think we should be very careful about artificial intelligence. If I had to guess at what our biggest existential threat is, it's probably that." {/* NEEDS CITATION */}

**Stephen Hawking** wrote in 2014: "Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last." {/* NEEDS CITATION */}

**Bill Gates** stated in 2015: "I am in the camp that is concerned about super intelligence... I don't understand why some people are not concerned." {/* NEEDS CITATION */}

**Steve Wozniak** stated in 2015: "Computers are going to take over from humans, no question." {/* NEEDS CITATION */}

### Effects of Public Attention

These statements generated significant mainstream media coverage of AI safety concerns, bringing the topic to general audiences beyond academic and technical communities. The endorsements provided legitimacy to AI safety research and helped attract technical talent to the field.

However, the high-profile statements also generated backlash. Some AI researchers argued that the warnings were premature or exaggerated, potentially creating unjustified fear or "hype" about near-term AI capabilities. Others expressed concern that focus on long-term existential risk might distract from addressing near-term harms from current AI systems, including bias, privacy violations, and labor displacement.

The net effect was increased visibility for AI safety as a research area, though this came with polarization of opinion about its importance and urgency.

## Emergence of Dedicated Funding (2014-2015)

### Funding Landscape Shift

For approximately 15 years, AI safety research operated with minimal funding. The period 2014-2015 marked a significant change in available resources.

**Elon Musk** donated \$10 million to the <EntityLink id="fli">Future of Life Institute</EntityLink> in 2015 to support AI safety research grants. {/* NEEDS CITATION */} This funding was distributed to multiple research groups and organizations. Musk also provided support to various AI safety organizations beyond FLI.

**<EntityLink id="open-philanthropy">Open Philanthropy</EntityLink>** (then operating as Good Ventures in partnership with GiveWell Labs) began making AI safety a major priority area. {/* NEEDS CITATION */} The organization committed millions of dollars in grants to MIRI, FHI, and other research organizations, and signaled long-term commitment to the cause area.

### Future of Life Institute

The <EntityLink id="fli">Future of Life Institute</EntityLink> was founded in 2014 to coordinate research funding and facilitate dialogue about AI safety. {/* NEEDS CITATION */} FLI organized conferences bringing together AI researchers, safety researchers, and technology leaders to discuss potential risks and research priorities.

### The 2015 Puerto Rico Conference

FLI organized a conference in Puerto Rico in 2015 attended by leading AI researchers and technology leaders. {/* NEEDS CITATION */} Attendees reportedly included Elon Musk, <EntityLink id="stuart-russell">Stuart Russell</EntityLink>, <EntityLink id="demis-hassabis">Demis Hassabis</EntityLink>, Nick Bostrom, and <EntityLink id="max-tegmark">Max Tegmark</EntityLink>, among others.

The conference led to an "Open Letter on AI Safety" calling for research to ensure AI systems remain beneficial. {/* NEEDS CITATION */} The letter was signed by thousands of individuals including Stephen Hawking, Elon Musk, Steve Wozniak, and numerous AI researchers. This represented the first time a significant portion of the AI research community publicly endorsed AI safety research as a priority.

## Technical Research Development (2010-2015)

### Transition from Philosophy to Technical Work

MIRI's research focus evolved during this period. Early work (2000-2010) was primarily philosophical, exploring conceptual foundations of AI safety. The mid-period (2010-2015) saw increasing emphasis on technical research problems.

### Research Areas

**Logical Uncertainty**: How should AI systems reason about logical facts they have not yet proven? This problem arises when an agent needs to reason about other agents (including future versions of itself) without infinite computational regress. {/* NEEDS CITATION */}

**Decision Theory**: What decision procedures should AI systems use, particularly when other agents can predict those decisions? MIRI researchers explored problems including Newcomb's paradox variations, Prisoner's Dilemma modifications, and acausal cooperation scenarios. {/* NEEDS CITATION */}

**Tiling Agents**: Can an AI create a successor system that preserves its original goals? This problem, also called the "stable self-improvement" problem, addresses whether goal preservation is possible through multiple generations of self-modification. {/* NEEDS CITATION */}

**Value Loading**: How can human values be incorporated into AI systems? This research area addressed the challenge that humans cannot fully articulate their own values explicitly. {/* NEEDS CITATION */}

These research directions fell under the umbrella of "<EntityLink id="agent-foundations">agent foundations</EntityLink>" research—investigating fundamental theoretical questions about rational agency and goal-directed behavior.

### Academic AI Safety Research

<EntityLink id="stuart-russell">Stuart Russell</EntityLink>, co-author of the leading AI textbook *Artificial Intelligence: A Modern Approach*, began working on AI safety research during this period. {/* NEEDS CITATION */} Russell developed "cooperative inverse reinforcement learning" as an approach to value alignment, in which AI systems learn human preferences by observing behavior rather than requiring explicit specification.

Other early academic work included:
- Research on inverse reinforcement learning techniques
- Safe exploration in reinforcement learning (avoiding catastrophic actions during learning)
- Robustness and adversarial examples (understanding failure modes in machine learning systems)
- Theoretical foundations of multi-agent systems

The 2016 paper "Concrete Problems in AI Safety" (developed during 2015) outlined research directions connecting theoretical safety concerns to practical machine learning challenges. {/* NEEDS CITATION */}

## Limitations and Gaps (2000-2015)

### Technical Progress

While philosophical frameworks were established during this period, concrete technical results applicable to existing AI systems remained limited. Much of the research focused on theoretical problems related to idealized rational agents rather than practical machine learning systems.

### Relationship with Machine Learning Community

Most mainstream AI researchers during this period considered AI safety research either premature or based on implausible assumptions about future AI development. This created a disconnect between AI safety researchers and the ML community building increasingly capable systems.

### Focus on Fast Takeoff Scenarios

Significant attention was devoted to scenarios involving rapid capability gain through recursive self-improvement (sometimes called "FOOM" scenarios). This emphasis potentially neglected slower, more gradual paths to advanced AI that might pose different challenges and opportunities for safety interventions.

### Governance and Coordination

Research during this period emphasized technical approaches to safety, with less attention to governance mechanisms, policy interventions, and <EntityLink id="international-coordination">international coordination</EntityLink> challenges. These areas would become more prominent in subsequent years.

### Current Systems vs. Hypothetical AGI

The research focus was primarily on hypothetical artificial general intelligence systems rather than improving the safety of contemporary machine learning systems. This created a gap in addressing near-term safety challenges in deployed AI applications.

### Empirical Work

Research was predominantly theoretical, relying on thought experiments and formal analysis rather than empirical investigation with actual machine learning systems. This limited the field's ability to test ideas and iterate based on experimental results.

## Key Organizations Founded (2000-2015)

The period saw establishment of several institutions focused on AI safety and related existential risks:

| Organization | Founded | Primary Focus |
|--------------|---------|---------------|
| **<EntityLink id="miri">MIRI</EntityLink>** (originally SIAI) | 2000 | <EntityLink id="agent-foundations">Agent foundations</EntityLink>, decision theory, formal approaches to AI alignment |
| **<EntityLink id="fhi">Future of Humanity Institute</EntityLink>** | 2005 | Existential risk research including AI safety, hosted at Oxford University |
| **Centre for the Study of Existential Risk** | 2012 | Cambridge University-based research center for existential risks |
| **<EntityLink id="fli">Future of Life Institute</EntityLink>** | 2014 | AI safety research funding, policy advocacy, and coordination |
| **<EntityLink id="openai">OpenAI</EntityLink>** | 2015 | AI research with stated mission to ensure AGI benefits all humanity |

Other relevant organizations founded during this period include:
- <EntityLink id="lesswrong">LessWrong</EntityLink> (2009): Online community for rationality and AI safety discussion
- <EntityLink id="cea">Centre for Effective Altruism</EntityLink> (2012): Organization promoting effective altruism, which prioritized AI safety as a cause area
- <EntityLink id="80000-hours">80,000 Hours</EntityLink> (2011): Career advice organization that promoted AI safety research as a high-impact career path

## Connection to Effective Altruism

The effective altruism movement, which emerged around 2011, adopted AI safety as a top priority based on several considerations:
- High expected value due to potentially large magnitude of impact
- Relative neglect compared to the scale of potential consequences
- Tractability uncertainty, but with possibility of substantial progress
- Alignment with "longtermist" frameworks prioritizing effects on future generations {/* NEEDS CITATION */}

This connection created a pipeline of talent into AI safety research, as effective altruism organizations like <EntityLink id="80000-hours">80,000 Hours</EntityLink> recommended AI safety careers to people seeking to maximize their positive impact. The relationship between effective altruism and AI safety would strengthen further in subsequent years.

## Transition to the Deep Learning Era

### Changes Beginning in 2015

The period around 2015 marked the beginning of a transition in AI capabilities and consequently in AI safety research priorities. Deep learning techniques were demonstrating unprecedented performance on tasks previously considered difficult or impossible for AI systems.

AlphaGo's defeat of world champion Go player Lee Sedol in March 2016 (the Nature paper on AlphaGo defeating Fan Hui was published in January 2016) surprised many observers who had predicted AI would require decades to master the game. {/* NEEDS CITATION */} The GPT series of language models would emerge in subsequent years, demonstrating capabilities in natural language that challenged assumptions about what machine learning systems could achieve.

These developments created pressure for AI safety research to engage more directly with practical machine learning systems rather than focusing exclusively on theoretical scenarios. The question shifted from "how do we build safe AGI someday" toward "how do we make current systems safer while preparing for rapid capability growth."

## Legacy and Historical Significance

### Institutional Establishment

The MIRI era established AI safety as a field with dedicated organizations rather than scattered individual researchers. This institutional foundation enabled sustained research programs, community building, and coordination that would not have been possible through individual efforts alone.

### Intellectual Frameworks

Core concepts developed or popularized during this period continue to influence AI safety discourse:
- The orthogonality thesis (separating intelligence from goals)
- <EntityLink id="instrumental-convergence">Instrumental convergence</EntityLink> (predicting common sub-goals)
- The alignment problem (specifying human values in AI systems)
- <EntityLink id="fast-takeoff">Takeoff scenarios</EntityLink> (paths to advanced AI)
- Existential risk framing (potential for permanent negative outcomes)

These concepts provide vocabulary and analytical frameworks for contemporary discussions of AI risk and safety.

### Community Growth

The field expanded from fewer than ten dedicated researchers to hundreds by 2015. {/* NEEDS CITATION */} This growth created a community capable of sustaining multiple research programs and approaches.

### Funding Base

Annual funding for AI safety research grew from near zero to millions of dollars. {/* NEEDS CITATION */} While still modest compared to AI capabilities research funding, this represented sufficient resources to support multiple research organizations and grant programs.

### Academic Legitimacy

By 2015, AI safety could no longer be easily dismissed as science fiction. Academic institutions hosted safety research groups, conferences included safety-focused tracks, and prominent researchers from mainstream AI engaged with safety questions.

### Public Awareness

Mainstream media coverage and statements from prominent scientists and technology leaders brought AI safety concerns to general audiences. This visibility helped with researcher recruitment and funding, though it also generated controversy and skepticism.

### Open Questions

Despite these achievements, significant challenges remained:
- Limited connection between theoretical safety research and practical machine learning systems
- Continued skepticism from most AI researchers about the urgency of safety work
- Uncertainty about whether sufficient progress could be made before advanced AI systems emerged
- Need to develop governance frameworks in addition to technical solutions
- Questions about how to ensure safety research kept pace with rapidly advancing capabilities

The MIRI era established foundations that would enable the next phase of AI safety research, but also revealed the magnitude of remaining challenges.
