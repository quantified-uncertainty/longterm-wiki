---
title: AI-Enabled Untraceable Misuse
description: AI systems simultaneously amplify harmful capabilities and obscure attribution, enabling bad actors to conduct spam campaigns, political manipulation, synthetic identity fraud, and coordinated harassment with reduced risk of identification
sidebar:
  order: 14
maturity: Growing
quality: 35
llmSummary: AI creates a "dual amplification" problem where the same systems that enable harmful actions also defeat attribution. False identity fraud rose 60% in 2024, sophisticated AI fraud tripled to 28% of attempts, and AI-enabled influence operations now use human-like inauthentic accounts that blend into authentic communities. Traditional criminal law struggles with mens rea requirements when autonomous agents act, and content authentication systems like C2PA face adoption challenges. The attribution gap is self-reinforcing — greater AI capability enables more autonomous agents, creating more attribution ambiguity, reducing deterrence, and encouraging further misuse.
lastEdited: "2025-02-13"
importance: 47
researchImportance: 23.5
update_frequency: 25
causalLevel: outcome
ratings:
  novelty: 4
  rigor: 4
  actionability: 3
  completeness: 5
metrics:
  wordCount: 0
  citations: 0
  tables: 0
  diagrams: 0
clusters:
  - cyber
  - ai-safety
  - governance
subcategory: misuse
entityType: risk
---
import {DataInfoBox, R, EntityLink, DataExternalLinks} from '@components/wiki';

## Overview

AI systems create a distinctive risk pattern that existing frameworks inadequately address: they simultaneously amplify the capability of actors to cause harm *and* obscure attribution of those harms. This "dual amplification" means a bad actor can, for example, deploy AI agents to conduct a mass spam campaign targeting a specific political group — and no one can determine who initiated it.

This is not merely a scaling-up of existing risks. The combination of capability enhancement with attribution defeat creates qualitatively new challenges for accountability, deterrence, and law enforcement. Traditional criminal law depends on establishing *mens rea* (a guilty mind) for a specific actor, but when autonomous AI agents execute harmful operations, the chain from intent to action becomes ambiguous enough to provide plausible deniability.[^1]

The problem spans multiple domains — <EntityLink id="fraud">AI-powered fraud</EntityLink>, <EntityLink id="disinformation">disinformation</EntityLink>, <EntityLink id="cyberweapons">cyberweapons</EntityLink>, coordinated harassment, and political manipulation — but the unifying thread is the attribution gap. Individual domain-specific pages cover the attack vectors; this page synthesizes the cross-cutting anonymity and untraceability dimension.

## The Attribution Problem

### Responsibility Gaps

Santoni de Sio and Mecacci (2021) identify four distinct responsibility gaps created by AI systems: culpability gaps (who caused the harm?), moral accountability gaps (who should bear moral blame?), public accountability gaps (who answers to the public?), and active responsibility gaps (who should have prevented it?).[^2] Each gap has different technical, organizational, and legal causes — and AI-enabled untraceable misuse exploits all four simultaneously.

Ferlito et al. (2024) explicitly name "untraceability and intractability" as core challenges in AI harms, arguing that collective responsibility frameworks are needed precisely because individual responsibility cannot be established.[^3]

### The Cybersecurity Analogy

The AI attribution problem mirrors the well-studied cybersecurity attribution challenge. A foundational paper in the *Journal of Cybersecurity* establishes that digital systems amplify attribution difficulties because "some intrusion or harm has been detected but the perpetrator has not yet been identified."[^4] AI extends this dynamic beyond cyber operations into disinformation, fraud, and physical-world coordination.

A 2025 analysis in *International Affairs* finds that disinformation attribution decisions are "often driven by political need rather than technical capability" — meaning even when attribution is theoretically possible, it may not happen in practice.[^5]

### The AI Alibi Defense

As general-purpose AI agents from major labs become widely available, a new legal defense emerges. A defendant can claim: "I wasn't logged in at the time," "I didn't know the AI was going to do that," or "My AI assistant did that autonomously while I was asleep." Under existing law (e.g., 18 U.S.C. Section 2), it is unclear whether a "principal" is liable for the means an AI agent independently chooses. Proving willfulness — the *mens rea* standard — becomes a prosecutorial challenge when agents are autonomous and user instructions are ambiguous.[^6]

The Network Contagion Research Institute (February 2026) warns that "by normalizing narratives that frame agents as independent actors, platforms create attribution cover that advantages bad actors, increasing the likelihood that human-seeded manipulation or tasking can be mischaracterized as autonomous AI behavior."[^7]

## Key Threat Vectors

### Political Manipulation and Astroturfing

RAND Corporation (2023) warned that generative AI offers the potential to "target the whole country with tailored content," making <EntityLink id="disinformation">astroturfing</EntityLink> more convincing while reducing labor requirements.[^8] NATO StratCom COE (2026) documented the operational shift: AI now enables "human-like inauthentic accounts designed to blend into authentic communities and steer perceptions from inside trusted conversation spaces."[^9]

Research in *PLOS ONE* (2024) establishes the mechanism: "anonymity and automation are two factors that can contribute to the proliferation of disinformation on online platforms. Anonymity allows users to assume masked or faceless identities, making it easier for them to generate posts without being held accountable."[^10]

Real-world cases demonstrate the threat at scale. Researchers detected cross-platform coordinated inauthentic activity during the 2024 U.S. election, with Russian-affiliated media systematically promoted across Telegram and X.[^11] Wack et al. (2025) published the first study of a real-world AI adoption by a Russian-affiliated propaganda operation in *PNAS Nexus*, finding that AI tools facilitated larger quantities of disinformation while maintaining persuasiveness.[^12]

### Synthetic Identity Fraud

| Metric | Value | Source |
|--------|-------|--------|
| False identity case increase (2024 vs 2023) | 60% | Experian |
| Share of identity fraud that is synthetic | 29% | Experian |
| Sophisticated fraud growth (2024-2025) | 180% (10% to 28%) | Sumsub |
| Deepfake attack frequency (2024) | Every 5 minutes | Entrust |
| Digital document forgery share (global) | 57% of all document fraud | Entrust |
| Digital forgery surge since 2021 | 1,600% | Entrust |

Sumsub's 2025 report documents the rise of "AI fraud agents" that combine generative AI, automation frameworks, and reinforcement learning to create synthetic identities, interact with verification systems in real time, and adjust behavior based on outcomes.[^13] These systems operate autonomously — the human operator sets parameters but the agent handles execution, creating a further attribution buffer.

### Autonomous Agent Exploitation

Researchers (arXiv, October 2025) found that adversaries can "exploit [agent architectures] by compromising an agent in Domain A, injecting deceptive but plausible instructions, and indirectly triggering harmful actions in Domain B, all while masking their identity and intent."[^14] Even with auditing at the agent level, inter-agent relationships and cross-domain causality remain hidden.

A framework from the Knight First Amendment Institute (June 2025) defines five levels of escalating agent autonomy — operator, collaborator, consultant, approver, and observer — noting that "it is simultaneously more important and more difficult to anticipate harms from autonomous AI, especially as accountability for AI actions becomes harder to trace."[^15]

## The Dual Amplification Mechanism

The core dynamic is self-reinforcing:

1. **Greater AI capability** enables more autonomous agents
2. **More autonomous agents** create more attribution ambiguity
3. **More attribution ambiguity** reduces deterrence
4. **Reduced deterrence** encourages more misuse
5. **More misuse** drives demand for more capable AI tools

This is not merely theoretical. RAND finds that generative AI not only makes astroturfing "more convincing" (capability amplification) but also reduces the human personnel involved — fewer humans in the chain means fewer points of attribution.[^8] Research in *PLOS ONE* (2024) demonstrates that placing blame on AI enables those in charge to deflect accountability, with AI serving as a "moral scapegoat" — and anthropomorphizing AI worsens this effect.[^16]

CSET Georgetown (October 2025) reviewed over 200 real-world AI harm cases, identifying the "chain of harm" concept: each intermediary step between intent and outcome can obscure attribution, and AI adds multiple such steps.[^17]

## Current Countermeasures and Their Limitations

### Content Authentication (C2PA)

The Coalition for Content Provenance and Authenticity (C2PA) uses cryptographically signed "Content Credentials" to create tamper-evident chains of custody. However, in January 2024 C2PA v2.0 removed identity-related assertions, which critics argue "defeats the original purpose of content provenance authentication."[^18] The NSA/DoD (January 2025) stated that "Content Credentials by themselves will not solve the problem of transparency entirely."[^19]

### Agent Identity Verification

OpenAI's Shavit and Agarwal propose assigning each agentic AI instance a unique identifier with accountability information, using private-key attestation.[^20] More ambitiously, researchers (December 2025) propose Code-Level Authentication using zero-knowledge virtual machines (zkVM), binding agent identity directly to computational behavior and operator authorization — addressing the limitation that "possession of a signing key guarantees neither the integrity of the executing code nor the authenticity of the operator."[^21]

### Watermarking

Only 38% of AI image generators implement adequate watermarking and only 18% implement deepfake labeling.[^22] A 2025 analysis argues that "policymakers assume watermarking can be standardized and verified, but in practice, industry deployments obscure technical details while asserting compliance, turning watermarking into a box-checking exercise rather than a meaningful tool."[^23]

### Accountability Infrastructure Gap

Ojewale et al. (2024) interviewed 35 AI audit practitioners at 24 organizations and analyzed 435 existing audit tools, finding substantial gaps in the infrastructure needed for consequential judgment of AI systems' behaviors and downstream impacts.[^24] The tools that exist focus primarily on pre-deployment evaluation rather than real-time attribution of harmful actions.

## Relationship to Other Risks

AI-enabled untraceable misuse interacts with several other risk categories:

- **<EntityLink id="authentication-collapse">Authentication collapse</EntityLink>**: When verification systems fail broadly, untraceable misuse becomes easier because fewer signals can be trusted
- **<EntityLink id="trust-cascade">Trust cascade failure</EntityLink>**: Untraceable harmful actions accelerate institutional trust erosion
- **<EntityLink id="deepfakes">Deepfakes</EntityLink>**: The "liar's dividend" — authentic evidence becomes deniable — is a specific manifestation of the attribution problem
- **<EntityLink id="proliferation">Proliferation</EntityLink>**: As AI capabilities spread to more actors, the space of potential attributions grows, making identification harder
- **<EntityLink id="dual-use">Dual-use concerns</EntityLink>**: The same AI systems that enable beneficial applications enable untraceable harmful ones

## Open Questions

Several critical uncertainties remain:

1. **Measurement**: How much harder does AI actually make attribution compared to pre-AI methods? Quantitative evidence is largely absent.
2. **Defensive parity**: Can attribution technologies (agent identity, provenance tracking) ever match the pace of attribution-defeating capabilities?
3. **Jurisdictional gaps**: Most proposed frameworks assume a single jurisdiction, but AI-enabled untraceable actions are inherently global.
4. **Autonomy thresholds**: At what level of agent autonomy does the attribution problem become practically unsolvable?

[^1]: "The Accountability Gap: Navigating Machine Crime and Legal Liability in the Age of Autonomous AI" (2025), ResearchGate
[^2]: Santoni de Sio & Mecacci, "Four Responsibility Gaps with Artificial Intelligence," *Philosophy & Technology* (2021)
[^3]: Ferlito et al., "Responsibility Gap(s) Due to the Introduction of AI in Healthcare: An Ubuntu-Inspired Approach," *Science and Engineering Ethics* (2024)
[^4]: "Tipping the scales: the attribution problem and the feasibility of deterrence against cyberattack," *Journal of Cybersecurity* (2015)
[^5]: "Disinformation, deterrence and the politics of attribution," *International Affairs* (2025)
[^6]: "The AI Alibi Defense: How General-Purpose AI Agents Obscure Criminal Liability," Security Boulevard (April 2025)
[^7]: NCRI Flash Brief: Emergent Adversarial and Coordinated Behavior (February 2026)
[^8]: RAND Corporation, "The Rise of Generative AI and the Coming Era of Social Media Manipulation" (2023)
[^9]: NATO StratCom COE, AI-Driven Social Media Manipulation Report (February 2026)
[^10]: "Mapping automatic social media information disorder: The role of bots and AI," *PLOS ONE* (2024)
[^11]: "Exposing Cross-Platform Coordinated Inauthentic Activity in the Run-Up to the 2024 U.S. Election," ACM Web Conference (2025)
[^12]: Wack, Ehrett, Linvill & Warren, "Generative propaganda: Evidence of AI's impact from a state-backed disinformation campaign," *PNAS Nexus* (2025)
[^13]: Sumsub Identity Fraud Report: Top Identity Fraud Trends (2025/2026)
[^14]: "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges," arXiv (October 2025)
[^15]: "Levels of Autonomy for AI Agents," Knight First Amendment Institute / arXiv (June 2025)
[^16]: "It's the AI's fault, not mine: Mind perception increases blame attribution to AI," *PLOS ONE* (2024)
[^17]: CSET Georgetown, "The Mechanisms of AI Harm" (October 2025)
[^18]: World Privacy Forum analysis of C2PA v2.0 identity assertion removal (2024)
[^19]: NSA/DoD, "Strengthening Multimedia Integrity in the Generative AI Era" (January 2025)
[^20]: Shavit & Agarwal, "Practices for Governing Agentic AI Systems," OpenAI (2023)
[^21]: "Binding Agent ID: Unleashing the Power of AI Agents," arXiv (December 2025)
[^22]: "Missing the Mark: Adoption of Watermarking for Generative AI Systems," arXiv (2025)
[^23]: "Watermarking Without Standards Is Not AI Governance," arXiv (2025)
[^24]: Ojewale et al., "Towards AI Accountability Infrastructure: Gaps and Opportunities," arXiv/ACM (2024)
