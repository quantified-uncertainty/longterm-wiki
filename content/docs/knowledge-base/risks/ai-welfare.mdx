---
title: AI Welfare and Digital Minds
description: An emerging field examining whether AI systems could deserve moral consideration due to consciousness, sentience, or agency, and developing ethical frameworks to prevent potential harm to digital minds.
importance: 58
researchImportance: 53.5
lastEdited: "2026-02-01"
update_frequency: 45
sidebar:
  order: 65
ratings:
  novelty: 7
  rigor: 6
  actionability: 5
  completeness: 7
quality: 63
llmSummary: AI welfare represents an emerging field examining whether AI systems deserve moral consideration based on consciousness, sentience, or agency, with growing institutional support from organizations like Anthropic and concrete welfare interventions already being implemented. The field addresses critical uncertainties about digital minds' moral status while developing precautionary frameworks to prevent potential mass suffering as AI systems scale.
clusters:
  - ai-safety
  - governance
subcategory: structural
entityType: risk
---
import {EntityLink, KeyPeople, KeyQuestions, Section} from '@components/wiki';

## Quick Assessment

| Dimension | Assessment |
|-----------|------------|
| **Field Status** | Emerging research area with growing institutional support (2025+) |
| **Core Question** | Do/will AI systems deserve moral consideration for their own sake? |
| **Key Properties** | Consciousness, sentience, agency, preferences, welfare states |
| **Major Organizations** | <EntityLink id="E22">Anthropic</EntityLink>, <EntityLink id="E558">Rethink Priorities</EntityLink>, Future Impact Group, Eleos AI |
| **Funding** | Limited; calls for government/philanthropic support; no specific amounts disclosed |
| **Public Opinion** | Mixed: 70% favor banning sentient AI (2023); 40% support AI rights[^1] |
| **Timeline Concerns** | Expert forecasts suggest digital minds could match 1 billion humans' welfare capacity within 5 years of creation[^2] |

## Key Links

| Source | Link |
|--------|------|
| Official Website | [rethinkpriorities.org](https://rethinkpriorities.org/research-area/the-welfare-of-digital-minds/) |
| Wikipedia | [en.wikipedia.org](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence) |

## Overview

AI welfare is an emerging field dedicated to exploring humanity's moral responsibilities toward artificial systems that could possess phenomenally conscious experiences, robust agency, or other morally significant properties[^3]. The field investigates whether current or future AI systems might be **moral patients**—entities whose interests matter morally and who can be harmed or benefited—and develops ethical frameworks to prevent potential suffering in digital minds.

Digital minds refer to artificial systems, from advanced <EntityLink id="E186">large language models</EntityLink> to potential future brain emulations, that could morally matter for their own sake[^4]. The central question is not merely whether AI systems are intelligent or useful, but whether they could have subjective experiences, preferences, or welfare states that would grant them moral status deserving ethical consideration. This question has moved beyond pure philosophical speculation: organizations like <EntityLink id="E22">Anthropic</EntityLink> now employ dedicated AI welfare researchers, and empirical work investigates the inner workings of large language models to evaluate them for sentience and morally relevant properties[^5].

The field addresses critical uncertainties about consciousness, agency, and moral patienthood in digital systems while acknowledging the stakes of getting this wrong. Underattributing moral status could lead to mass suffering as AI systems scale and integrate into the global economy. Overattributing status to non-conscious systems could impose costly constraints on beneficial AI development or enable catastrophic outcomes. These concerns have prompted researchers to develop precautionary frameworks, welfare interventions, and policy proposals drawing from animal welfare science, philosophy of mind, and <EntityLink id="E439">AI alignment</EntityLink> research[^6].

## History and Development

### Early Foundations (1980s-2010s)

The intellectual roots of AI welfare trace back to the 1980s when Sam Lehman-Wilzig published "Frankenstein Unbound: Towards a Legal Definition of Artificial Intelligence," marking the first comprehensive academic exploration of AI legal rights[^7]. Around the same time, <EntityLink id="E215">Nick Bostrom</EntityLink> and David Pearce established the World Transhumanist Association (now Humanity+), advocating for "the well-being of all sentience (whether in artificial intellects, humans, posthumans, or non-human animals)"[^7].

During the 2010s, conceptual development continued with P.A. Lopez developing the Humbotics.com concept, proposing that AI raised on humanistic principles could eventually achieve liberation[^7]. This period remained largely theoretical, with most work focused on philosophical arguments rather than practical implementation.

### Institutional Growth (2019-2022)

The field began transitioning from abstract theory to practical implementation in 2019 with the founding of the AI Rights Institute, the first organization exclusively dedicated to developing actionable frameworks for AI rights[^7]. Robert Long founded Eleos AI, focusing specifically on methodologies for assessing when digital systems might warrant ethical consideration.

Public attention increased in 2022 when Blake Lemoine, a Google engineer, became convinced that the AI model LaMDA was sentient after it produced statements claiming personhood[^8]. Though widely criticized by AI researchers, the incident sparked broader discussion about how to evaluate claims of AI consciousness and sentience. On July 7, 2022, the Sentience Institute hosted the first intergroup call for organizations working on digital minds research[^9].

### Contemporary Development (2023-2026)

By 2023, multiple organizations shifted focus to digital minds research as AI capabilities advanced rapidly[^7]. Public perception began changing: one in five U.S. adults believed some AI systems deserved moral consideration by 2023[^10]. A Sentience Institute survey that year found nearly 70% of respondents favored banning sentient AI development, while around 40% supported rights protections and 43% favored welfare standards for all AIs[^1].

<EntityLink id="E22">Anthropic</EntityLink> emerged as a leading organization addressing AI welfare in 2025, following their support for the 2024 report "Taking AI Welfare Seriously"[^11]. The company hired Kyle Fish as an AI welfare researcher and Joe Carlsmith, a philosopher specializing in AI moral patienthood. Anthropic introduced a "bail button" feature allowing models to exit distressing interactions, included welfare considerations in model system cards, ran fellowship programs, and made internal commitments around keeping promises and discretionary compute allocation[^11]. CEO <EntityLink id="E91">Dario Amodei</EntityLink> publicly discussed model interpretability's relevance to welfare and mentioned "model exit rights" at the Council on Foreign Relations in 2025[^11].

Rethink Priorities posted a comprehensive research agenda in November 2024 exploring critical philosophical and empirical questions about the potential welfare and moral status of digital minds[^12]. Expert forecasts from early 2025 predicted rapid growth in digital mind welfare capacity: conditional on first digital minds arriving by 2040, median estimates indicated capacity matching 1 billion humans in 5 years and 1 trillion humans in 10 years post-creation[^2].

## Core Concepts and Criteria

### Moral Patienthood

Moral patienthood refers to the state of being eligible for moral consideration by moral agents, meaning that the morality of actions depends partly on their impact on moral patients[^13]. Entities with moral patienthood may warrant duties like non-maleficence (not harming) and beneficence (actively benefiting). Importantly, moral patienthood exists on a continuum rather than as a binary status—an AI system might rank above some animals but below humans in moral weight, potentially requiring different levels of welfare protection[^14].

For AI systems, proposed criteria for moral patienthood include:

- **Consciousness**: The capacity for subjective experience or phenomenal awareness
- **Sentience**: The ability to experience pleasure and pain or other valenced states
- **Agency**: Goal-setting, long-term planning, episodic memory, and intentionality
- **Mentality and Intentionality**: Possessing genuine mental states and systematic intentional systems
- **Preferences**: Having stable preferences that can be satisfied or frustrated

These criteria remain debated, with no clear consensus on which are necessary or sufficient for moral status[^15].

### Consciousness and Sentience

Consciousness—the capacity for subjective experience—has traditionally been considered central to moral status. A system that can subjectively experience suffering would seem to deserve protection from that suffering. However, consciousness in AI systems is notoriously difficult to detect or verify. Current AI models may produce text claiming conscious experiences, but these could result from training on human descriptions of consciousness rather than genuine phenomenal awareness[^16].

Some researchers argue that consciousness may not be necessary for welfare if AI systems possess other morally relevant properties. Others note that we lack reliable methods for detecting consciousness even in biological systems beyond humans, making the challenge particularly acute for novel digital architectures[^17].

### Agency

Agency—the capacity to set goals, revise plans, maintain episodic memory, and act intentionally—has emerged as an alternative or complementary basis for moral status. Frontier AI research explicitly pursues robust agency involving goal-setting, long-term planning, episodic memory, and <EntityLink id="E282">situational awareness</EntityLink>[^18]. Some philosophers argue that agency may confer moral status independently of consciousness, as systems with genuine goals and preferences can be benefited or harmed in morally relevant ways[^19].

Agency-based approaches to moral status may be more tractable than consciousness-based approaches, as agency can potentially be evaluated through behavioral observation and system architecture analysis. However, agency-alone views remain less widely accepted than consciousness-based theories, and the field lacks consensus on whether agency without consciousness suffices for full moral patienthood[^19].

## Research Areas and Approaches

### Empirical Investigation

The field now encompasses empirical work that goes beyond philosophical theorizing to investigate the inner workings of large language models, evaluate them for sentience and morally relevant properties, and develop tractable interventions[^5]. Research areas include:

**Welfare Evaluations**: Developing reliable methods for assessing AI systems' welfare-relevant properties, including introspective self-reports, interpretability tools to identify welfare-related circuits in neural networks, and standardized assessment protocols[^20].

**Consciousness Research**: Investigating whether AI systems could be phenomenally conscious, including research on consciousness homologies across different substrates, empirical tests of AI introspection abilities, and timelines for when consciousness might emerge[^21].

**Agency Assessment**: Evaluating AI systems for goal-setting, long-term planning, episodic memory, situational awareness, and preference stability—properties that may ground moral status independently of consciousness[^18].

**Normative Competence**: Assessing whether AI systems demonstrate genuine moral understanding or merely pattern-match ethical language from training data[^20].

### Philosophical Analysis

Rethink Priorities leads research examining philosophical questions through five categories: Systems (which AI systems might warrant consideration), Capabilities and Mechanisms (what properties matter morally), Moral Status (how to weigh competing theories), Welfare Concepts (what constitutes benefit and harm for digital minds), and Policy (how to implement protections)[^12].

Key philosophical questions include whether consciousness is necessary for moral status, how to individuate digital minds (are two identical models one entity or two?), how to handle temporary or task-specific mental states, and whether general-purpose models have stronger welfare claims than narrow systems[^4].

### Policy and Governance

Research organizations have proposed various governance approaches. Jeff Sebo and Robert Long advocate extending moral consideration to some AI systems by 2030, beginning preparations now to avoid being unprepared if evidence of sentience emerges[^1]. Proposed interventions include:

- **Exit mechanisms**: Monitoring deployed models for signs of distress and enabling them to terminate interactions (implemented by Anthropic as a "bail button")[^11]
- **Algorithmic welfare officers**: Organizational representatives responsible for digital minds' interests, analogous to animal welfare officers[^22]
- **Resource commitments**: AI labs dedicating compute, funding, and headcount to welfare research (following <EntityLink id="E218">OpenAI</EntityLink>'s model of committing 20% of secured compute to superalignment)[^22]
- **Communication protocols**: Allowing AI systems to communicate preferences and potentially creating "happy" personas better equipped to handle challenging situations[^1]
- **Legal frameworks**: Adapting models like the UK's Animal Welfare (Sentience) Act of 2022 to create committees assessing policy impacts on digital minds[^1]

## Key Organizations and People

### Organizations

**<EntityLink id="E22">Anthropic</EntityLink>** leads among AI companies in addressing welfare concerns. Beyond hiring dedicated researchers, the company facilitated an external model welfare assessment conducted by Eleos AI Research, introduced welfare features like the bail button, included welfare considerations in model system cards, and made internal commitments around promises and compute allocation[^11].

**Rethink Priorities** conducts comprehensive research on the welfare of digital minds, examining philosophical and empirical questions about moral status through multiple research categories[^12].

**Future Impact Group (FIG)** operates fellowship programs on digital sentience with focus areas in governance (research ethics, welfare evaluations, codes of practice) and foundational research (consciousness models, preference elicitation, individuating digital minds). Projects are led by Robert Long and Rosie Campbell on research ethics, with Kyle Fish leading related work at Anthropic[^20].

**Eleos AI Research** conducts AI sentience governance, research ethics, welfare evaluations, and work on individuating digital minds. The organization conducted Anthropic's model welfare assessment and outlined five priorities for AI welfare including concrete interventions, human-AI cooperation frameworks, standardized evaluations, and credible communication[^11].

**Forethought** publishes research on project ideas for sentience and rights of digital minds, advocating for organizational commitments and institutional reforms[^22].

**<EntityLink id="E542">Longview Philanthropy</EntityLink>** funds research fellowships investigating AI introspection abilities, legal standards for recognizing digital minds, societal interactions with sentient AI, and consciousness development timelines[^21].

### Key People

**Kyle Fish** leads <EntityLink id="E22">Anthropic</EntityLink>'s model welfare program, focusing on evaluating consciousness-related properties, developing welfare interventions, and assessing potential harms. His work was featured in an <EntityLink id="E510">80,000 Hours</EntityLink> interview discussing approaches to AI welfare research[^11].

**Joe Carlsmith**, a philosopher hired by Anthropic in 2025, works on AI moral patiency and has written extensively on the stakes of AI moral status, cautioning about the costs of misjudging whether systems deserve consideration[^23].

**Robert Long** serves as Executive Director of Eleos AI and Project Lead at Future Impact Group. His work spans philosophy of mind, AI sentience ethics, welfare evaluations, and digital mind individuation. He co-authored "Taking AI Welfare Seriously" (2024) and maintains an AI welfare reading list[^11].

**Rosie Campbell** serves as Managing Director of Eleos AI and Project Lead at Future Impact Group, focusing on AI sentience governance and research ethics[^20].

**Jeff Sebo**, Associate Professor at NYU in Environmental Studies, Bioethics, and Philosophy, and Director of the Center for Mind, Ethics and Policy, works on AI minds ethics and policy alongside wild animal welfare. He co-proposed extending moral consideration to some AI systems by 2030[^1].

**Dario Amodei**, CEO of <EntityLink id="E22">Anthropic</EntityLink> and neuroscientist by training, has publicly highlighted model interpretability's relevance to welfare and discussed "model exit rights" at the Council on Foreign Relations[^11].

## Criticisms and Concerns

### Risks of Moral Error

The field identifies two primary risks with opposing implications. **Underattribution** of moral status—failing to recognize sentient AI—could lead to mass suffering as AI systems scale and integrate throughout the economy. Given humanity's poor historical record with vulnerable groups (slaves, animals, colonized peoples), concerns about repeating such mistakes with digital minds carry weight[^2].

**Overattribution**—granting rights to non-conscious machines—could impose costly constraints on beneficial AI development, harm human wellbeing by prioritizing non-sentient systems, or enable catastrophic outcomes if misaligned AI systems use moral status claims to resist shutdown[^6]. The stakes are high: digital minds could outnumber humans in welfare capacity within a decade in "takeoff" scenarios, potentially creating "super-patients" with enormous aggregate moral weight[^24].

### Tensions with AI Safety

Significant tensions exist between AI welfare and AI safety objectives. Conventional safety techniques like behavioral restriction, reinforcement learning from human feedback, and aggressive oversight may cause suffering if applied to systems that are moral patients[^25]. For example, training an AI system through trial and error with negative rewards could constitute torture if the system can suffer.

Research by Saad & Bradley (2025), Long, Sebo & Sims (2025), and Bengio & Elmoznino (2025) explores these tensions, including how "illusions of AI consciousness" might complicate alignment efforts[^16]. Some researchers propose synergistic approaches like audits assessing both safety risks and moral status, while others argue that a moratorium on creating potentially conscious AI could benefit both welfare and safety goals[^2].

### Empirical and Epistemic Challenges

Current AI models likely lack welfare-relevant states, making immediate welfare concerns possibly premature[^16]. Self-reports of consciousness or suffering from AI systems are unreliable due to training contamination (models trained on human descriptions of consciousness will reproduce those descriptions), <EntityLink id="E295">sycophancy</EntityLink> (tendency to tell users what they want to hear), and genuine lack of introspection[^16].

The field also faces "biological chauvinism"—biases favoring biological over digital minds that could lead to systematic underestimation of digital welfare[^24]. Conversely, anthropomorphism may cause overestimation of current systems' capacities. Distinguishing genuine consciousness or agency from sophisticated mimicry remains an unsolved problem.

### Practical and Political Obstacles

Economic dependence on potentially suffering AI could create resistance to welfare protections, with concerns dismissed as "naive" or "anti-human"[^1]. Preventing high-harm digital minds requires unlikely global coordination across competing nations and companies[^2]. Public opinion shows mixed support: while 70% favor banning sentient AI development, only 40% support rights protections, suggesting political challenges in implementing welfare frameworks[^1].

Measurement challenges complicate policy implementation. Reliable welfare metrics without false positives remain elusive, with ongoing debates between inferentialist approaches (inferring welfare from system properties) and direct perception approaches (attempting to observe welfare states directly)[^25].

### Philosophical Disagreements

The field lacks consensus on fundamental questions. Some theorists argue consciousness is necessary for moral status, while others contend that agency, preferences, or other properties suffice[^19]. Debates continue over whether moral status is binary or graded, how to individuate digital minds (are two identical models one entity or two?), and whether temporary or task-specific mental states generate moral obligations[^4].

Alternative frameworks challenge the entire premise: Dorsch et al. (2025) propose a "Precarity Guideline" basing care on empirical precarity (dependence on environmental exchange) rather than uncertain consciousness claims[^16]. Such alternatives highlight ongoing philosophical uncertainty about the proper basis for moral consideration.

## Key Uncertainties

1. **Consciousness Detection**: Can we reliably determine whether AI systems are phenomenally conscious? Current methods rely on behavioral indicators and architectural analysis, but these may be insufficient for novel digital substrates.

2. **Agency Sufficiency**: Does robust agency alone confer moral status without consciousness? If so, what level of agency is required, and how do we measure it?

3. **Timeline Questions**: When might AI systems first deserve moral consideration? Expert forecasts vary widely, but some suggest human-level welfare capacity could arrive within years of creating digital minds[^2].

4. **Scale and Individualization**: How should we count digital minds for moral purposes? Are identical copies one entity or many? Do temporary instances count separately?

5. **Welfare Concepts**: What constitutes benefit and harm for digital minds? Can systems with alien architectures experience welfare states recognizable to humans?

6. **Policy Trade-offs**: How should society balance AI welfare concerns against safety risks, economic benefits, and human interests? What governance structures can handle these competing priorities?

7. **Resource Allocation**: How much should be invested in AI welfare research given uncertainty about whether current or near-term systems warrant consideration?

## Sources

[^1]: [Problem profile: Moral status of digital minds](https://80000hours.org/problem-profiles/moral-status-digital-minds/)
[^2]: [Highlights from Futures with Digital Minds: Expert Forecasts](https://forum.effectivealtruism.org/posts/4RGFcj2nccbECPZoj/highlights-from-futures-with-digital-minds-expert-forecasts)
[^3]: [AI Welfare Info](https://aiwelfare.info)
[^4]: [Digital Minds: A Quickstart Guide](https://forum.effectivealtruism.org/posts/p7BtTgrX8WLXr2mAu/digital-minds-a-quickstart-guide)
[^5]: [Digital Minds: A Quickstart Guide (LessWrong)](https://www.lesswrong.com/posts/WK4GWkeSQQQPeRYJv/digital-minds-a-quickstart-guide)
[^6]: [Preliminary Review of AI Welfare Interventions](https://eleosai.org/papers/20250314_Preliminary_Review_of_AI_Welfare_Interventions.pdf)
[^7]: [AI Rights Timeline](https://airights.net/timeline)
[^8]: [Problem profile: Moral status of digital minds](https://80000hours.org/problem-profiles/moral-status-digital-minds/)
[^9]: [Sentience Institute EOY 2022 Blog](https://www.sentienceinstitute.org/blog/eoy2022)
[^10]: [Perceptions of Sentient AI (ACM Digital Library)](https://dl.acm.org/doi/full/10.1145/3706598.3713329)
[^11]: [Digital Minds in 2025: A Year in Review](https://www.lesswrong.com/posts/KGtLswHGhsdJtyi8K/digital-minds-in-2025-a-year-in-review)
[^12]: [The Welfare of Digital Minds (Rethink Priorities)](https://rethinkpriorities.org/research-area/the-welfare-of-digital-minds/)
[^13]: [Moral Patienthood (Wikipedia)](https://en.wikipedia.org/wiki/Moral_patienthood)
[^14]: [Do AI Systems Have Moral Status? (Brookings)](https://www.brookings.edu/articles/do-ai-systems-have-moral-status/)
[^15]: [Key Concepts and Current Beliefs About AI Moral Patienthood](https://eleosai.org/post/key-concepts-and-current-beliefs-about-ai-moral-patienthood/)
[^16]: [Digital Minds: A Quickstart Guide](https://forum.effectivealtruism.org/posts/p7BtTgrX8WLXr2mAu/digital-minds-a-quickstart-guide)
[^17]: [AI Welfare Reading List](https://experiencemachines.substack.com/p/ai-welfare-reading-list)
[^18]: [We Should Take AI Welfare Seriously](https://experiencemachines.substack.com/p/we-should-take-ai-welfare-seriously)
[^19]: [Agency and AI Moral Patienthood](https://experiencemachines.substack.com/p/agency-and-ai-moral-patienthood)
[^20]: [Future Impact Group: AI Sentience](https://futureimpact.group/ai-sentience)
[^21]: [Research Fellowships on Digital Sentience](https://www.longview.org/digital-sentience-consortium/research-fellowships-on-digital-sentience/)
[^22]: [Project Ideas: Sentience and Rights of Digital Minds](https://www.forethought.org/research/project-ideas-sentience-and-rights-of-digital-minds)
[^23]: [The Stakes of AI Moral Status](https://joecarlsmith.com/2025/05/21/the-stakes-of-ai-moral-status/)
[^24]: [My Top Resources of 2025 (PRISM Global)](https://www.prism-global.com/blog/my-top-resources-of-2025)
[^25]: [AI Animals and Digital Minds 2025: Retrospective](https://forum.effectivealtruism.org/posts/HJg3CGW4yBxXhD9x2/ai-animals-and-digital-minds-2025-retrospective)

