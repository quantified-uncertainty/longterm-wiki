name: Database Backup

# Exports session logs, edit logs, and job data from the wiki-server API.
# Stored as GitHub Actions artifacts (90-day retention).
#
# This provides a safety net for the PostgreSQL data that was previously
# committed as YAML files in git. If the database is lost, these JSON
# snapshots can be used to restore session history and edit logs.
#
# Note: Each endpoint is fetched once with its MAX_PAGE_SIZE limit.
# If data grows beyond a single page, this workflow will need pagination.

on:
  schedule:
    - cron: "0 4 * * *" # daily at 04:00 UTC
  workflow_dispatch:

concurrency:
  group: db-backup
  cancel-in-progress: false

jobs:
  backup:
    name: Export wiki-server data
    runs-on: ubuntu-latest
    steps:
      - name: Export sessions
        env:
          LONGTERMWIKI_SERVER_URL: ${{ secrets.LONGTERMWIKI_SERVER_URL }}
          LONGTERMWIKI_SERVER_API_KEY: ${{ secrets.LONGTERMWIKI_SERVER_API_KEY }}
        run: |
          mkdir -p /tmp/backup

          if [ -z "$LONGTERMWIKI_SERVER_URL" ]; then
            echo "::warning::LONGTERMWIKI_SERVER_URL not set, skipping backup"
            exit 0
          fi

          AUTH_HEADER=""
          if [ -n "$LONGTERMWIKI_SERVER_API_KEY" ]; then
            AUTH_HEADER="Authorization: Bearer $LONGTERMWIKI_SERVER_API_KEY"
          fi

          DATE=$(date -u '+%Y-%m-%d')
          echo "Backing up wiki-server data for $DATE"
          FAILURES=0

          fetch_endpoint() {
            local label="$1" url="$2" output="$3"
            echo "Exporting ${label}..."
            if curl -sf --max-time 60 \
              ${AUTH_HEADER:+-H "$AUTH_HEADER"} \
              "${url}" \
              -o "${output}"; then
              echo "  âœ“ ${label} OK"
            else
              echo "::error::Failed to export ${label} from ${url}"
              FAILURES=$((FAILURES + 1))
            fi
          }

          # Sessions: server MAX_PAGE_SIZE is 500
          fetch_endpoint "sessions" \
            "${LONGTERMWIKI_SERVER_URL}/api/sessions?limit=500&offset=0" \
            /tmp/backup/sessions.json

          # Edit logs: server MAX_PAGE_SIZE is 1000
          fetch_endpoint "edit logs" \
            "${LONGTERMWIKI_SERVER_URL}/api/edit-logs/all?limit=1000&offset=0" \
            /tmp/backup/edit-logs.json

          fetch_endpoint "edit log stats" \
            "${LONGTERMWIKI_SERVER_URL}/api/edit-logs/stats" \
            /tmp/backup/edit-log-stats.json

          fetch_endpoint "job stats" \
            "${LONGTERMWIKI_SERVER_URL}/api/jobs/stats" \
            /tmp/backup/job-stats.json

          fetch_endpoint "latest dates" \
            "${LONGTERMWIKI_SERVER_URL}/api/edit-logs/latest-dates" \
            /tmp/backup/latest-dates.json

          # Summary
          echo ""
          echo "=== Backup Summary ==="
          for f in /tmp/backup/*.json; do
            SIZE=$(wc -c < "$f" | tr -d ' ')
            echo "  $(basename "$f"): ${SIZE} bytes"
          done

          if [ "$FAILURES" -gt 0 ]; then
            echo "::error::${FAILURES} endpoint(s) failed to export"
            exit 1
          fi

      - name: Upload backup artifact
        uses: actions/upload-artifact@v4
        with:
          name: db-backup-${{ github.run_id }}
          path: /tmp/backup/
          retention-days: 90
