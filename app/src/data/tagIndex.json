{
  "academic-ai-safety": [
    {
      "id": "chai",
      "type": "lab-academic",
      "title": "CHAI"
    },
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    }
  ],
  "academic-fraud": [
    {
      "id": "scientific-corruption",
      "type": "risk",
      "title": "Scientific Knowledge Corruption"
    }
  ],
  "accident-risk": [
    {
      "id": "technical-pathways",
      "type": "analysis",
      "title": "Technical Pathway Decomposition"
    }
  ],
  "accountability": [
    {
      "id": "institutional-quality",
      "type": "ai-transition-model-parameter",
      "title": "Institutional Quality"
    },
    {
      "id": "openai-foundation",
      "type": "organization",
      "title": "OpenAI Foundation"
    }
  ],
  "activation-engineering": [
    {
      "id": "representation-engineering",
      "type": "approach",
      "title": "Representation Engineering"
    }
  ],
  "activation-patching": [
    {
      "id": "interpretability",
      "type": "safety-agenda",
      "title": "Interpretability"
    }
  ],
  "actor-analysis": [
    {
      "id": "multi-actor-landscape",
      "type": "analysis",
      "title": "Multi-Actor Strategic Landscape"
    }
  ],
  "adaptation": [
    {
      "id": "institutional-adaptation-speed",
      "type": "model",
      "title": "Institutional Adaptation Speed Model"
    }
  ],
  "adversarial": [
    {
      "id": "disinformation-detection-race",
      "type": "model",
      "title": "Disinformation Detection Arms Race Model"
    }
  ],
  "adversarial-methods": [
    {
      "id": "debate",
      "type": "approach",
      "title": "AI Safety via Debate"
    }
  ],
  "adversarial-robustness": [
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    },
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    },
    {
      "id": "circuit-breakers",
      "type": "approach",
      "title": "Circuit Breakers / Inference Interventions"
    }
  ],
  "adversarial-testing": [
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    },
    {
      "id": "red-teaming",
      "type": "approach",
      "title": "Red Teaming"
    }
  ],
  "adversarial-training": [
    {
      "id": "sleeper-agents",
      "type": "risk",
      "title": "Sleeper Agents: Training Deceptive LLMs"
    }
  ],
  "agent-foundations": [
    {
      "id": "research-agendas",
      "type": "crux",
      "title": "Research Agendas"
    },
    {
      "id": "technical-research",
      "type": "crux",
      "title": "Technical AI Safety Research"
    },
    {
      "id": "instrumental-convergence-framework",
      "type": "model",
      "title": "Instrumental Convergence Framework"
    },
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    },
    {
      "id": "eliezer-yudkowsky",
      "type": "researcher",
      "title": "Eliezer Yudkowsky"
    }
  ],
  "agent-safety": [
    {
      "id": "sandboxing",
      "type": "approach",
      "title": "Sandboxing / Containment"
    },
    {
      "id": "tool-restrictions",
      "type": "approach",
      "title": "Tool-Use Restrictions"
    },
    {
      "id": "multi-agent",
      "type": "approach",
      "title": "Multi-Agent Safety"
    }
  ],
  "agentic": [
    {
      "id": "agentic-ai",
      "type": "capability",
      "title": "Agentic AI"
    },
    {
      "id": "long-horizon",
      "type": "capability",
      "title": "Long-Horizon Autonomous Tasks"
    }
  ],
  "agentic-ai": [
    {
      "id": "rogue-ai-scenarios",
      "type": "risk",
      "title": "Rogue AI Scenarios"
    }
  ],
  "agentic-systems": [
    {
      "id": "heavy-scaffolding",
      "type": "concept",
      "title": "Heavy Scaffolding / Agentic Systems"
    }
  ],
  "agi": [
    {
      "id": "superintelligence",
      "type": "concept",
      "title": "Superintelligence"
    },
    {
      "id": "agi-timeline-debate",
      "type": "crux",
      "title": "When Will AGI Arrive?"
    },
    {
      "id": "agi-timeline",
      "type": "concept",
      "title": "AGI Timeline"
    },
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    },
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    }
  ],
  "agi-development": [
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    }
  ],
  "agi-timelines": [
    {
      "id": "connor-leahy",
      "type": "researcher",
      "title": "Connor Leahy"
    },
    {
      "id": "eli-lifland",
      "type": "researcher",
      "title": "Eli Lifland"
    }
  ],
  "ai-advocacy": [
    {
      "id": "controlai",
      "type": "organization",
      "title": "ControlAI"
    }
  ],
  "ai-assistants": [
    {
      "id": "sycophancy",
      "type": "risk",
      "title": "Sycophancy"
    }
  ],
  "ai-assisted-development": [
    {
      "id": "coding",
      "type": "capability",
      "title": "Autonomous Coding"
    }
  ],
  "ai-assisted-research": [
    {
      "id": "ai-assisted",
      "type": "approach",
      "title": "AI-Assisted Alignment"
    }
  ],
  "ai-bias": [
    {
      "id": "institutional-capture",
      "type": "risk",
      "title": "Institutional Decision Capture"
    }
  ],
  "ai-bio-convergence": [
    {
      "id": "johns-hopkins-center-for-health-security",
      "type": "organization",
      "title": "Johns Hopkins Center for Health Security"
    }
  ],
  "ai-capabilities": [
    {
      "id": "tmc-compute",
      "type": "ai-transition-model-subitem",
      "title": "Compute"
    },
    {
      "id": "tmc-algorithms",
      "type": "ai-transition-model-subitem",
      "title": "Algorithms"
    },
    {
      "id": "ai-forecasting",
      "type": "approach",
      "title": "AI-Augmented Forecasting"
    }
  ],
  "ai-control": [
    {
      "id": "agentic-ai",
      "type": "capability",
      "title": "Agentic AI"
    },
    {
      "id": "long-horizon",
      "type": "capability",
      "title": "Long-Horizon Autonomous Tasks"
    },
    {
      "id": "research-agendas",
      "type": "crux",
      "title": "Research Agendas"
    },
    {
      "id": "technical-research",
      "type": "crux",
      "title": "Technical AI Safety Research"
    },
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    },
    {
      "id": "corrigibility",
      "type": "safety-agenda",
      "title": "Corrigibility"
    },
    {
      "id": "hybrid-systems",
      "type": "approach",
      "title": "AI-Human Hybrid Systems"
    },
    {
      "id": "sleeper-agent-detection",
      "type": "approach",
      "title": "Sleeper Agent Detection"
    },
    {
      "id": "corrigibility-failure",
      "type": "risk",
      "title": "Corrigibility Failure"
    }
  ],
  "ai-controllability": [
    {
      "id": "palisade-research",
      "type": "lab-research",
      "title": "Palisade Research"
    }
  ],
  "ai-dependency": [
    {
      "id": "expertise-atrophy",
      "type": "risk",
      "title": "Expertise Atrophy"
    }
  ],
  "ai-deregulation": [
    {
      "id": "marc-andreessen",
      "type": "researcher",
      "title": "Marc Andreessen"
    }
  ],
  "ai-detection": [
    {
      "id": "scientific-corruption",
      "type": "risk",
      "title": "Scientific Knowledge Corruption"
    }
  ],
  "ai-development": [
    {
      "id": "ai-timelines",
      "type": "concept",
      "title": "AI Timelines"
    }
  ],
  "ai-ethics": [
    {
      "id": "ai-welfare",
      "type": "concept",
      "title": "AI Welfare and Digital Minds"
    },
    {
      "id": "cyber-psychosis",
      "type": "risk",
      "title": "Cyber Psychosis"
    },
    {
      "id": "preference-manipulation",
      "type": "risk",
      "title": "Preference Manipulation"
    }
  ],
  "ai-evals": [
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    }
  ],
  "ai-evaluation": [
    {
      "id": "nist-ai",
      "type": "organization",
      "title": "NIST and AI Safety"
    },
    {
      "id": "scalable-oversight",
      "type": "safety-agenda",
      "title": "Scalable Oversight"
    }
  ],
  "ai-for-good": [
    {
      "id": "epistemic-infrastructure",
      "type": "approach",
      "title": "Epistemic Infrastructure"
    }
  ],
  "ai-forecasting": [
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    }
  ],
  "ai-governance": [
    {
      "id": "eli-lifland",
      "type": "researcher",
      "title": "Eli Lifland"
    }
  ],
  "ai-governance-legal": [
    {
      "id": "musk-openai-lawsuit",
      "type": "analysis",
      "title": "Musk v. OpenAI Lawsuit"
    }
  ],
  "ai-industry-finance": [
    {
      "id": "anthropic-valuation",
      "type": "analysis",
      "title": "Anthropic Valuation Analysis"
    }
  ],
  "ai-labs": [
    {
      "id": "elon-musk",
      "type": "researcher",
      "title": "Elon Musk"
    }
  ],
  "ai-misuse": [
    {
      "id": "bioweapons",
      "type": "risk",
      "title": "Bioweapons Risk"
    },
    {
      "id": "cyberweapons",
      "type": "risk",
      "title": "Cyberweapons Risk"
    },
    {
      "id": "deepfakes",
      "type": "risk",
      "title": "Deepfakes"
    }
  ],
  "ai-pause": [
    {
      "id": "max-tegmark",
      "type": "researcher",
      "title": "Max Tegmark"
    }
  ],
  "ai-philanthropy": [
    {
      "id": "openai-foundation",
      "type": "organization",
      "title": "OpenAI Foundation"
    }
  ],
  "ai-policy": [
    {
      "id": "short-timeline-policy-implications",
      "type": "analysis",
      "title": "Short Timeline Policy Implications"
    },
    {
      "id": "david-sacks",
      "type": "researcher",
      "title": "David Sacks"
    }
  ],
  "ai-regulation": [
    {
      "id": "leading-the-future",
      "type": "organization",
      "title": "Leading the Future super PAC"
    }
  ],
  "ai-risk": [
    {
      "id": "misuse",
      "type": "concept",
      "title": "AI Misuse"
    }
  ],
  "ai-risk-communication": [
    {
      "id": "cais",
      "type": "lab-research",
      "title": "CAIS"
    }
  ],
  "ai-safety": [
    {
      "id": "agentic-ai",
      "type": "capability",
      "title": "Agentic AI"
    },
    {
      "id": "self-improvement",
      "type": "capability",
      "title": "Self-Improvement and Recursive Enhancement"
    },
    {
      "id": "anthropic",
      "type": "lab",
      "title": "Anthropic"
    },
    {
      "id": "cais",
      "type": "lab-research",
      "title": "CAIS"
    },
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    },
    {
      "id": "arc-evals",
      "type": "organization",
      "title": "ARC Evaluations"
    },
    {
      "id": "chris-olah",
      "type": "researcher",
      "title": "Chris Olah"
    },
    {
      "id": "connor-leahy",
      "type": "researcher",
      "title": "Connor Leahy"
    },
    {
      "id": "dan-hendrycks",
      "type": "researcher",
      "title": "Dan Hendrycks"
    },
    {
      "id": "geoffrey-hinton",
      "type": "researcher",
      "title": "Geoffrey Hinton"
    },
    {
      "id": "ilya-sutskever",
      "type": "researcher",
      "title": "Ilya Sutskever"
    },
    {
      "id": "neel-nanda",
      "type": "researcher",
      "title": "Neel Nanda"
    },
    {
      "id": "toby-ord",
      "type": "researcher",
      "title": "Toby Ord"
    },
    {
      "id": "yoshua-bengio",
      "type": "researcher",
      "title": "Yoshua Bengio"
    },
    {
      "id": "beth-barnes",
      "type": "researcher",
      "title": "Beth Barnes"
    },
    {
      "id": "anthropic-core-views",
      "type": "safety-agenda",
      "title": "Anthropic Core Views"
    },
    {
      "id": "hybrid-systems",
      "type": "approach",
      "title": "AI-Human Hybrid Systems"
    },
    {
      "id": "automation-bias",
      "type": "risk",
      "title": "Automation Bias"
    },
    {
      "id": "deceptive-alignment",
      "type": "risk",
      "title": "Deceptive Alignment"
    },
    {
      "id": "emergent-capabilities",
      "type": "risk",
      "title": "Emergent Capabilities"
    },
    {
      "id": "irreversibility",
      "type": "risk",
      "title": "Irreversibility"
    },
    {
      "id": "sandbagging",
      "type": "risk",
      "title": "Sandbagging"
    },
    {
      "id": "scheming",
      "type": "risk",
      "title": "Scheming"
    }
  ],
  "ai-safety-advocacy": [
    {
      "id": "max-tegmark",
      "type": "researcher",
      "title": "Max Tegmark"
    }
  ],
  "ai-safety-funding": [
    {
      "id": "elon-musk-philanthropy",
      "type": "analysis",
      "title": "Elon Musk (Funder)"
    },
    {
      "id": "holden-karnofsky",
      "type": "researcher",
      "title": "Holden Karnofsky"
    },
    {
      "id": "dustin-moskovitz",
      "type": "researcher",
      "title": "Dustin Moskovitz"
    }
  ],
  "ai-safety-institutes": [
    {
      "id": "bletchley-declaration",
      "type": "policy",
      "title": "Bletchley Declaration"
    },
    {
      "id": "coordination-mechanisms",
      "type": "policy",
      "title": "International Coordination Mechanisms"
    }
  ],
  "ai-safety-lab": [
    {
      "id": "ssi",
      "type": "lab-research",
      "title": "Safe Superintelligence Inc (SSI)"
    }
  ],
  "ai-safety-levels": [
    {
      "id": "dario-amodei",
      "type": "researcher",
      "title": "Dario Amodei"
    }
  ],
  "ai-safety-philosophy": [
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    }
  ],
  "ai-safety-startup": [
    {
      "id": "goodfire",
      "type": "lab-research",
      "title": "Goodfire"
    }
  ],
  "ai-safety-summit": [
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    }
  ],
  "ai-safety-summits": [
    {
      "id": "governance-policy",
      "type": "crux",
      "title": "AI Governance and Policy"
    },
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    }
  ],
  "ai-safety-via-debate": [
    {
      "id": "paul-christiano",
      "type": "researcher",
      "title": "Paul Christiano"
    }
  ],
  "ai-standards": [
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    },
    {
      "id": "nist-ai",
      "type": "organization",
      "title": "NIST and AI Safety"
    },
    {
      "id": "frontier-model-forum",
      "type": "organization",
      "title": "Frontier Model Forum"
    }
  ],
  "ai-summit": [
    {
      "id": "bletchley-declaration",
      "type": "policy",
      "title": "Bletchley Declaration"
    }
  ],
  "ai-timelines": [
    {
      "id": "fast-takeoff",
      "type": "concept",
      "title": "Fast Takeoff"
    },
    {
      "id": "transformative-ai",
      "type": "concept",
      "title": "Transformative AI"
    },
    {
      "id": "capability-alignment-race",
      "type": "analysis",
      "title": "Capability-Alignment Race Model"
    },
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    },
    {
      "id": "holden-karnofsky",
      "type": "researcher",
      "title": "Holden Karnofsky"
    }
  ],
  "ai-transition-model": [
    {
      "id": "misalignment-potential",
      "type": "ai-transition-model-factor",
      "title": "Misalignment Potential"
    },
    {
      "id": "misuse-potential",
      "type": "ai-transition-model-factor",
      "title": "Misuse Potential"
    },
    {
      "id": "ai-capabilities",
      "type": "ai-transition-model-factor",
      "title": "AI Capabilities"
    },
    {
      "id": "ai-uses",
      "type": "ai-transition-model-factor",
      "title": "AI Uses"
    },
    {
      "id": "ai-ownership",
      "type": "ai-transition-model-factor",
      "title": "AI Ownership"
    },
    {
      "id": "civilizational-competence",
      "type": "ai-transition-model-factor",
      "title": "Civilizational Competence"
    },
    {
      "id": "transition-turbulence",
      "type": "ai-transition-model-factor",
      "title": "Transition Turbulence"
    },
    {
      "id": "existential-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Existential Catastrophe"
    },
    {
      "id": "long-term-trajectory",
      "type": "ai-transition-model-scenario",
      "title": "Long-term Trajectory"
    },
    {
      "id": "ai-takeover",
      "type": "ai-transition-model-scenario",
      "title": "AI Takeover"
    },
    {
      "id": "human-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Human-Caused Catastrophe"
    },
    {
      "id": "long-term-lockin",
      "type": "ai-transition-model-scenario",
      "title": "Long-term Lock-in"
    }
  ],
  "alan-turing": [
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    }
  ],
  "alexnet": [
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    }
  ],
  "algorithmic-accountability": [
    {
      "id": "china-ai-regulations",
      "type": "policy",
      "title": "China AI Regulatory Framework"
    },
    {
      "id": "institutional-capture",
      "type": "risk",
      "title": "Institutional Decision Capture"
    }
  ],
  "algorithmic-efficiency": [
    {
      "id": "thresholds",
      "type": "policy",
      "title": "Compute Thresholds"
    }
  ],
  "algorithmic-progress": [
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    }
  ],
  "algorithmic-trading": [
    {
      "id": "flash-dynamics",
      "type": "risk",
      "title": "Flash Dynamics"
    }
  ],
  "algorithms": [
    {
      "id": "tmc-algorithms",
      "type": "ai-transition-model-subitem",
      "title": "Algorithms"
    }
  ],
  "aligned-agi": [
    {
      "id": "aligned-agi",
      "type": "ai-transition-model-scenario",
      "title": "Aligned AGI - The Good Ending"
    }
  ],
  "alignment": [
    {
      "id": "misalignment-potential",
      "type": "ai-transition-model-factor",
      "title": "Misalignment Potential"
    },
    {
      "id": "alignment-progress",
      "type": "ai-transition-model-metric",
      "title": "Alignment Progress"
    },
    {
      "id": "alignment-robustness",
      "type": "ai-transition-model-parameter",
      "title": "Alignment Robustness"
    },
    {
      "id": "rlhf",
      "type": "capability",
      "title": "RLHF"
    },
    {
      "id": "research-agendas",
      "type": "crux",
      "title": "Research Agendas"
    },
    {
      "id": "why-alignment-easy",
      "type": "argument",
      "title": "Why Alignment Might Be Easy"
    },
    {
      "id": "why-alignment-hard",
      "type": "argument",
      "title": "Why Alignment Might Be Hard"
    },
    {
      "id": "alignment-robustness-trajectory",
      "type": "model",
      "title": "Alignment Robustness Trajectory Model"
    },
    {
      "id": "ssi",
      "type": "lab-research",
      "title": "Safe Superintelligence Inc (SSI)"
    },
    {
      "id": "eliezer-yudkowsky",
      "type": "researcher",
      "title": "Eliezer Yudkowsky"
    },
    {
      "id": "value-learning",
      "type": "safety-agenda",
      "title": "Value Learning"
    },
    {
      "id": "prosaic-alignment",
      "type": "safety-agenda",
      "title": "Prosaic Alignment"
    },
    {
      "id": "alignment",
      "type": "approach",
      "title": "AI Alignment"
    },
    {
      "id": "epistemic-sycophancy",
      "type": "risk",
      "title": "Epistemic Sycophancy"
    }
  ],
  "alignment-difficulty": [
    {
      "id": "accident-risks",
      "type": "crux",
      "title": "Accident Risk Cruxes"
    },
    {
      "id": "critical-uncertainties",
      "type": "crux",
      "title": "Critical Uncertainties Model"
    }
  ],
  "alignment-evaluation": [
    {
      "id": "alignment-evals",
      "type": "approach",
      "title": "Alignment Evaluations"
    }
  ],
  "alignment-failures": [
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    }
  ],
  "alignment-faking": [
    {
      "id": "scheming-likelihood-model",
      "type": "model",
      "title": "Scheming Likelihood Assessment"
    }
  ],
  "alignment-race": [
    {
      "id": "capability-alignment-race",
      "type": "analysis",
      "title": "Capability-Alignment Race Model"
    }
  ],
  "alignment-research": [
    {
      "id": "model-organisms-of-misalignment",
      "type": "analysis",
      "title": "Model Organisms of Misalignment"
    },
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    },
    {
      "id": "ilya-sutskever",
      "type": "researcher",
      "title": "Ilya Sutskever"
    }
  ],
  "alignment-stability": [
    {
      "id": "sharp-left-turn",
      "type": "risk",
      "title": "Sharp Left Turn"
    }
  ],
  "alignment-theory": [
    {
      "id": "eliciting-latent-knowledge",
      "type": "approach",
      "title": "Eliciting Latent Knowledge (ELK)"
    },
    {
      "id": "debate",
      "type": "approach",
      "title": "AI Safety via Debate"
    }
  ],
  "alignment-training": [
    {
      "id": "preference-optimization",
      "type": "approach",
      "title": "Preference Optimization Methods"
    }
  ],
  "alphafold": [
    {
      "id": "scientific-research",
      "type": "capability",
      "title": "Scientific Research Capabilities"
    },
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    }
  ],
  "alphago": [
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    },
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    }
  ],
  "alternative-paradigms": [
    {
      "id": "conjecture",
      "type": "lab-research",
      "title": "Conjecture"
    }
  ],
  "amplification": [
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    }
  ],
  "andreessen-horowitz": [
    {
      "id": "marc-andreessen",
      "type": "researcher",
      "title": "Marc Andreessen"
    }
  ],
  "anthropic": [
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    },
    {
      "id": "anthropic-valuation",
      "type": "analysis",
      "title": "Anthropic Valuation Analysis"
    },
    {
      "id": "anthropic-investors",
      "type": "analysis",
      "title": "Anthropic (Funder)"
    },
    {
      "id": "long-term-benefit-trust",
      "type": "analysis",
      "title": "Long-Term Benefit Trust (Anthropic)"
    },
    {
      "id": "anthropic-ipo",
      "type": "analysis",
      "title": "Anthropic IPO"
    },
    {
      "id": "anthropic-pledge-enforcement",
      "type": "analysis",
      "title": "Anthropic Founder Pledges: Interventions to Increase Follow-Through"
    },
    {
      "id": "anthropic-pre-ipo-daf-transfers",
      "type": "analysis",
      "title": "Anthropic Pre-IPO DAF Transfers"
    },
    {
      "id": "anthropic-impact",
      "type": "analysis",
      "title": "Anthropic Impact Assessment Model"
    },
    {
      "id": "anthropic-core-views",
      "type": "safety-agenda",
      "title": "Anthropic Core Views"
    },
    {
      "id": "constitutional-ai",
      "type": "approach",
      "title": "Constitutional AI"
    }
  ],
  "anthropic-research": [
    {
      "id": "sleeper-agents",
      "type": "risk",
      "title": "Sleeper Agents: Training Deceptive LLMs"
    }
  ],
  "anti-alignment": [
    {
      "id": "marc-andreessen",
      "type": "researcher",
      "title": "Marc Andreessen"
    }
  ],
  "anti-regulation": [
    {
      "id": "david-sacks",
      "type": "researcher",
      "title": "David Sacks"
    }
  ],
  "antitrust": [
    {
      "id": "knowledge-monopoly",
      "type": "risk",
      "title": "AI Knowledge Monopoly"
    },
    {
      "id": "winner-take-all",
      "type": "risk",
      "title": "Winner-Take-All Dynamics"
    }
  ],
  "api-access": [
    {
      "id": "structured-access",
      "type": "approach",
      "title": "Structured Access / API-Only"
    }
  ],
  "api-integration": [
    {
      "id": "tool-use",
      "type": "capability",
      "title": "Tool Use and Computer Use"
    }
  ],
  "applications": [
    {
      "id": "ai-uses",
      "type": "ai-transition-model-factor",
      "title": "AI Uses"
    }
  ],
  "arc-research": [
    {
      "id": "eliciting-latent-knowledge",
      "type": "approach",
      "title": "Eliciting Latent Knowledge (ELK)"
    }
  ],
  "archives": [
    {
      "id": "historical-revisionism",
      "type": "risk",
      "title": "AI-Enabled Historical Revisionism"
    }
  ],
  "argument": [
    {
      "id": "why-alignment-easy",
      "type": "argument",
      "title": "Why Alignment Might Be Easy"
    },
    {
      "id": "case-against-xrisk",
      "type": "argument",
      "title": "The Case Against AI Existential Risk"
    },
    {
      "id": "why-alignment-hard",
      "type": "argument",
      "title": "Why Alignment Might Be Hard"
    },
    {
      "id": "case-for-xrisk",
      "type": "argument",
      "title": "The Case For AI Existential Risk"
    }
  ],
  "aria": [
    {
      "id": "provable-safe",
      "type": "concept",
      "title": "Provable / Guaranteed Safe AI"
    }
  ],
  "aria-programme": [
    {
      "id": "provably-safe",
      "type": "approach",
      "title": "Provably Safe AI (davidad agenda)"
    }
  ],
  "arms-control": [
    {
      "id": "autonomous-weapons",
      "type": "risk",
      "title": "Autonomous Weapons"
    }
  ],
  "arms-race": [
    {
      "id": "disinformation-detection-race",
      "type": "model",
      "title": "Disinformation Detection Arms Race Model"
    },
    {
      "id": "racing-dynamics",
      "type": "risk",
      "title": "Racing Dynamics"
    }
  ],
  "assistance-games": [
    {
      "id": "chai",
      "type": "lab-academic",
      "title": "CHAI"
    }
  ],
  "astroturfing": [
    {
      "id": "consensus-manufacturing",
      "type": "risk",
      "title": "Consensus Manufacturing"
    }
  ],
  "attack-chain": [
    {
      "id": "bioweapons-attack-chain",
      "type": "model",
      "title": "Bioweapons Attack Chain Model"
    }
  ],
  "attention-mechanism": [
    {
      "id": "dense-transformers",
      "type": "concept",
      "title": "Dense Transformers"
    }
  ],
  "attitudes": [
    {
      "id": "public-opinion-evolution",
      "type": "model",
      "title": "Public Opinion Evolution Model"
    }
  ],
  "audit-capacity": [
    {
      "id": "scalable-eval-approaches",
      "type": "approach",
      "title": "Scalable Eval Approaches"
    }
  ],
  "auditing": [
    {
      "id": "safety-cases",
      "type": "approach",
      "title": "AI Safety Cases"
    }
  ],
  "authentication": [
    {
      "id": "deepfakes-authentication-crisis",
      "type": "model",
      "title": "Deepfakes Authentication Crisis Model"
    },
    {
      "id": "authentication-collapse-timeline",
      "type": "model",
      "title": "Authentication Collapse Timeline Model"
    },
    {
      "id": "deepfakes",
      "type": "risk",
      "title": "Deepfakes"
    },
    {
      "id": "legal-evidence-crisis",
      "type": "risk",
      "title": "Legal Evidence Crisis"
    }
  ],
  "authoritarianism": [
    {
      "id": "surveillance-authoritarian-stability",
      "type": "model",
      "title": "AI Surveillance and Regime Durability Model"
    },
    {
      "id": "authoritarian-tools-diffusion",
      "type": "model",
      "title": "Authoritarian Tools Diffusion Model"
    },
    {
      "id": "authoritarian-tools",
      "type": "risk",
      "title": "AI Authoritarian Tools"
    },
    {
      "id": "authoritarian-takeover",
      "type": "risk",
      "title": "Authoritarian Takeover"
    },
    {
      "id": "surveillance",
      "type": "risk",
      "title": "AI Mass Surveillance"
    }
  ],
  "automated-evals": [
    {
      "id": "scalable-eval-approaches",
      "type": "approach",
      "title": "Scalable Eval Approaches"
    }
  ],
  "automation": [
    {
      "id": "economic-labor",
      "type": "ai-transition-model-metric",
      "title": "Economic & Labor"
    },
    {
      "id": "cyberweapons-attack-automation",
      "type": "model",
      "title": "Autonomous Cyber Attack Timeline"
    },
    {
      "id": "economic-disruption-model",
      "type": "model",
      "title": "Economic Disruption Structural Model"
    },
    {
      "id": "economic-disruption",
      "type": "risk",
      "title": "Economic Disruption"
    },
    {
      "id": "enfeeblement",
      "type": "risk",
      "title": "Enfeeblement"
    },
    {
      "id": "expertise-atrophy",
      "type": "risk",
      "title": "Expertise Atrophy"
    }
  ],
  "automation-bias": [
    {
      "id": "hybrid-systems",
      "type": "approach",
      "title": "AI-Human Hybrid Systems"
    },
    {
      "id": "institutional-capture",
      "type": "risk",
      "title": "Institutional Decision Capture"
    }
  ],
  "automl": [
    {
      "id": "self-improvement",
      "type": "capability",
      "title": "Self-Improvement and Recursive Enhancement"
    }
  ],
  "autonomous-agents": [
    {
      "id": "tool-use",
      "type": "capability",
      "title": "Tool Use and Computer Use"
    }
  ],
  "autonomous-hacking": [
    {
      "id": "palisade-research",
      "type": "lab-research",
      "title": "Palisade Research"
    }
  ],
  "autonomous-operation": [
    {
      "id": "heavy-scaffolding",
      "type": "concept",
      "title": "Heavy Scaffolding / Agentic Systems"
    }
  ],
  "autonomous-replication": [
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    },
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    }
  ],
  "autonomous-weapons": [
    {
      "id": "autonomous-weapons-escalation",
      "type": "model",
      "title": "Autonomous Weapons Escalation Model"
    },
    {
      "id": "autonomous-weapons-proliferation",
      "type": "model",
      "title": "LAWS Proliferation Model"
    },
    {
      "id": "geoffrey-hinton",
      "type": "researcher",
      "title": "Geoffrey Hinton"
    }
  ],
  "autonomy": [
    {
      "id": "human-agency",
      "type": "ai-transition-model-parameter",
      "title": "Human Agency"
    },
    {
      "id": "preference-authenticity",
      "type": "ai-transition-model-parameter",
      "title": "Preference Authenticity"
    },
    {
      "id": "cyberweapons-attack-automation",
      "type": "model",
      "title": "Autonomous Cyber Attack Timeline"
    },
    {
      "id": "preference-manipulation-drift",
      "type": "model",
      "title": "Preference Manipulation Drift Model"
    },
    {
      "id": "erosion-of-agency",
      "type": "risk",
      "title": "Erosion of Human Agency"
    },
    {
      "id": "preference-manipulation",
      "type": "risk",
      "title": "Preference Manipulation"
    },
    {
      "id": "autonomous-replication",
      "type": "risk",
      "title": "Autonomous Replication"
    }
  ],
  "backdoor-attacks": [
    {
      "id": "sleeper-agents",
      "type": "risk",
      "title": "Sleeper Agents: Training Deceptive LLMs"
    }
  ],
  "backdoor-detection": [
    {
      "id": "sleeper-agent-detection",
      "type": "approach",
      "title": "Sleeper Agent Detection"
    }
  ],
  "backpropagation": [
    {
      "id": "geoffrey-hinton",
      "type": "researcher",
      "title": "Geoffrey Hinton"
    }
  ],
  "balance": [
    {
      "id": "cyberweapons-offense-defense",
      "type": "model",
      "title": "Cyber Offense-Defense Balance Model"
    }
  ],
  "base-case": [
    {
      "id": "slow-takeoff-muddle",
      "type": "ai-transition-model-scenario",
      "title": "Slow Takeoff Muddle - Muddling Through"
    }
  ],
  "base-optimizer": [
    {
      "id": "mesa-optimization",
      "type": "risk",
      "title": "Mesa-Optimization"
    }
  ],
  "behavior-steering": [
    {
      "id": "representation-engineering",
      "type": "approach",
      "title": "Representation Engineering"
    }
  ],
  "behavioral-change": [
    {
      "id": "preference-manipulation-drift",
      "type": "model",
      "title": "Preference Manipulation Drift Model"
    }
  ],
  "behavioral-evaluation": [
    {
      "id": "evaluation-awareness",
      "type": "approach",
      "title": "Evaluation Awareness"
    }
  ],
  "behavioral-testing": [
    {
      "id": "scheming-detection",
      "type": "approach",
      "title": "Scheming & Deception Detection"
    },
    {
      "id": "alignment-evals",
      "type": "approach",
      "title": "Alignment Evaluations"
    }
  ],
  "belief-extraction": [
    {
      "id": "eliciting-latent-knowledge",
      "type": "approach",
      "title": "Eliciting Latent Knowledge (ELK)"
    }
  ],
  "benchmarking": [
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    }
  ],
  "benchmarks": [
    {
      "id": "capabilities",
      "type": "ai-transition-model-metric",
      "title": "AI Capabilities"
    },
    {
      "id": "dan-hendrycks",
      "type": "researcher",
      "title": "Dan Hendrycks"
    },
    {
      "id": "evals",
      "type": "safety-agenda",
      "title": "AI Evaluations"
    },
    {
      "id": "eval-saturation",
      "type": "approach",
      "title": "Eval Saturation & The Evals Gap"
    }
  ],
  "big-tech": [
    {
      "id": "winner-take-all",
      "type": "risk",
      "title": "Winner-Take-All Dynamics"
    }
  ],
  "binding-regulation": [
    {
      "id": "controlai",
      "type": "organization",
      "title": "ControlAI"
    }
  ],
  "bio-risk": [
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    }
  ],
  "biosecurity": [
    {
      "id": "biological-threat-exposure",
      "type": "ai-transition-model-parameter",
      "title": "Biological Threat Exposure"
    },
    {
      "id": "ea-biosecurity-scope",
      "type": "analysis",
      "title": "Is EA Biosecurity Work Limited to Restricting LLM Biological Use?"
    },
    {
      "id": "johns-hopkins-center-for-health-security",
      "type": "organization",
      "title": "Johns Hopkins Center for Health Security"
    },
    {
      "id": "centre-for-long-term-resilience",
      "type": "organization",
      "title": "Centre for Long-Term Resilience"
    },
    {
      "id": "bioweapons",
      "type": "risk",
      "title": "Bioweapons Risk"
    },
    {
      "id": "bio-risk",
      "type": "risk",
      "title": "AI-Enabled Biological Risks"
    }
  ],
  "bioweapons": [
    {
      "id": "misuse-risks",
      "type": "crux",
      "title": "Misuse Risk Cruxes"
    },
    {
      "id": "bioweapons-attack-chain",
      "type": "model",
      "title": "Bioweapons Attack Chain Model"
    },
    {
      "id": "bioweapons-ai-uplift",
      "type": "model",
      "title": "AI Uplift Assessment Model"
    },
    {
      "id": "bioweapons-timeline",
      "type": "model",
      "title": "AI-Bioweapons Timeline Model"
    },
    {
      "id": "dangerous-cap-evals",
      "type": "approach",
      "title": "Dangerous Capability Evaluations"
    },
    {
      "id": "capability-unlearning",
      "type": "approach",
      "title": "Capability Unlearning / Removal"
    }
  ],
  "bioweapons-risk": [
    {
      "id": "scientific-research",
      "type": "capability",
      "title": "Scientific Research Capabilities"
    }
  ],
  "bletchley-declaration": [
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    },
    {
      "id": "international-summits",
      "type": "policy",
      "title": "International AI Safety Summit Series"
    }
  ],
  "board-oversight": [
    {
      "id": "openai-foundation-governance",
      "type": "analysis",
      "title": "OpenAI Foundation Governance Paradox"
    },
    {
      "id": "long-term-benefit-trust",
      "type": "analysis",
      "title": "Long-Term Benefit Trust (Anthropic)"
    }
  ],
  "bot-detection": [
    {
      "id": "consensus-manufacturing",
      "type": "risk",
      "title": "Consensus Manufacturing"
    }
  ],
  "calibration": [
    {
      "id": "philip-tetlock",
      "type": "researcher",
      "title": "Philip Tetlock"
    },
    {
      "id": "ai-forecasting",
      "type": "approach",
      "title": "AI-Augmented Forecasting"
    }
  ],
  "california": [
    {
      "id": "california-sb1047",
      "type": "policy",
      "title": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act"
    },
    {
      "id": "california-sb53",
      "type": "policy",
      "title": "California SB 53"
    }
  ],
  "capabilities": [
    {
      "id": "ai-capabilities",
      "type": "ai-transition-model-factor",
      "title": "AI Capabilities"
    },
    {
      "id": "capabilities",
      "type": "ai-transition-model-metric",
      "title": "AI Capabilities"
    },
    {
      "id": "capability-evaluations",
      "type": "concept",
      "title": "Capability Evaluations"
    },
    {
      "id": "benchmarking",
      "type": "concept",
      "title": "AI Benchmarking"
    },
    {
      "id": "scaling-laws",
      "type": "concept",
      "title": "Scaling Laws"
    },
    {
      "id": "ai-timelines",
      "type": "concept",
      "title": "AI Timelines"
    },
    {
      "id": "data-constraints",
      "type": "concept",
      "title": "Data Constraints"
    },
    {
      "id": "scaling-debate",
      "type": "crux",
      "title": "Is Scaling All You Need?"
    },
    {
      "id": "safety-capability-tradeoff",
      "type": "model",
      "title": "Safety-Capability Tradeoff Model"
    }
  ],
  "capability": [
    {
      "id": "risk-activation-timeline",
      "type": "model",
      "title": "Risk Activation Timeline Model"
    },
    {
      "id": "capability-threshold-model",
      "type": "model",
      "title": "Capability Threshold Model"
    }
  ],
  "capability-assessment": [
    {
      "id": "evals",
      "type": "safety-agenda",
      "title": "AI Evaluations"
    },
    {
      "id": "capability-elicitation",
      "type": "approach",
      "title": "Capability Elicitation"
    },
    {
      "id": "evaluation",
      "type": "approach",
      "title": "AI Evaluation"
    }
  ],
  "capability-elicitation": [
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    }
  ],
  "capability-evaluation": [
    {
      "id": "connor-leahy",
      "type": "researcher",
      "title": "Connor Leahy"
    },
    {
      "id": "emergent-capabilities",
      "type": "risk",
      "title": "Emergent Capabilities"
    }
  ],
  "capability-gap": [
    {
      "id": "capability-alignment-race",
      "type": "analysis",
      "title": "Capability-Alignment Race Model"
    }
  ],
  "capability-generalization": [
    {
      "id": "goal-misgeneralization",
      "type": "risk",
      "title": "Goal Misgeneralization"
    },
    {
      "id": "sharp-left-turn",
      "type": "risk",
      "title": "Sharp Left Turn"
    }
  ],
  "capability-growth": [
    {
      "id": "feedback-loops",
      "type": "analysis",
      "title": "Feedback Loop & Cascade Model"
    }
  ],
  "capability-progression": [
    {
      "id": "fraud-sophistication-curve",
      "type": "model",
      "title": "Fraud Sophistication Curve Model"
    }
  ],
  "capability-removal": [
    {
      "id": "capability-unlearning",
      "type": "approach",
      "title": "Capability Unlearning / Removal"
    }
  ],
  "capability-restrictions": [
    {
      "id": "tool-restrictions",
      "type": "approach",
      "title": "Tool-Use Restrictions"
    }
  ],
  "capability-uplift": [
    {
      "id": "misuse-risks",
      "type": "crux",
      "title": "Misuse Risk Cruxes"
    }
  ],
  "capacity-building": [
    {
      "id": "regulatory-capacity-threshold",
      "type": "model",
      "title": "Regulatory Capacity Threshold Model"
    }
  ],
  "career-development": [
    {
      "id": "field-building",
      "type": "crux",
      "title": "Field Building and Community"
    },
    {
      "id": "training-programs",
      "type": "approach",
      "title": "AI Safety Training Programs"
    }
  ],
  "career-transitions": [
    {
      "id": "capabilities-to-safety-pipeline",
      "type": "model",
      "title": "Capabilities-to-Safety Pipeline Model"
    }
  ],
  "cascade": [
    {
      "id": "trust-cascade-model",
      "type": "model",
      "title": "Trust Cascade Failure Model"
    },
    {
      "id": "expertise-atrophy-cascade",
      "type": "model",
      "title": "Expertise Atrophy Cascade Model"
    }
  ],
  "cascades": [
    {
      "id": "flash-dynamics-threshold",
      "type": "model",
      "title": "Flash Dynamics Threshold Model"
    },
    {
      "id": "risk-cascade-pathways",
      "type": "model",
      "title": "Risk Cascade Pathways Model"
    }
  ],
  "catastrophe": [
    {
      "id": "existential-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Existential Catastrophe"
    },
    {
      "id": "misaligned-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Misaligned Catastrophe - The Bad Ending"
    }
  ],
  "catastrophic-risk": [
    {
      "id": "existential-risk",
      "type": "concept",
      "title": "Existential Risk"
    }
  ],
  "causal-model": [
    {
      "id": "corrigibility-failure-pathways",
      "type": "model",
      "title": "Corrigibility Failure Pathways"
    }
  ],
  "causal-representation-learning": [
    {
      "id": "yoshua-bengio",
      "type": "researcher",
      "title": "Yoshua Bengio"
    }
  ],
  "causal-scrubbing": [
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    }
  ],
  "cbrn": [
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    },
    {
      "id": "bioweapons",
      "type": "risk",
      "title": "Bioweapons Risk"
    }
  ],
  "cev": [
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    },
    {
      "id": "eliezer-yudkowsky",
      "type": "researcher",
      "title": "Eliezer Yudkowsky"
    }
  ],
  "chain-of-thought": [
    {
      "id": "scheming-detection",
      "type": "approach",
      "title": "Scheming & Deception Detection"
    },
    {
      "id": "process-supervision",
      "type": "approach",
      "title": "Process Supervision"
    }
  ],
  "charitable-trust": [
    {
      "id": "musk-openai-lawsuit",
      "type": "analysis",
      "title": "Musk v. OpenAI Lawsuit"
    }
  ],
  "chatgpt": [
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    },
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    }
  ],
  "chilling-effects": [
    {
      "id": "surveillance-chilling-effects",
      "type": "model",
      "title": "Surveillance Chilling Effects Model"
    }
  ],
  "china": [
    {
      "id": "china-ai-regulations",
      "type": "policy",
      "title": "China AI Regulatory Framework"
    }
  ],
  "chip-tracking": [
    {
      "id": "hardware-enabled-governance",
      "type": "policy",
      "title": "Hardware-Enabled Governance"
    }
  ],
  "circuit-analysis": [
    {
      "id": "conjecture",
      "type": "lab-research",
      "title": "Conjecture"
    }
  ],
  "circuit-discovery": [
    {
      "id": "mech-interp",
      "type": "approach",
      "title": "Mechanistic Interpretability"
    }
  ],
  "circuits": [
    {
      "id": "interpretability",
      "type": "safety-agenda",
      "title": "Interpretability"
    }
  ],
  "class-n-shares": [
    {
      "id": "openai-foundation-governance",
      "type": "analysis",
      "title": "OpenAI Foundation Governance Paradox"
    }
  ],
  "claude": [
    {
      "id": "language-models",
      "type": "capability",
      "title": "Large Language Models"
    },
    {
      "id": "anthropic",
      "type": "lab",
      "title": "Anthropic"
    },
    {
      "id": "dario-amodei",
      "type": "researcher",
      "title": "Dario Amodei"
    }
  ],
  "cloud-computing": [
    {
      "id": "compute-governance",
      "type": "policy",
      "title": "Compute Governance"
    },
    {
      "id": "compute-monitoring",
      "type": "policy",
      "title": "Compute Monitoring"
    },
    {
      "id": "us-executive-order",
      "type": "policy",
      "title": "Executive Order on Safe, Secure, and Trustworthy AI"
    }
  ],
  "cloud-kyc": [
    {
      "id": "monitoring",
      "type": "policy",
      "title": "Compute Monitoring"
    }
  ],
  "code-execution": [
    {
      "id": "tool-use",
      "type": "capability",
      "title": "Tool Use and Computer Use"
    }
  ],
  "code-generation": [
    {
      "id": "coding",
      "type": "capability",
      "title": "Autonomous Coding"
    }
  ],
  "coefficient-giving": [
    {
      "id": "dustin-moskovitz",
      "type": "researcher",
      "title": "Dustin Moskovitz"
    }
  ],
  "coem": [
    {
      "id": "conjecture",
      "type": "lab-research",
      "title": "Conjecture"
    }
  ],
  "cognitive": [
    {
      "id": "human-expertise",
      "type": "ai-transition-model-parameter",
      "title": "Human Expertise"
    }
  ],
  "cognitive-bias": [
    {
      "id": "automation-bias-cascade",
      "type": "model",
      "title": "Automation Bias Cascade Model"
    },
    {
      "id": "automation-bias",
      "type": "risk",
      "title": "Automation Bias"
    }
  ],
  "cognitive-emulation": [
    {
      "id": "conjecture",
      "type": "lab-research",
      "title": "Conjecture"
    }
  ],
  "collapse": [
    {
      "id": "epistemic-collapse-threshold",
      "type": "model",
      "title": "Epistemic Collapse Threshold Model"
    }
  ],
  "collective-action": [
    {
      "id": "multipolar-trap-model",
      "type": "model",
      "title": "Multipolar Trap Coordination Model"
    },
    {
      "id": "multipolar-trap",
      "type": "risk",
      "title": "Multipolar Trap"
    }
  ],
  "collective-intelligence": [
    {
      "id": "deliberation",
      "type": "approach",
      "title": "AI-Assisted Deliberation"
    },
    {
      "id": "prediction-markets",
      "type": "approach",
      "title": "Prediction Markets"
    }
  ],
  "collective-memory": [
    {
      "id": "historical-revisionism",
      "type": "risk",
      "title": "AI-Enabled Historical Revisionism"
    }
  ],
  "collusion-risk": [
    {
      "id": "multi-agent",
      "type": "approach",
      "title": "Multi-Agent Safety"
    }
  ],
  "community": [
    {
      "id": "field-building",
      "type": "crux",
      "title": "Field Building and Community"
    }
  ],
  "comparative": [
    {
      "id": "cyberweapons-offense-defense",
      "type": "model",
      "title": "Cyber Offense-Defense Balance Model"
    }
  ],
  "comparative-analysis": [
    {
      "id": "ai-risk-portfolio-analysis",
      "type": "model",
      "title": "AI Risk Portfolio Analysis"
    }
  ],
  "comparison": [
    {
      "id": "bioweapons-ai-uplift",
      "type": "model",
      "title": "AI Uplift Assessment Model"
    }
  ],
  "competition": [
    {
      "id": "multipolar-competition",
      "type": "ai-transition-model-scenario",
      "title": "Multipolar Competition - The Fragmented World"
    },
    {
      "id": "agi-race",
      "type": "concept",
      "title": "AGI Race"
    },
    {
      "id": "racing-dynamics-impact",
      "type": "model",
      "title": "Racing Dynamics Impact Model"
    },
    {
      "id": "multipolar-trap",
      "type": "risk",
      "title": "Multipolar Trap"
    },
    {
      "id": "racing-dynamics",
      "type": "risk",
      "title": "Racing Dynamics"
    }
  ],
  "compounding-effects": [
    {
      "id": "compounding-risks-analysis",
      "type": "model",
      "title": "Compounding Risks Analysis Model"
    }
  ],
  "compounding-risks": [
    {
      "id": "risk-interaction-matrix",
      "type": "model",
      "title": "Risk Interaction Matrix"
    }
  ],
  "compute": [
    {
      "id": "compute-hardware",
      "type": "ai-transition-model-metric",
      "title": "Compute & Hardware"
    },
    {
      "id": "tmc-compute",
      "type": "ai-transition-model-subitem",
      "title": "Compute"
    }
  ],
  "compute-governance": [
    {
      "id": "governance-policy",
      "type": "crux",
      "title": "AI Governance and Policy"
    },
    {
      "id": "governance-focused",
      "type": "concept",
      "title": "Governance-Focused Worldview"
    },
    {
      "id": "critical-uncertainties",
      "type": "crux",
      "title": "Critical Uncertainties Model"
    },
    {
      "id": "short-timeline-policy-implications",
      "type": "analysis",
      "title": "Short Timeline Policy Implications"
    },
    {
      "id": "proliferation-model",
      "type": "model",
      "title": "AI Capability Proliferation Model"
    },
    {
      "id": "govai",
      "type": "lab-research",
      "title": "GovAI"
    },
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    },
    {
      "id": "dan-hendrycks",
      "type": "researcher",
      "title": "Dan Hendrycks"
    },
    {
      "id": "compute-thresholds",
      "type": "policy",
      "title": "Compute Thresholds"
    },
    {
      "id": "compute-monitoring",
      "type": "policy",
      "title": "Compute Monitoring"
    },
    {
      "id": "international-compute-regimes",
      "type": "policy",
      "title": "International Compute Regimes"
    },
    {
      "id": "hardware-enabled-governance",
      "type": "policy",
      "title": "Hardware-Enabled Governance"
    },
    {
      "id": "monitoring",
      "type": "policy",
      "title": "Compute Monitoring"
    },
    {
      "id": "thresholds",
      "type": "policy",
      "title": "Compute Thresholds"
    },
    {
      "id": "international-regimes",
      "type": "policy",
      "title": "International Compute Regimes"
    }
  ],
  "compute-monitoring": [
    {
      "id": "monitoring",
      "type": "policy",
      "title": "Compute Monitoring"
    }
  ],
  "compute-scaling": [
    {
      "id": "capability-alignment-race",
      "type": "analysis",
      "title": "Capability-Alignment Race Model"
    }
  ],
  "compute-thresholds": [
    {
      "id": "california-sb1047",
      "type": "policy",
      "title": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act"
    },
    {
      "id": "compute-governance",
      "type": "policy",
      "title": "Compute Governance"
    },
    {
      "id": "eu-ai-act",
      "type": "policy",
      "title": "EU AI Act"
    },
    {
      "id": "us-executive-order",
      "type": "policy",
      "title": "Executive Order on Safe, Secure, and Trustworthy AI"
    },
    {
      "id": "thresholds",
      "type": "policy",
      "title": "Compute Thresholds"
    },
    {
      "id": "model-registries",
      "type": "policy",
      "title": "Model Registries"
    }
  ],
  "compute-trends": [
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    }
  ],
  "computer-use": [
    {
      "id": "agentic-ai",
      "type": "capability",
      "title": "Agentic AI"
    },
    {
      "id": "tool-use",
      "type": "capability",
      "title": "Tool Use and Computer Use"
    }
  ],
  "concentration": [
    {
      "id": "ai-ownership",
      "type": "ai-transition-model-factor",
      "title": "AI Ownership"
    },
    {
      "id": "winner-take-all-concentration",
      "type": "model",
      "title": "Winner-Take-All Concentration Model"
    }
  ],
  "concerned": [
    {
      "id": "case-for-xrisk",
      "type": "argument",
      "title": "The Case For AI Existential Risk"
    }
  ],
  "concrete-problems": [
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    }
  ],
  "conflict": [
    {
      "id": "autonomous-weapons-escalation",
      "type": "model",
      "title": "Autonomous Weapons Escalation Model"
    }
  ],
  "consciousness": [
    {
      "id": "ai-welfare",
      "type": "concept",
      "title": "AI Welfare and Digital Minds"
    }
  ],
  "consensus-building": [
    {
      "id": "deliberation",
      "type": "approach",
      "title": "AI-Assisted Deliberation"
    }
  ],
  "constitutional-ai": [
    {
      "id": "research-agendas",
      "type": "crux",
      "title": "Research Agendas"
    },
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    },
    {
      "id": "anthropic",
      "type": "lab",
      "title": "Anthropic"
    },
    {
      "id": "dario-amodei",
      "type": "researcher",
      "title": "Dario Amodei"
    },
    {
      "id": "anthropic-core-views",
      "type": "safety-agenda",
      "title": "Anthropic Core Views"
    },
    {
      "id": "constitutional-ai",
      "type": "approach",
      "title": "Constitutional AI"
    }
  ],
  "container-security": [
    {
      "id": "sandboxing",
      "type": "approach",
      "title": "Sandboxing / Containment"
    }
  ],
  "containment": [
    {
      "id": "ai-control",
      "type": "safety-agenda",
      "title": "AI Control"
    },
    {
      "id": "sandboxing",
      "type": "approach",
      "title": "Sandboxing / Containment"
    }
  ],
  "content-authentication": [
    {
      "id": "epistemic-risks",
      "type": "crux",
      "title": "Epistemic Cruxes"
    },
    {
      "id": "epistemic-security",
      "type": "approach",
      "title": "Epistemic Security"
    }
  ],
  "content-control": [
    {
      "id": "china-ai-regulations",
      "type": "policy",
      "title": "China AI Regulatory Framework"
    }
  ],
  "content-moderation": [
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    }
  ],
  "content-provenance": [
    {
      "id": "legal-evidence-crisis",
      "type": "risk",
      "title": "Legal Evidence Crisis"
    }
  ],
  "content-verification": [
    {
      "id": "authentication-collapse",
      "type": "risk",
      "title": "Authentication Collapse"
    }
  ],
  "control": [
    {
      "id": "proliferation-risk-model",
      "type": "model",
      "title": "AI Proliferation Risk Model"
    }
  ],
  "control-problem": [
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    },
    {
      "id": "nick-bostrom",
      "type": "researcher",
      "title": "Nick Bostrom"
    }
  ],
  "convergent-evolution": [
    {
      "id": "instrumental-convergence-framework",
      "type": "model",
      "title": "Instrumental Convergence Framework"
    }
  ],
  "cooperative-ai": [
    {
      "id": "stuart-russell",
      "type": "researcher",
      "title": "Stuart Russell"
    }
  ],
  "coordination": [
    {
      "id": "geopolitics",
      "type": "ai-transition-model-metric",
      "title": "Geopolitics"
    },
    {
      "id": "international-coordination",
      "type": "ai-transition-model-parameter",
      "title": "International Coordination"
    },
    {
      "id": "coordination-capacity",
      "type": "ai-transition-model-parameter",
      "title": "Coordination Capacity"
    },
    {
      "id": "pause-and-redirect",
      "type": "ai-transition-model-scenario",
      "title": "Pause and Redirect - The Deliberate Path"
    },
    {
      "id": "solutions",
      "type": "crux",
      "title": "Solution Cruxes"
    },
    {
      "id": "multipolar-trap-dynamics",
      "type": "model",
      "title": "Multipolar Trap Dynamics Model"
    },
    {
      "id": "racing-dynamics-model",
      "type": "model",
      "title": "Racing Dynamics Game Theory Model"
    },
    {
      "id": "international-compute-regimes",
      "type": "policy",
      "title": "International Compute Regimes"
    },
    {
      "id": "pause-advocacy",
      "type": "approach",
      "title": "Pause Advocacy"
    },
    {
      "id": "pause-moratorium",
      "type": "policy",
      "title": "Pause / Moratorium"
    },
    {
      "id": "multi-agent",
      "type": "approach",
      "title": "Multi-Agent Safety"
    },
    {
      "id": "multipolar-trap",
      "type": "risk",
      "title": "Multipolar Trap"
    },
    {
      "id": "racing-dynamics",
      "type": "risk",
      "title": "Racing Dynamics"
    },
    {
      "id": "trust-cascade",
      "type": "risk",
      "title": "Trust Cascade Failure"
    }
  ],
  "coordination-failure": [
    {
      "id": "multipolar-trap-model",
      "type": "model",
      "title": "Multipolar Trap Coordination Model"
    }
  ],
  "corporate-governance": [
    {
      "id": "corporate-influence",
      "type": "crux",
      "title": "Corporate Influence"
    }
  ],
  "corporate-safety": [
    {
      "id": "corporate",
      "type": "approach",
      "title": "Corporate Responses"
    }
  ],
  "corporate-structure": [
    {
      "id": "openai-foundation",
      "type": "organization",
      "title": "OpenAI Foundation"
    }
  ],
  "corrigibility": [
    {
      "id": "corrigibility-failure-pathways",
      "type": "model",
      "title": "Corrigibility Failure Pathways"
    },
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    },
    {
      "id": "stuart-russell",
      "type": "researcher",
      "title": "Stuart Russell"
    },
    {
      "id": "alignment-evals",
      "type": "approach",
      "title": "Alignment Evaluations"
    },
    {
      "id": "corrigibility-failure",
      "type": "risk",
      "title": "Corrigibility Failure"
    },
    {
      "id": "instrumental-convergence",
      "type": "risk",
      "title": "Instrumental Convergence"
    },
    {
      "id": "power-seeking",
      "type": "risk",
      "title": "Power-Seeking AI"
    },
    {
      "id": "treacherous-turn",
      "type": "risk",
      "title": "Treacherous Turn"
    }
  ],
  "cost-effectiveness": [
    {
      "id": "anthropic-pledge-enforcement",
      "type": "analysis",
      "title": "Anthropic Founder Pledges: Interventions to Increase Follow-Through"
    },
    {
      "id": "safety-research-value",
      "type": "model",
      "title": "Safety Research Value Model"
    }
  ],
  "crime": [
    {
      "id": "fraud-sophistication-curve",
      "type": "model",
      "title": "Fraud Sophistication Curve Model"
    }
  ],
  "critical-infrastructure": [
    {
      "id": "cyberweapons",
      "type": "risk",
      "title": "Cyberweapons Risk"
    },
    {
      "id": "flash-dynamics",
      "type": "risk",
      "title": "Flash Dynamics"
    }
  ],
  "cyber-security": [
    {
      "id": "palisade-research",
      "type": "lab-research",
      "title": "Palisade Research"
    }
  ],
  "cybersecurity": [
    {
      "id": "cyber-threat-exposure",
      "type": "ai-transition-model-parameter",
      "title": "Cyber Threat Exposure"
    },
    {
      "id": "misuse-risks",
      "type": "crux",
      "title": "Misuse Risk Cruxes"
    },
    {
      "id": "cyberweapons-offense-defense",
      "type": "model",
      "title": "Cyber Offense-Defense Balance Model"
    },
    {
      "id": "cyberweapons-attack-automation",
      "type": "model",
      "title": "Autonomous Cyber Attack Timeline"
    },
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    },
    {
      "id": "dangerous-cap-evals",
      "type": "approach",
      "title": "Dangerous Capability Evaluations"
    },
    {
      "id": "red-teaming",
      "type": "approach",
      "title": "Red Teaming"
    },
    {
      "id": "cyberweapons",
      "type": "risk",
      "title": "Cyberweapons Risk"
    },
    {
      "id": "cyber-offense",
      "type": "risk",
      "title": "AI-Enabled Cyberattacks"
    }
  ],
  "dangerous-capabilities": [
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    },
    {
      "id": "dangerous-cap-evals",
      "type": "approach",
      "title": "Dangerous Capability Evaluations"
    },
    {
      "id": "autonomous-replication",
      "type": "risk",
      "title": "Autonomous Replication"
    },
    {
      "id": "cyber-offense",
      "type": "risk",
      "title": "AI-Enabled Cyberattacks"
    }
  ],
  "dario-amodei": [
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    }
  ],
  "data-constraints": [
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    }
  ],
  "debate": [
    {
      "id": "is-ai-xrisk-real",
      "type": "crux",
      "title": "Is AI Existential Risk Real?"
    },
    {
      "id": "open-vs-closed",
      "type": "crux",
      "title": "Open vs Closed Source AI"
    },
    {
      "id": "pause-debate",
      "type": "crux",
      "title": "Should We Pause AI Development?"
    },
    {
      "id": "agi-timeline-debate",
      "type": "crux",
      "title": "When Will AGI Arrive?"
    },
    {
      "id": "regulation-debate",
      "type": "crux",
      "title": "Government Regulation vs Industry Self-Governance"
    },
    {
      "id": "interpretability-sufficient",
      "type": "crux",
      "title": "Is Interpretability Sufficient for Safety?"
    },
    {
      "id": "scaling-debate",
      "type": "crux",
      "title": "Is Scaling All You Need?"
    },
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    },
    {
      "id": "scalable-oversight",
      "type": "safety-agenda",
      "title": "Scalable Oversight"
    }
  ],
  "decentralization": [
    {
      "id": "open-source",
      "type": "approach",
      "title": "Open Source Safety"
    }
  ],
  "deception": [
    {
      "id": "persuasion",
      "type": "capability",
      "title": "Persuasion and Social Manipulation"
    },
    {
      "id": "situational-awareness",
      "type": "capability",
      "title": "Situational Awareness"
    },
    {
      "id": "deceptive-alignment-decomposition",
      "type": "model",
      "title": "Deceptive Alignment Decomposition Model"
    },
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    },
    {
      "id": "eliezer-yudkowsky",
      "type": "researcher",
      "title": "Eliezer Yudkowsky"
    },
    {
      "id": "jan-leike",
      "type": "researcher",
      "title": "Jan Leike"
    },
    {
      "id": "paul-christiano",
      "type": "researcher",
      "title": "Paul Christiano"
    },
    {
      "id": "evaluation-awareness",
      "type": "approach",
      "title": "Evaluation Awareness"
    },
    {
      "id": "deceptive-alignment",
      "type": "risk",
      "title": "Deceptive Alignment"
    },
    {
      "id": "mesa-optimization",
      "type": "risk",
      "title": "Mesa-Optimization"
    },
    {
      "id": "sandbagging",
      "type": "risk",
      "title": "Sandbagging"
    },
    {
      "id": "scheming",
      "type": "risk",
      "title": "Scheming"
    }
  ],
  "deception-detection": [
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    },
    {
      "id": "scheming-detection",
      "type": "approach",
      "title": "Scheming & Deception Detection"
    },
    {
      "id": "mech-interp",
      "type": "approach",
      "title": "Mechanistic Interpretability"
    },
    {
      "id": "representation-engineering",
      "type": "approach",
      "title": "Representation Engineering"
    },
    {
      "id": "eliciting-latent-knowledge",
      "type": "approach",
      "title": "Eliciting Latent Knowledge (ELK)"
    }
  ],
  "deceptive-alignment": [
    {
      "id": "accident-risks",
      "type": "crux",
      "title": "Accident Risk Cruxes"
    },
    {
      "id": "technical-pathways",
      "type": "analysis",
      "title": "Technical Pathway Decomposition"
    },
    {
      "id": "model-organisms-of-misalignment",
      "type": "analysis",
      "title": "Model Organisms of Misalignment"
    },
    {
      "id": "alignment",
      "type": "approach",
      "title": "AI Alignment"
    },
    {
      "id": "sleeper-agent-detection",
      "type": "approach",
      "title": "Sleeper Agent Detection"
    },
    {
      "id": "sleeper-agents",
      "type": "risk",
      "title": "Sleeper Agents: Training Deceptive LLMs"
    }
  ],
  "decision-making": [
    {
      "id": "irreversibility-threshold",
      "type": "model",
      "title": "Irreversibility Threshold Model"
    },
    {
      "id": "philip-tetlock",
      "type": "researcher",
      "title": "Philip Tetlock"
    },
    {
      "id": "ai-forecasting",
      "type": "approach",
      "title": "AI-Augmented Forecasting"
    },
    {
      "id": "hybrid-systems",
      "type": "approach",
      "title": "AI-Human Hybrid Systems"
    },
    {
      "id": "prediction-markets",
      "type": "approach",
      "title": "Prediction Markets"
    },
    {
      "id": "automation-bias",
      "type": "risk",
      "title": "Automation Bias"
    }
  ],
  "decision-theory": [
    {
      "id": "reasoning",
      "type": "capability",
      "title": "Reasoning and Planning"
    },
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    },
    {
      "id": "eliezer-yudkowsky",
      "type": "researcher",
      "title": "Eliezer Yudkowsky"
    }
  ],
  "decomposition": [
    {
      "id": "deceptive-alignment-decomposition",
      "type": "model",
      "title": "Deceptive Alignment Decomposition Model"
    },
    {
      "id": "carlsmith-six-premises",
      "type": "model",
      "title": "Carlsmith's Six-Premise Argument"
    },
    {
      "id": "bioweapons-attack-chain",
      "type": "model",
      "title": "Bioweapons Attack Chain Model"
    }
  ],
  "deconfusion": [
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    }
  ],
  "deep-learning": [
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    },
    {
      "id": "geoffrey-hinton",
      "type": "researcher",
      "title": "Geoffrey Hinton"
    },
    {
      "id": "ilya-sutskever",
      "type": "researcher",
      "title": "Ilya Sutskever"
    },
    {
      "id": "yoshua-bengio",
      "type": "researcher",
      "title": "Yoshua Bengio"
    }
  ],
  "deepfakes": [
    {
      "id": "misuse-risks",
      "type": "crux",
      "title": "Misuse Risk Cruxes"
    },
    {
      "id": "deepfakes-authentication-crisis",
      "type": "model",
      "title": "Deepfakes Authentication Crisis Model"
    },
    {
      "id": "authentication-collapse-timeline",
      "type": "model",
      "title": "Authentication Collapse Timeline Model"
    },
    {
      "id": "trust-erosion-dynamics",
      "type": "model",
      "title": "Trust Erosion Dynamics Model"
    },
    {
      "id": "china-ai-regulations",
      "type": "policy",
      "title": "China AI Regulatory Framework"
    },
    {
      "id": "epistemic-security",
      "type": "approach",
      "title": "Epistemic Security"
    },
    {
      "id": "content-authentication",
      "type": "approach",
      "title": "Content Authentication"
    },
    {
      "id": "authentication-collapse",
      "type": "risk",
      "title": "Authentication Collapse"
    },
    {
      "id": "cyber-psychosis",
      "type": "risk",
      "title": "Cyber Psychosis"
    },
    {
      "id": "disinformation",
      "type": "risk",
      "title": "AI Disinformation"
    },
    {
      "id": "fraud",
      "type": "risk",
      "title": "AI-Powered Fraud"
    },
    {
      "id": "historical-revisionism",
      "type": "risk",
      "title": "AI-Enabled Historical Revisionism"
    },
    {
      "id": "legal-evidence-crisis",
      "type": "risk",
      "title": "Legal Evidence Crisis"
    }
  ],
  "deepmind": [
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    }
  ],
  "defense": [
    {
      "id": "biological-threat-exposure",
      "type": "ai-transition-model-parameter",
      "title": "Biological Threat Exposure"
    },
    {
      "id": "cyber-threat-exposure",
      "type": "ai-transition-model-parameter",
      "title": "Cyber Threat Exposure"
    },
    {
      "id": "defense-in-depth-model",
      "type": "model",
      "title": "Defense in Depth Model"
    }
  ],
  "defense-in-depth": [
    {
      "id": "ai-control",
      "type": "safety-agenda",
      "title": "AI Control"
    },
    {
      "id": "circuit-breakers",
      "type": "approach",
      "title": "Circuit Breakers / Inference Interventions"
    },
    {
      "id": "sandboxing",
      "type": "approach",
      "title": "Sandboxing / Containment"
    },
    {
      "id": "tool-restrictions",
      "type": "approach",
      "title": "Tool-Use Restrictions"
    }
  ],
  "delay-detect-defend": [
    {
      "id": "ea-biosecurity-scope",
      "type": "analysis",
      "title": "Is EA Biosecurity Work Limited to Restricting LLM Biological Use?"
    }
  ],
  "delegation": [
    {
      "id": "rogue-ai-scenarios",
      "type": "risk",
      "title": "Rogue AI Scenarios"
    }
  ],
  "democracy": [
    {
      "id": "reality-coherence",
      "type": "ai-transition-model-parameter",
      "title": "Reality Coherence"
    },
    {
      "id": "disinformation-electoral-impact",
      "type": "model",
      "title": "Electoral Impact Assessment Model"
    },
    {
      "id": "disinformation",
      "type": "risk",
      "title": "AI Disinformation"
    },
    {
      "id": "epistemic-collapse",
      "type": "risk",
      "title": "Epistemic Collapse"
    },
    {
      "id": "trust-decline",
      "type": "risk",
      "title": "Trust Decline"
    }
  ],
  "democratic-backsliding": [
    {
      "id": "trust-cascade",
      "type": "risk",
      "title": "Trust Cascade Failure"
    }
  ],
  "democratic-decay": [
    {
      "id": "learned-helplessness",
      "type": "risk",
      "title": "Epistemic Learned Helplessness"
    }
  ],
  "democratic-innovation": [
    {
      "id": "deliberation",
      "type": "approach",
      "title": "AI-Assisted Deliberation"
    }
  ],
  "democratic-process": [
    {
      "id": "consensus-manufacturing",
      "type": "risk",
      "title": "Consensus Manufacturing"
    }
  ],
  "denial": [
    {
      "id": "historical-revisionism",
      "type": "risk",
      "title": "AI-Enabled Historical Revisionism"
    }
  ],
  "dependence": [
    {
      "id": "enfeeblement",
      "type": "risk",
      "title": "Enfeeblement"
    }
  ],
  "dependency": [
    {
      "id": "expertise-atrophy-progression",
      "type": "model",
      "title": "Expertise Atrophy Progression Model"
    }
  ],
  "deployment": [
    {
      "id": "ai-uses",
      "type": "ai-transition-model-factor",
      "title": "AI Uses"
    },
    {
      "id": "content-moderation",
      "type": "concept",
      "title": "Content Moderation"
    },
    {
      "id": "distributional-shift",
      "type": "risk",
      "title": "Distributional Shift"
    }
  ],
  "deployment-decisions": [
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    },
    {
      "id": "dangerous-cap-evals",
      "type": "approach",
      "title": "Dangerous Capability Evaluations"
    },
    {
      "id": "safety-cases",
      "type": "approach",
      "title": "AI Safety Cases"
    },
    {
      "id": "evaluation",
      "type": "approach",
      "title": "AI Evaluation"
    }
  ],
  "deployment-gates": [
    {
      "id": "evals-governance",
      "type": "policy",
      "title": "Evals-Based Deployment Gates"
    }
  ],
  "deployment-oversight": [
    {
      "id": "model-auditing",
      "type": "approach",
      "title": "Third-Party Model Auditing"
    }
  ],
  "deployment-safety": [
    {
      "id": "goal-misgeneralization-probability",
      "type": "model",
      "title": "Goal Misgeneralization Probability Model"
    },
    {
      "id": "sandboxing",
      "type": "approach",
      "title": "Sandboxing / Containment"
    },
    {
      "id": "structured-access",
      "type": "approach",
      "title": "Structured Access / API-Only"
    },
    {
      "id": "tool-restrictions",
      "type": "approach",
      "title": "Tool-Use Restrictions"
    }
  ],
  "detection": [
    {
      "id": "disinformation-detection-race",
      "type": "model",
      "title": "Disinformation Detection Arms Race Model"
    }
  ],
  "detection-arms-race": [
    {
      "id": "epistemic-risks",
      "type": "crux",
      "title": "Epistemic Cruxes"
    }
  ],
  "development-moratorium": [
    {
      "id": "pause",
      "type": "approach",
      "title": "Pause Advocacy"
    }
  ],
  "development-pause": [
    {
      "id": "pause-moratorium",
      "type": "policy",
      "title": "Pause / Moratorium"
    }
  ],
  "devin": [
    {
      "id": "coding",
      "type": "capability",
      "title": "Autonomous Coding"
    }
  ],
  "diffusion": [
    {
      "id": "proliferation-risk-model",
      "type": "model",
      "title": "AI Proliferation Risk Model"
    },
    {
      "id": "autonomous-weapons-proliferation",
      "type": "model",
      "title": "LAWS Proliferation Model"
    },
    {
      "id": "proliferation-model",
      "type": "model",
      "title": "AI Capability Proliferation Model"
    },
    {
      "id": "authoritarian-tools-diffusion",
      "type": "model",
      "title": "Authoritarian Tools Diffusion Model"
    },
    {
      "id": "proliferation",
      "type": "risk",
      "title": "AI Proliferation"
    }
  ],
  "digital-evidence": [
    {
      "id": "content-authentication",
      "type": "approach",
      "title": "Content Authentication"
    },
    {
      "id": "legal-evidence-crisis",
      "type": "risk",
      "title": "Legal Evidence Crisis"
    }
  ],
  "digital-forensics": [
    {
      "id": "authentication-collapse",
      "type": "risk",
      "title": "Authentication Collapse"
    }
  ],
  "digital-manipulation": [
    {
      "id": "preference-manipulation",
      "type": "risk",
      "title": "Preference Manipulation"
    }
  ],
  "digital-minds": [
    {
      "id": "ai-welfare",
      "type": "concept",
      "title": "AI Welfare and Digital Minds"
    }
  ],
  "digital-repression": [
    {
      "id": "authoritarian-tools",
      "type": "risk",
      "title": "AI Authoritarian Tools"
    }
  ],
  "digital-rights": [
    {
      "id": "erosion-of-agency",
      "type": "risk",
      "title": "Erosion of Human Agency"
    },
    {
      "id": "surveillance",
      "type": "risk",
      "title": "AI Mass Surveillance"
    }
  ],
  "digital-trust": [
    {
      "id": "deepfakes",
      "type": "risk",
      "title": "Deepfakes"
    }
  ],
  "digital-wellbeing": [
    {
      "id": "cyber-psychosis",
      "type": "risk",
      "title": "Cyber Psychosis"
    }
  ],
  "diplomacy": [
    {
      "id": "coordination-mechanisms",
      "type": "policy",
      "title": "International Coordination Mechanisms"
    }
  ],
  "diplomatic-cooperation": [
    {
      "id": "bletchley-declaration",
      "type": "policy",
      "title": "Bletchley Declaration"
    }
  ],
  "discontinuous-progress": [
    {
      "id": "sharp-left-turn",
      "type": "risk",
      "title": "Sharp Left Turn"
    }
  ],
  "disinformation": [
    {
      "id": "persuasion",
      "type": "capability",
      "title": "Persuasion and Social Manipulation"
    },
    {
      "id": "epistemic-risks",
      "type": "crux",
      "title": "Epistemic Cruxes"
    },
    {
      "id": "disinformation-detection-race",
      "type": "model",
      "title": "Disinformation Detection Arms Race Model"
    },
    {
      "id": "disinformation-electoral-impact",
      "type": "model",
      "title": "Electoral Impact Assessment Model"
    },
    {
      "id": "consensus-manufacturing-dynamics",
      "type": "model",
      "title": "Consensus Manufacturing Dynamics Model"
    },
    {
      "id": "epistemic-security",
      "type": "approach",
      "title": "Epistemic Security"
    },
    {
      "id": "consensus-manufacturing",
      "type": "risk",
      "title": "Consensus Manufacturing"
    },
    {
      "id": "cyber-psychosis",
      "type": "risk",
      "title": "Cyber Psychosis"
    },
    {
      "id": "disinformation",
      "type": "risk",
      "title": "AI Disinformation"
    },
    {
      "id": "epistemic-collapse",
      "type": "risk",
      "title": "Epistemic Collapse"
    },
    {
      "id": "reality-fragmentation",
      "type": "risk",
      "title": "Reality Fragmentation"
    }
  ],
  "disruption": [
    {
      "id": "transition-turbulence",
      "type": "ai-transition-model-factor",
      "title": "Transition Turbulence"
    }
  ],
  "distribution-shift": [
    {
      "id": "goal-misgeneralization-probability",
      "type": "model",
      "title": "Goal Misgeneralization Probability Model"
    },
    {
      "id": "goal-misgeneralization",
      "type": "risk",
      "title": "Goal Misgeneralization"
    }
  ],
  "dna-synthesis-screening": [
    {
      "id": "ea-biosecurity-scope",
      "type": "analysis",
      "title": "Is EA Biosecurity Work Limited to Restricting LLM Biological Use?"
    }
  ],
  "donor-advised-funds": [
    {
      "id": "anthropic-investors",
      "type": "analysis",
      "title": "Anthropic (Funder)"
    },
    {
      "id": "anthropic-pledge-enforcement",
      "type": "analysis",
      "title": "Anthropic Founder Pledges: Interventions to Increase Follow-Through"
    },
    {
      "id": "anthropic-pre-ipo-daf-transfers",
      "type": "analysis",
      "title": "Anthropic Pre-IPO DAF Transfers"
    }
  ],
  "dpo": [
    {
      "id": "preference-optimization",
      "type": "approach",
      "title": "Preference Optimization Methods"
    }
  ],
  "drug-discovery": [
    {
      "id": "scientific-research",
      "type": "capability",
      "title": "Scientific Research Capabilities"
    }
  ],
  "dual-use": [
    {
      "id": "dual-use",
      "type": "concept",
      "title": "Dual-Use Technology"
    },
    {
      "id": "proliferation-risk-model",
      "type": "model",
      "title": "AI Proliferation Risk Model"
    },
    {
      "id": "proliferation",
      "type": "risk",
      "title": "AI Proliferation"
    }
  ],
  "dual-use-research": [
    {
      "id": "bioweapons",
      "type": "risk",
      "title": "Bioweapons Risk"
    }
  ],
  "dual-use-technology": [
    {
      "id": "scientific-research",
      "type": "capability",
      "title": "Scientific Research Capabilities"
    }
  ],
  "ea-capital": [
    {
      "id": "anthropic-investors",
      "type": "analysis",
      "title": "Anthropic (Funder)"
    }
  ],
  "ea-funding-implications": [
    {
      "id": "anthropic-ipo",
      "type": "analysis",
      "title": "Anthropic IPO"
    }
  ],
  "ea-portfolio": [
    {
      "id": "ea-biosecurity-scope",
      "type": "analysis",
      "title": "Is EA Biosecurity Work Limited to Restricting LLM Biological Use?"
    }
  ],
  "early-warning": [
    {
      "id": "warning-signs-model",
      "type": "model",
      "title": "Warning Signs Model"
    }
  ],
  "echo-chambers": [
    {
      "id": "sycophancy-feedback-loop",
      "type": "model",
      "title": "Sycophancy Feedback Loop Model"
    },
    {
      "id": "epistemic-sycophancy",
      "type": "risk",
      "title": "Epistemic Sycophancy"
    }
  ],
  "economic": [
    {
      "id": "economic-stability",
      "type": "ai-transition-model-parameter",
      "title": "Economic Stability"
    }
  ],
  "economic-inequality": [
    {
      "id": "winner-take-all",
      "type": "risk",
      "title": "Winner-Take-All Dynamics"
    }
  ],
  "economic-policy": [
    {
      "id": "economic-disruption",
      "type": "risk",
      "title": "Economic Disruption"
    }
  ],
  "economics": [
    {
      "id": "economic-labor",
      "type": "ai-transition-model-metric",
      "title": "Economic & Labor"
    },
    {
      "id": "economic-disruption-impact",
      "type": "model",
      "title": "Economic Disruption Impact Model"
    }
  ],
  "effective-altruism": [
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    },
    {
      "id": "centre-for-long-term-resilience",
      "type": "organization",
      "title": "Centre for Long-Term Resilience"
    },
    {
      "id": "holden-karnofsky",
      "type": "researcher",
      "title": "Holden Karnofsky"
    },
    {
      "id": "toby-ord",
      "type": "researcher",
      "title": "Toby Ord"
    },
    {
      "id": "dustin-moskovitz",
      "type": "researcher",
      "title": "Dustin Moskovitz"
    }
  ],
  "effectiveness": [
    {
      "id": "intervention-effectiveness-matrix",
      "type": "model",
      "title": "Intervention Effectiveness Matrix"
    }
  ],
  "elections": [
    {
      "id": "disinformation-electoral-impact",
      "type": "model",
      "title": "Electoral Impact Assessment Model"
    },
    {
      "id": "leading-the-future",
      "type": "organization",
      "title": "Leading the Future super PAC"
    }
  ],
  "eleutherai": [
    {
      "id": "conjecture",
      "type": "lab-research",
      "title": "Conjecture"
    },
    {
      "id": "connor-leahy",
      "type": "researcher",
      "title": "Connor Leahy"
    }
  ],
  "elicitation": [
    {
      "id": "capability-elicitation",
      "type": "approach",
      "title": "Capability Elicitation"
    }
  ],
  "eliciting-latent-knowledge": [
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    }
  ],
  "eliezer-yudkowsky": [
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    }
  ],
  "elk": [
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    },
    {
      "id": "paul-christiano",
      "type": "researcher",
      "title": "Paul Christiano"
    }
  ],
  "elon-musk": [
    {
      "id": "elon-musk-philanthropy",
      "type": "analysis",
      "title": "Elon Musk (Funder)"
    },
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    }
  ],
  "embedded-agency": [
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    }
  ],
  "emergency-coordination": [
    {
      "id": "short-timeline-policy-implications",
      "type": "analysis",
      "title": "Short Timeline Policy Implications"
    }
  ],
  "emergent-capabilities": [
    {
      "id": "language-models",
      "type": "capability",
      "title": "Large Language Models"
    },
    {
      "id": "large-language-models",
      "type": "concept",
      "title": "Large Language Models"
    },
    {
      "id": "dense-transformers",
      "type": "concept",
      "title": "Dense Transformers"
    }
  ],
  "empirical-alignment": [
    {
      "id": "dario-amodei",
      "type": "researcher",
      "title": "Dario Amodei"
    }
  ],
  "empirical-analysis": [
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    }
  ],
  "empirical-research": [
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    }
  ],
  "employee-matching": [
    {
      "id": "anthropic-pre-ipo-daf-transfers",
      "type": "analysis",
      "title": "Anthropic Pre-IPO DAF Transfers"
    }
  ],
  "employee-protections": [
    {
      "id": "whistleblower-protections",
      "type": "policy",
      "title": "AI Whistleblower Protections"
    }
  ],
  "enterprise-ai": [
    {
      "id": "structured-access",
      "type": "approach",
      "title": "Structured Access / API-Only"
    }
  ],
  "enterprise-metrics": [
    {
      "id": "anthropic-valuation",
      "type": "analysis",
      "title": "Anthropic Valuation Analysis"
    }
  ],
  "entrepreneur": [
    {
      "id": "elon-musk",
      "type": "researcher",
      "title": "Elon Musk"
    }
  ],
  "epistemic": [
    {
      "id": "societal-trust",
      "type": "ai-transition-model-parameter",
      "title": "Societal Trust"
    },
    {
      "id": "epistemic-health",
      "type": "ai-transition-model-parameter",
      "title": "Epistemic Health"
    },
    {
      "id": "information-authenticity",
      "type": "ai-transition-model-parameter",
      "title": "Information Authenticity"
    },
    {
      "id": "human-expertise",
      "type": "ai-transition-model-parameter",
      "title": "Human Expertise"
    },
    {
      "id": "reality-coherence",
      "type": "ai-transition-model-parameter",
      "title": "Reality Coherence"
    },
    {
      "id": "preference-authenticity",
      "type": "ai-transition-model-parameter",
      "title": "Preference Authenticity"
    },
    {
      "id": "trust-cascade-model",
      "type": "model",
      "title": "Trust Cascade Failure Model"
    },
    {
      "id": "sycophancy-feedback-loop",
      "type": "model",
      "title": "Sycophancy Feedback Loop Model"
    },
    {
      "id": "authentication-collapse-timeline",
      "type": "model",
      "title": "Authentication Collapse Timeline Model"
    },
    {
      "id": "expertise-atrophy-cascade",
      "type": "model",
      "title": "Expertise Atrophy Cascade Model"
    },
    {
      "id": "epistemic-collapse-threshold",
      "type": "model",
      "title": "Epistemic Collapse Threshold Model"
    },
    {
      "id": "reality-fragmentation-network",
      "type": "model",
      "title": "Reality Fragmentation Network Model"
    }
  ],
  "epistemic-infrastructure": [
    {
      "id": "solutions",
      "type": "crux",
      "title": "Solution Cruxes"
    }
  ],
  "epistemic-integrity": [
    {
      "id": "epistemic-sycophancy",
      "type": "risk",
      "title": "Epistemic Sycophancy"
    }
  ],
  "epistemic-risks": [
    {
      "id": "epistemic-risks",
      "type": "crux",
      "title": "Epistemic Cruxes"
    }
  ],
  "epistemics": [
    {
      "id": "reasoning",
      "type": "capability",
      "title": "Reasoning and Planning"
    },
    {
      "id": "learned-helplessness",
      "type": "risk",
      "title": "Epistemic Learned Helplessness"
    }
  ],
  "epistemology": [
    {
      "id": "epistemic-collapse",
      "type": "risk",
      "title": "Epistemic Collapse"
    }
  ],
  "equilibrium": [
    {
      "id": "multipolar-trap-dynamics",
      "type": "model",
      "title": "Multipolar Trap Dynamics Model"
    },
    {
      "id": "safety-culture-equilibrium",
      "type": "model",
      "title": "Safety Culture Equilibrium Model"
    }
  ],
  "escalation": [
    {
      "id": "autonomous-weapons-escalation",
      "type": "model",
      "title": "Autonomous Weapons Escalation Model"
    }
  ],
  "eu-ai-act": [
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    },
    {
      "id": "evals-governance",
      "type": "policy",
      "title": "Evals-Based Deployment Gates"
    },
    {
      "id": "thresholds",
      "type": "policy",
      "title": "Compute Thresholds"
    }
  ],
  "european-ai-safety": [
    {
      "id": "conjecture",
      "type": "lab-research",
      "title": "Conjecture"
    }
  ],
  "evaluation": [
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    },
    {
      "id": "evaluation",
      "type": "approach",
      "title": "AI Evaluation"
    }
  ],
  "evaluation-gaming": [
    {
      "id": "evaluation-awareness",
      "type": "approach",
      "title": "Evaluation Awareness"
    }
  ],
  "evaluation-gap": [
    {
      "id": "eval-saturation",
      "type": "approach",
      "title": "Eval Saturation & The Evals Gap"
    }
  ],
  "evaluations": [
    {
      "id": "situational-awareness",
      "type": "capability",
      "title": "Situational Awareness"
    },
    {
      "id": "technical-research",
      "type": "crux",
      "title": "Technical AI Safety Research"
    },
    {
      "id": "capability-evaluations",
      "type": "concept",
      "title": "Capability Evaluations"
    },
    {
      "id": "benchmarking",
      "type": "concept",
      "title": "AI Benchmarking"
    },
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    },
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    },
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    },
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    },
    {
      "id": "arc-evals",
      "type": "organization",
      "title": "ARC Evaluations"
    },
    {
      "id": "beth-barnes",
      "type": "researcher",
      "title": "Beth Barnes"
    },
    {
      "id": "evals-governance",
      "type": "policy",
      "title": "Evals-Based Deployment Gates"
    },
    {
      "id": "sandbagging",
      "type": "risk",
      "title": "Sandbagging"
    }
  ],
  "executive-policy": [
    {
      "id": "us-executive-order",
      "type": "policy",
      "title": "Executive Order on Safe, Secure, and Trustworthy AI"
    }
  ],
  "existential-risk": [
    {
      "id": "is-ai-xrisk-real",
      "type": "crux",
      "title": "Is AI Existential Risk Real?"
    },
    {
      "id": "case-against-xrisk",
      "type": "argument",
      "title": "The Case Against AI Existential Risk"
    },
    {
      "id": "case-for-xrisk",
      "type": "argument",
      "title": "The Case For AI Existential Risk"
    },
    {
      "id": "carlsmith-six-premises",
      "type": "model",
      "title": "Carlsmith's Six-Premise Argument"
    },
    {
      "id": "fhi",
      "type": "organization",
      "title": "Future of Humanity Institute"
    }
  ],
  "expected-value": [
    {
      "id": "safety-research-value",
      "type": "model",
      "title": "Safety Research Value Model"
    }
  ],
  "expert-surveys": [
    {
      "id": "agi-timeline",
      "type": "concept",
      "title": "AGI Timeline"
    }
  ],
  "expertise": [
    {
      "id": "expertise-atrophy-cascade",
      "type": "model",
      "title": "Expertise Atrophy Cascade Model"
    }
  ],
  "experts": [
    {
      "id": "expert-opinion",
      "type": "ai-transition-model-metric",
      "title": "Expert Opinion"
    }
  ],
  "export-controls": [
    {
      "id": "governance-policy",
      "type": "crux",
      "title": "AI Governance and Policy"
    },
    {
      "id": "compute-governance",
      "type": "policy",
      "title": "Compute Governance"
    },
    {
      "id": "hardware-enabled-governance",
      "type": "policy",
      "title": "Hardware-Enabled Governance"
    }
  ],
  "extreme-risks": [
    {
      "id": "centre-for-long-term-resilience",
      "type": "organization",
      "title": "Centre for Long-Term Resilience"
    }
  ],
  "facebook": [
    {
      "id": "dustin-moskovitz",
      "type": "researcher",
      "title": "Dustin Moskovitz"
    }
  ],
  "facial-recognition": [
    {
      "id": "surveillance",
      "type": "risk",
      "title": "AI Mass Surveillance"
    }
  ],
  "factor": [
    {
      "id": "misalignment-potential",
      "type": "ai-transition-model-factor",
      "title": "Misalignment Potential"
    },
    {
      "id": "misuse-potential",
      "type": "ai-transition-model-factor",
      "title": "Misuse Potential"
    },
    {
      "id": "ai-capabilities",
      "type": "ai-transition-model-factor",
      "title": "AI Capabilities"
    },
    {
      "id": "ai-uses",
      "type": "ai-transition-model-factor",
      "title": "AI Uses"
    },
    {
      "id": "ai-ownership",
      "type": "ai-transition-model-factor",
      "title": "AI Ownership"
    },
    {
      "id": "civilizational-competence",
      "type": "ai-transition-model-factor",
      "title": "Civilizational Competence"
    },
    {
      "id": "transition-turbulence",
      "type": "ai-transition-model-factor",
      "title": "Transition Turbulence"
    }
  ],
  "feature-extraction": [
    {
      "id": "mech-interp",
      "type": "approach",
      "title": "Mechanistic Interpretability"
    },
    {
      "id": "sparse-autoencoders",
      "type": "approach",
      "title": "Sparse Autoencoders (SAEs)"
    }
  ],
  "feature-steering": [
    {
      "id": "goodfire",
      "type": "lab-research",
      "title": "Goodfire"
    }
  ],
  "feature-visualization": [
    {
      "id": "chris-olah",
      "type": "researcher",
      "title": "Chris Olah"
    }
  ],
  "features": [
    {
      "id": "interpretability",
      "type": "safety-agenda",
      "title": "Interpretability"
    }
  ],
  "feedback-loops": [
    {
      "id": "feedback-loops",
      "type": "analysis",
      "title": "Feedback Loop & Cascade Model"
    },
    {
      "id": "sycophancy-feedback-loop",
      "type": "model",
      "title": "Sycophancy Feedback Loop Model"
    },
    {
      "id": "concentration-of-power-model",
      "type": "model",
      "title": "Concentration of Power Systems Model"
    },
    {
      "id": "media-policy-feedback-loop",
      "type": "model",
      "title": "Media-Policy Feedback Loop Model"
    },
    {
      "id": "parameter-interaction-network",
      "type": "model",
      "title": "Parameter Interaction Network Model"
    }
  ],
  "field-analysis": [
    {
      "id": "intervention-portfolio",
      "type": "approach",
      "title": "Intervention Portfolio"
    }
  ],
  "field-building": [
    {
      "id": "field-building",
      "type": "crux",
      "title": "Field Building and Community"
    },
    {
      "id": "capabilities-to-safety-pipeline",
      "type": "model",
      "title": "Capabilities-to-Safety Pipeline Model"
    },
    {
      "id": "safety-researcher-gap",
      "type": "model",
      "title": "Safety Researcher Gap Model"
    },
    {
      "id": "cais",
      "type": "lab-research",
      "title": "CAIS"
    },
    {
      "id": "training-programs",
      "type": "approach",
      "title": "AI Safety Training Programs"
    },
    {
      "id": "field-building-analysis",
      "type": "approach",
      "title": "Field Building Analysis"
    }
  ],
  "filter-bubbles": [
    {
      "id": "reality-fragmentation",
      "type": "risk",
      "title": "Reality Fragmentation"
    }
  ],
  "financial-crime": [
    {
      "id": "fraud",
      "type": "risk",
      "title": "AI-Powered Fraud"
    }
  ],
  "financial-stability": [
    {
      "id": "flash-dynamics",
      "type": "risk",
      "title": "Flash Dynamics"
    }
  ],
  "flop-thresholds": [
    {
      "id": "compute-thresholds",
      "type": "policy",
      "title": "Compute Thresholds"
    },
    {
      "id": "thresholds",
      "type": "policy",
      "title": "Compute Thresholds"
    }
  ],
  "flourishing": [
    {
      "id": "long-term-trajectory",
      "type": "ai-transition-model-scenario",
      "title": "Long-term Trajectory"
    }
  ],
  "forecasting": [
    {
      "id": "scaling-laws",
      "type": "concept",
      "title": "Scaling Laws"
    },
    {
      "id": "ai-timelines",
      "type": "concept",
      "title": "AI Timelines"
    },
    {
      "id": "critical-uncertainties",
      "type": "crux",
      "title": "Critical Uncertainties Model"
    },
    {
      "id": "agi-timeline",
      "type": "concept",
      "title": "AGI Timeline"
    },
    {
      "id": "bioweapons-timeline",
      "type": "model",
      "title": "AI-Bioweapons Timeline Model"
    },
    {
      "id": "risk-activation-timeline",
      "type": "model",
      "title": "Risk Activation Timeline Model"
    },
    {
      "id": "capability-threshold-model",
      "type": "model",
      "title": "Capability Threshold Model"
    },
    {
      "id": "philip-tetlock",
      "type": "researcher",
      "title": "Philip Tetlock"
    },
    {
      "id": "eli-lifland",
      "type": "researcher",
      "title": "Eli Lifland"
    },
    {
      "id": "ai-forecasting",
      "type": "approach",
      "title": "AI-Augmented Forecasting"
    },
    {
      "id": "prediction-markets",
      "type": "approach",
      "title": "Prediction Markets"
    }
  ],
  "forecasts": [
    {
      "id": "expert-opinion",
      "type": "ai-transition-model-metric",
      "title": "Expert Opinion"
    }
  ],
  "formal-analysis": [
    {
      "id": "power-seeking-conditions",
      "type": "model",
      "title": "Power-Seeking Emergence Conditions Model"
    }
  ],
  "formal-methods": [
    {
      "id": "formal-verification",
      "type": "approach",
      "title": "Formal Verification"
    },
    {
      "id": "provably-safe",
      "type": "approach",
      "title": "Provably Safe AI (davidad agenda)"
    }
  ],
  "formal-verification": [
    {
      "id": "provable-safe",
      "type": "concept",
      "title": "Provable / Guaranteed Safe AI"
    }
  ],
  "foundation-analysis": [
    {
      "id": "elon-musk-philanthropy",
      "type": "analysis",
      "title": "Elon Musk (Funder)"
    }
  ],
  "foundation-models": [
    {
      "id": "language-models",
      "type": "capability",
      "title": "Large Language Models"
    },
    {
      "id": "eu-ai-act",
      "type": "policy",
      "title": "EU AI Act"
    }
  ],
  "founder-pledges": [
    {
      "id": "anthropic-investors",
      "type": "analysis",
      "title": "Anthropic (Funder)"
    },
    {
      "id": "anthropic-pledge-enforcement",
      "type": "analysis",
      "title": "Anthropic Founder Pledges: Interventions to Increase Follow-Through"
    }
  ],
  "fragmentation": [
    {
      "id": "reality-fragmentation-network",
      "type": "model",
      "title": "Reality Fragmentation Network Model"
    }
  ],
  "framework": [
    {
      "id": "instrumental-convergence-framework",
      "type": "model",
      "title": "Instrumental Convergence Framework"
    }
  ],
  "fraud": [
    {
      "id": "fraud-sophistication-curve",
      "type": "model",
      "title": "Fraud Sophistication Curve Model"
    }
  ],
  "free-speech": [
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    }
  ],
  "freedom": [
    {
      "id": "surveillance-chilling-effects",
      "type": "model",
      "title": "Surveillance Chilling Effects Model"
    }
  ],
  "friendly-ai": [
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    }
  ],
  "frontier-ai": [
    {
      "id": "anthropic",
      "type": "lab",
      "title": "Anthropic"
    },
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    },
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    },
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    },
    {
      "id": "ssi",
      "type": "lab-research",
      "title": "Safe Superintelligence Inc (SSI)"
    },
    {
      "id": "international-summits",
      "type": "policy",
      "title": "International AI Safety Summit Series"
    }
  ],
  "frontier-ai-safety": [
    {
      "id": "bletchley-declaration",
      "type": "policy",
      "title": "Bletchley Declaration"
    }
  ],
  "frontier-labs": [
    {
      "id": "corporate-influence",
      "type": "crux",
      "title": "Corporate Influence"
    },
    {
      "id": "rsp",
      "type": "policy",
      "title": "Responsible Scaling Policies"
    }
  ],
  "frontier-model-evaluation": [
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    }
  ],
  "frontier-models": [
    {
      "id": "large-language-models",
      "type": "concept",
      "title": "Large Language Models"
    },
    {
      "id": "frontier-model-forum",
      "type": "organization",
      "title": "Frontier Model Forum"
    },
    {
      "id": "california-sb1047",
      "type": "policy",
      "title": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act"
    },
    {
      "id": "california-sb53",
      "type": "policy",
      "title": "California SB 53"
    },
    {
      "id": "new-york-raise-act",
      "type": "policy",
      "title": "New York RAISE Act"
    }
  ],
  "frontier-safety-framework": [
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    }
  ],
  "function-calling": [
    {
      "id": "tool-use",
      "type": "capability",
      "title": "Tool Use and Computer Use"
    }
  ],
  "fundamental": [
    {
      "id": "is-ai-xrisk-real",
      "type": "crux",
      "title": "Is AI Existential Risk Real?"
    }
  ],
  "funding": [
    {
      "id": "safety-research",
      "type": "ai-transition-model-metric",
      "title": "Safety Research"
    },
    {
      "id": "field-building",
      "type": "crux",
      "title": "Field Building and Community"
    },
    {
      "id": "intervention-portfolio",
      "type": "approach",
      "title": "Intervention Portfolio"
    }
  ],
  "funding-analysis": [
    {
      "id": "field-building-analysis",
      "type": "approach",
      "title": "Field Building Analysis"
    }
  ],
  "future-generations": [
    {
      "id": "toby-ord",
      "type": "researcher",
      "title": "Toby Ord"
    }
  ],
  "future-of-life-institute": [
    {
      "id": "max-tegmark",
      "type": "researcher",
      "title": "Max Tegmark"
    }
  ],
  "game-theory": [
    {
      "id": "racing-dynamics-impact",
      "type": "model",
      "title": "Racing Dynamics Impact Model"
    },
    {
      "id": "multipolar-trap-dynamics",
      "type": "model",
      "title": "Multipolar Trap Dynamics Model"
    },
    {
      "id": "racing-dynamics-model",
      "type": "model",
      "title": "Racing Dynamics Game Theory Model"
    },
    {
      "id": "international-coordination-game",
      "type": "model",
      "title": "International Coordination Game Model"
    },
    {
      "id": "safety-culture-equilibrium",
      "type": "model",
      "title": "Safety Culture Equilibrium Model"
    },
    {
      "id": "coordination-tech",
      "type": "approach",
      "title": "Coordination Technologies"
    },
    {
      "id": "multi-agent",
      "type": "approach",
      "title": "Multi-Agent Safety"
    },
    {
      "id": "multipolar-trap",
      "type": "risk",
      "title": "Multipolar Trap"
    }
  ],
  "gemini": [
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    }
  ],
  "generalization": [
    {
      "id": "goal-misgeneralization-probability",
      "type": "model",
      "title": "Goal Misgeneralization Probability Model"
    },
    {
      "id": "distributional-shift",
      "type": "risk",
      "title": "Distributional Shift"
    }
  ],
  "generational": [
    {
      "id": "expertise-atrophy-cascade",
      "type": "model",
      "title": "Expertise Atrophy Cascade Model"
    }
  ],
  "generative-ai": [
    {
      "id": "china-ai-regulations",
      "type": "policy",
      "title": "China AI Regulatory Framework"
    }
  ],
  "geoffrey-hinton": [
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    }
  ],
  "geopolitics": [
    {
      "id": "geopolitics",
      "type": "ai-transition-model-metric",
      "title": "Geopolitics"
    },
    {
      "id": "multi-actor-landscape",
      "type": "analysis",
      "title": "Multi-Actor Strategic Landscape"
    },
    {
      "id": "authoritarian-tools-diffusion",
      "type": "model",
      "title": "Authoritarian Tools Diffusion Model"
    },
    {
      "id": "china-ai-regulations",
      "type": "policy",
      "title": "China AI Regulatory Framework"
    },
    {
      "id": "coordination-mechanisms",
      "type": "policy",
      "title": "International Coordination Mechanisms"
    }
  ],
  "github-copilot": [
    {
      "id": "coding",
      "type": "capability",
      "title": "Autonomous Coding"
    }
  ],
  "giving-pledge": [
    {
      "id": "elon-musk-philanthropy",
      "type": "analysis",
      "title": "Elon Musk (Funder)"
    },
    {
      "id": "dustin-moskovitz",
      "type": "researcher",
      "title": "Dustin Moskovitz"
    }
  ],
  "global-ai-safety": [
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    }
  ],
  "goal-misgeneralization": [
    {
      "id": "technical-pathways",
      "type": "analysis",
      "title": "Technical Pathway Decomposition"
    }
  ],
  "goal-stability": [
    {
      "id": "long-horizon",
      "type": "capability",
      "title": "Long-Horizon Autonomous Tasks"
    },
    {
      "id": "instrumental-convergence",
      "type": "risk",
      "title": "Instrumental Convergence"
    }
  ],
  "goodharts-law": [
    {
      "id": "reward-hacking",
      "type": "risk",
      "title": "Reward Hacking"
    }
  ],
  "google": [
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    }
  ],
  "governance": [
    {
      "id": "civilizational-competence",
      "type": "ai-transition-model-factor",
      "title": "Civilizational Competence"
    },
    {
      "id": "lab-behavior",
      "type": "ai-transition-model-metric",
      "title": "Lab Behavior"
    },
    {
      "id": "international-coordination",
      "type": "ai-transition-model-parameter",
      "title": "International Coordination"
    },
    {
      "id": "societal-trust",
      "type": "ai-transition-model-parameter",
      "title": "Societal Trust"
    },
    {
      "id": "ai-control-concentration",
      "type": "ai-transition-model-parameter",
      "title": "AI Control Concentration"
    },
    {
      "id": "human-oversight-quality",
      "type": "ai-transition-model-parameter",
      "title": "Human Oversight Quality"
    },
    {
      "id": "safety-capability-gap",
      "type": "ai-transition-model-parameter",
      "title": "Safety-Capability Gap"
    },
    {
      "id": "regulatory-capacity",
      "type": "ai-transition-model-parameter",
      "title": "Regulatory Capacity"
    },
    {
      "id": "institutional-quality",
      "type": "ai-transition-model-parameter",
      "title": "Institutional Quality"
    },
    {
      "id": "racing-intensity",
      "type": "ai-transition-model-parameter",
      "title": "Racing Intensity"
    },
    {
      "id": "safety-culture-strength",
      "type": "ai-transition-model-parameter",
      "title": "Safety Culture Strength"
    },
    {
      "id": "coordination-capacity",
      "type": "ai-transition-model-parameter",
      "title": "Coordination Capacity"
    },
    {
      "id": "tmc-compute",
      "type": "ai-transition-model-subitem",
      "title": "Compute"
    },
    {
      "id": "dual-use",
      "type": "concept",
      "title": "Dual-Use Technology"
    },
    {
      "id": "agi-race",
      "type": "concept",
      "title": "AGI Race"
    },
    {
      "id": "open-vs-closed",
      "type": "crux",
      "title": "Open vs Closed Source AI"
    },
    {
      "id": "pause-debate",
      "type": "crux",
      "title": "Should We Pause AI Development?"
    },
    {
      "id": "regulation-debate",
      "type": "crux",
      "title": "Government Regulation vs Industry Self-Governance"
    },
    {
      "id": "governance-focused",
      "type": "concept",
      "title": "Governance-Focused Worldview"
    },
    {
      "id": "ai-safety-summit",
      "type": "historical",
      "title": "AI Safety Summit (Bletchley Park)"
    },
    {
      "id": "openai-foundation-governance",
      "type": "analysis",
      "title": "OpenAI Foundation Governance Paradox"
    },
    {
      "id": "long-term-benefit-trust",
      "type": "analysis",
      "title": "Long-Term Benefit Trust (Anthropic)"
    },
    {
      "id": "international-coordination-game",
      "type": "model",
      "title": "International Coordination Game Model"
    },
    {
      "id": "regulatory-capacity-threshold",
      "type": "model",
      "title": "Regulatory Capacity Threshold Model"
    },
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    },
    {
      "id": "govai",
      "type": "lab-research",
      "title": "GovAI"
    },
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    },
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    },
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    },
    {
      "id": "dan-hendrycks",
      "type": "researcher",
      "title": "Dan Hendrycks"
    },
    {
      "id": "stuart-russell",
      "type": "researcher",
      "title": "Stuart Russell"
    },
    {
      "id": "yoshua-bengio",
      "type": "researcher",
      "title": "Yoshua Bengio"
    },
    {
      "id": "international-summits",
      "type": "policy",
      "title": "International AI Safety Summit Series"
    },
    {
      "id": "us-executive-order",
      "type": "policy",
      "title": "Executive Order on Safe, Secure, and Trustworthy AI"
    },
    {
      "id": "voluntary-commitments",
      "type": "policy",
      "title": "Voluntary AI Safety Commitments"
    },
    {
      "id": "pause-advocacy",
      "type": "approach",
      "title": "Pause Advocacy"
    },
    {
      "id": "coordination-tech",
      "type": "approach",
      "title": "Coordination Technologies"
    },
    {
      "id": "deliberation",
      "type": "approach",
      "title": "AI-Assisted Deliberation"
    },
    {
      "id": "eval-saturation",
      "type": "approach",
      "title": "Eval Saturation & The Evals Gap"
    },
    {
      "id": "safety-cases",
      "type": "approach",
      "title": "AI Safety Cases"
    },
    {
      "id": "evaluation",
      "type": "approach",
      "title": "AI Evaluation"
    },
    {
      "id": "model-auditing",
      "type": "approach",
      "title": "Third-Party Model Auditing"
    },
    {
      "id": "authoritarian-tools",
      "type": "risk",
      "title": "AI Authoritarian Tools"
    },
    {
      "id": "autonomous-weapons",
      "type": "risk",
      "title": "Autonomous Weapons"
    },
    {
      "id": "concentration-of-power",
      "type": "risk",
      "title": "Concentration of Power"
    },
    {
      "id": "institutional-capture",
      "type": "risk",
      "title": "Institutional Decision Capture"
    },
    {
      "id": "knowledge-monopoly",
      "type": "risk",
      "title": "AI Knowledge Monopoly"
    },
    {
      "id": "lock-in",
      "type": "risk",
      "title": "Lock-in"
    },
    {
      "id": "authoritarian-takeover",
      "type": "risk",
      "title": "Authoritarian Takeover"
    },
    {
      "id": "multipolar-trap",
      "type": "risk",
      "title": "Multipolar Trap"
    },
    {
      "id": "proliferation",
      "type": "risk",
      "title": "AI Proliferation"
    },
    {
      "id": "racing-dynamics",
      "type": "risk",
      "title": "Racing Dynamics"
    },
    {
      "id": "surveillance",
      "type": "risk",
      "title": "AI Mass Surveillance"
    }
  ],
  "governance-gap": [
    {
      "id": "institutional-adaptation-speed",
      "type": "model",
      "title": "Institutional Adaptation Speed Model"
    }
  ],
  "governance-infrastructure": [
    {
      "id": "model-registries",
      "type": "policy",
      "title": "Model Registries"
    }
  ],
  "governance-readiness": [
    {
      "id": "capability-alignment-race",
      "type": "analysis",
      "title": "Capability-Alignment Race Model"
    }
  ],
  "government-advisor": [
    {
      "id": "david-sacks",
      "type": "researcher",
      "title": "David Sacks"
    }
  ],
  "government-advisory": [
    {
      "id": "centre-for-long-term-resilience",
      "type": "organization",
      "title": "Centre for Long-Term Resilience"
    }
  ],
  "government-ai-safety": [
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    }
  ],
  "government-oversight": [
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    }
  ],
  "government-policy": [
    {
      "id": "nist-ai",
      "type": "organization",
      "title": "NIST and AI Safety"
    }
  ],
  "government-regulation": [
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    }
  ],
  "gpai": [
    {
      "id": "eu-ai-act",
      "type": "policy",
      "title": "EU AI Act"
    }
  ],
  "gpt": [
    {
      "id": "language-models",
      "type": "capability",
      "title": "Large Language Models"
    },
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    },
    {
      "id": "ilya-sutskever",
      "type": "researcher",
      "title": "Ilya Sutskever"
    }
  ],
  "gpt-4": [
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    },
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    }
  ],
  "grantmaking": [
    {
      "id": "holden-karnofsky",
      "type": "researcher",
      "title": "Holden Karnofsky"
    }
  ],
  "grassroots": [
    {
      "id": "controlai",
      "type": "organization",
      "title": "ControlAI"
    }
  ],
  "grok": [
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    }
  ],
  "hardware": [
    {
      "id": "compute-hardware",
      "type": "ai-transition-model-metric",
      "title": "Compute & Hardware"
    },
    {
      "id": "tmc-compute",
      "type": "ai-transition-model-subitem",
      "title": "Compute"
    }
  ],
  "hardware-governance": [
    {
      "id": "compute-governance",
      "type": "policy",
      "title": "Compute Governance"
    },
    {
      "id": "hardware-enabled-governance",
      "type": "policy",
      "title": "Hardware-Enabled Governance"
    }
  ],
  "harmlessness": [
    {
      "id": "constitutional-ai",
      "type": "approach",
      "title": "Constitutional AI"
    }
  ],
  "health-security": [
    {
      "id": "johns-hopkins-center-for-health-security",
      "type": "organization",
      "title": "Johns Hopkins Center for Health Security"
    }
  ],
  "hidden-capabilities": [
    {
      "id": "capability-elicitation",
      "type": "approach",
      "title": "Capability Elicitation"
    }
  ],
  "historical-evidence": [
    {
      "id": "historical-revisionism",
      "type": "risk",
      "title": "AI-Enabled Historical Revisionism"
    }
  ],
  "honesty": [
    {
      "id": "sycophancy",
      "type": "risk",
      "title": "Sycophancy"
    }
  ],
  "human-agency": [
    {
      "id": "enfeeblement",
      "type": "risk",
      "title": "Enfeeblement"
    },
    {
      "id": "erosion-of-agency",
      "type": "risk",
      "title": "Erosion of Human Agency"
    }
  ],
  "human-ai-interaction": [
    {
      "id": "human-agency",
      "type": "ai-transition-model-parameter",
      "title": "Human Agency"
    },
    {
      "id": "preference-authenticity",
      "type": "ai-transition-model-parameter",
      "title": "Preference Authenticity"
    },
    {
      "id": "automation-bias-cascade",
      "type": "model",
      "title": "Automation Bias Cascade Model"
    },
    {
      "id": "hybrid-systems",
      "type": "approach",
      "title": "AI-Human Hybrid Systems"
    },
    {
      "id": "automation-bias",
      "type": "risk",
      "title": "Automation Bias"
    }
  ],
  "human-autonomy": [
    {
      "id": "persuasion",
      "type": "capability",
      "title": "Persuasion and Social Manipulation"
    }
  ],
  "human-compatible-ai": [
    {
      "id": "chai",
      "type": "lab-academic",
      "title": "CHAI"
    },
    {
      "id": "stuart-russell",
      "type": "researcher",
      "title": "Stuart Russell"
    }
  ],
  "human-factors": [
    {
      "id": "human-expertise",
      "type": "ai-transition-model-parameter",
      "title": "Human Expertise"
    },
    {
      "id": "human-oversight-quality",
      "type": "ai-transition-model-parameter",
      "title": "Human Oversight Quality"
    },
    {
      "id": "expertise-atrophy",
      "type": "risk",
      "title": "Expertise Atrophy"
    }
  ],
  "human-feedback": [
    {
      "id": "rlhf",
      "type": "capability",
      "title": "RLHF"
    },
    {
      "id": "sycophancy",
      "type": "risk",
      "title": "Sycophancy"
    }
  ],
  "human-judgment": [
    {
      "id": "debate",
      "type": "approach",
      "title": "AI Safety via Debate"
    }
  ],
  "human-oversight": [
    {
      "id": "flash-dynamics",
      "type": "risk",
      "title": "Flash Dynamics"
    }
  ],
  "human-rights": [
    {
      "id": "authoritarian-tools",
      "type": "risk",
      "title": "AI Authoritarian Tools"
    }
  ],
  "iaea-model": [
    {
      "id": "international-compute-regimes",
      "type": "policy",
      "title": "International Compute Regimes"
    }
  ],
  "identity": [
    {
      "id": "deepfakes",
      "type": "risk",
      "title": "Deepfakes"
    },
    {
      "id": "fraud",
      "type": "risk",
      "title": "AI-Powered Fraud"
    }
  ],
  "ij-good": [
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    }
  ],
  "impact-assessment": [
    {
      "id": "anthropic-impact",
      "type": "analysis",
      "title": "Anthropic Impact Assessment Model"
    },
    {
      "id": "disinformation-electoral-impact",
      "type": "model",
      "title": "Electoral Impact Assessment Model"
    }
  ],
  "incentives": [
    {
      "id": "racing-dynamics-impact",
      "type": "model",
      "title": "Racing Dynamics Impact Model"
    },
    {
      "id": "lab-incentives-model",
      "type": "model",
      "title": "Lab Incentives Model"
    },
    {
      "id": "whistleblower-dynamics",
      "type": "model",
      "title": "Whistleblower Dynamics Model"
    }
  ],
  "incident-reporting": [
    {
      "id": "model-registries",
      "type": "policy",
      "title": "Model Registries"
    }
  ],
  "incidents": [
    {
      "id": "post-incident-recovery",
      "type": "model",
      "title": "Post-Incident Recovery Model"
    }
  ],
  "independent-evaluation": [
    {
      "id": "model-auditing",
      "type": "approach",
      "title": "Third-Party Model Auditing"
    }
  ],
  "induction-heads": [
    {
      "id": "neel-nanda",
      "type": "researcher",
      "title": "Neel Nanda"
    }
  ],
  "industry-accountability": [
    {
      "id": "lab-culture",
      "type": "approach",
      "title": "Lab Safety Culture"
    }
  ],
  "industry-commitments": [
    {
      "id": "voluntary-commitments",
      "type": "policy",
      "title": "Voluntary AI Safety Commitments"
    }
  ],
  "industry-practices": [
    {
      "id": "corporate",
      "type": "approach",
      "title": "Corporate Responses"
    }
  ],
  "industry-self-governance": [
    {
      "id": "frontier-model-forum",
      "type": "organization",
      "title": "Frontier Model Forum"
    }
  ],
  "inequality": [
    {
      "id": "winner-take-all-model",
      "type": "model",
      "title": "Winner-Take-All Market Dynamics Model"
    },
    {
      "id": "economic-disruption-model",
      "type": "model",
      "title": "Economic Disruption Structural Model"
    },
    {
      "id": "concentration-of-power",
      "type": "risk",
      "title": "Concentration of Power"
    },
    {
      "id": "economic-disruption",
      "type": "risk",
      "title": "Economic Disruption"
    }
  ],
  "inference-intervention": [
    {
      "id": "circuit-breakers",
      "type": "approach",
      "title": "Circuit Breakers / Inference Interventions"
    }
  ],
  "inference-time-intervention": [
    {
      "id": "representation-engineering",
      "type": "approach",
      "title": "Representation Engineering"
    }
  ],
  "influence-operations": [
    {
      "id": "disinformation",
      "type": "risk",
      "title": "AI Disinformation"
    }
  ],
  "information-aggregation": [
    {
      "id": "prediction-markets",
      "type": "approach",
      "title": "Prediction Markets"
    }
  ],
  "information-asymmetry": [
    {
      "id": "whistleblower-protections",
      "type": "policy",
      "title": "AI Whistleblower Protections"
    }
  ],
  "information-environment": [
    {
      "id": "epistemic-health",
      "type": "ai-transition-model-parameter",
      "title": "Epistemic Health"
    },
    {
      "id": "information-authenticity",
      "type": "ai-transition-model-parameter",
      "title": "Information Authenticity"
    },
    {
      "id": "reality-coherence",
      "type": "ai-transition-model-parameter",
      "title": "Reality Coherence"
    }
  ],
  "information-infrastructure": [
    {
      "id": "epistemic-infrastructure",
      "type": "approach",
      "title": "Epistemic Infrastructure"
    },
    {
      "id": "knowledge-monopoly",
      "type": "risk",
      "title": "AI Knowledge Monopoly"
    }
  ],
  "information-overload": [
    {
      "id": "learned-helplessness",
      "type": "risk",
      "title": "Epistemic Learned Helplessness"
    }
  ],
  "information-security": [
    {
      "id": "epistemic-security",
      "type": "approach",
      "title": "Epistemic Security"
    }
  ],
  "information-silos": [
    {
      "id": "reality-fragmentation-network",
      "type": "model",
      "title": "Reality Fragmentation Network Model"
    }
  ],
  "information-warfare": [
    {
      "id": "cyberweapons",
      "type": "risk",
      "title": "Cyberweapons Risk"
    },
    {
      "id": "disinformation",
      "type": "risk",
      "title": "AI Disinformation"
    }
  ],
  "infrastructure": [
    {
      "id": "compute-hardware",
      "type": "ai-transition-model-metric",
      "title": "Compute & Hardware"
    },
    {
      "id": "societal-resilience",
      "type": "ai-transition-model-parameter",
      "title": "Societal Resilience"
    }
  ],
  "inner-alignment": [
    {
      "id": "situational-awareness",
      "type": "capability",
      "title": "Situational Awareness"
    },
    {
      "id": "accident-risks",
      "type": "crux",
      "title": "Accident Risk Cruxes"
    },
    {
      "id": "deceptive-alignment-decomposition",
      "type": "model",
      "title": "Deceptive Alignment Decomposition Model"
    },
    {
      "id": "mesa-optimization-analysis",
      "type": "model",
      "title": "Mesa-Optimization Risk Analysis"
    },
    {
      "id": "deceptive-alignment",
      "type": "risk",
      "title": "Deceptive Alignment"
    },
    {
      "id": "goal-misgeneralization",
      "type": "risk",
      "title": "Goal Misgeneralization"
    },
    {
      "id": "mesa-optimization",
      "type": "risk",
      "title": "Mesa-Optimization"
    },
    {
      "id": "scheming",
      "type": "risk",
      "title": "Scheming"
    }
  ],
  "instability": [
    {
      "id": "economic-disruption-impact",
      "type": "model",
      "title": "Economic Disruption Impact Model"
    }
  ],
  "institutional-adaptation": [
    {
      "id": "structural-risks",
      "type": "crux",
      "title": "Structural Risk Cruxes"
    }
  ],
  "institutional-risk": [
    {
      "id": "institutional-capture",
      "type": "risk",
      "title": "Institutional Decision Capture"
    }
  ],
  "institutional-trust": [
    {
      "id": "trust-cascade",
      "type": "risk",
      "title": "Trust Cascade Failure"
    }
  ],
  "institutions": [
    {
      "id": "civilizational-competence",
      "type": "ai-transition-model-factor",
      "title": "Civilizational Competence"
    },
    {
      "id": "structural",
      "type": "ai-transition-model-metric",
      "title": "Structural Indicators"
    },
    {
      "id": "regulatory-capacity",
      "type": "ai-transition-model-parameter",
      "title": "Regulatory Capacity"
    },
    {
      "id": "institutional-quality",
      "type": "ai-transition-model-parameter",
      "title": "Institutional Quality"
    },
    {
      "id": "trust-cascade-model",
      "type": "model",
      "title": "Trust Cascade Failure Model"
    },
    {
      "id": "trust-erosion-dynamics",
      "type": "model",
      "title": "Trust Erosion Dynamics Model"
    },
    {
      "id": "institutional-adaptation-speed",
      "type": "model",
      "title": "Institutional Adaptation Speed Model"
    },
    {
      "id": "trust-decline",
      "type": "risk",
      "title": "Trust Decline"
    }
  ],
  "instrumental-convergence": [
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    },
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    },
    {
      "id": "nick-bostrom",
      "type": "researcher",
      "title": "Nick Bostrom"
    },
    {
      "id": "corrigibility-failure",
      "type": "risk",
      "title": "Corrigibility Failure"
    },
    {
      "id": "power-seeking",
      "type": "risk",
      "title": "Power-Seeking AI"
    },
    {
      "id": "rogue-ai-scenarios",
      "type": "risk",
      "title": "Rogue AI Scenarios"
    }
  ],
  "instrumental-goals": [
    {
      "id": "power-seeking-conditions",
      "type": "model",
      "title": "Power-Seeking Emergence Conditions Model"
    },
    {
      "id": "instrumental-convergence-framework",
      "type": "model",
      "title": "Instrumental Convergence Framework"
    }
  ],
  "intelligence-explosion": [
    {
      "id": "self-improvement",
      "type": "capability",
      "title": "Self-Improvement and Recursive Enhancement"
    },
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    }
  ],
  "international": [
    {
      "id": "geopolitics",
      "type": "ai-transition-model-metric",
      "title": "Geopolitics"
    },
    {
      "id": "international-coordination",
      "type": "ai-transition-model-parameter",
      "title": "International Coordination"
    },
    {
      "id": "coordination-capacity",
      "type": "ai-transition-model-parameter",
      "title": "Coordination Capacity"
    },
    {
      "id": "governance-policy",
      "type": "crux",
      "title": "AI Governance and Policy"
    },
    {
      "id": "ai-safety-summit",
      "type": "historical",
      "title": "AI Safety Summit (Bletchley Park)"
    },
    {
      "id": "govai",
      "type": "lab-research",
      "title": "GovAI"
    },
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    },
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    },
    {
      "id": "china-ai-regulations",
      "type": "policy",
      "title": "China AI Regulatory Framework"
    },
    {
      "id": "compute-governance",
      "type": "policy",
      "title": "Compute Governance"
    },
    {
      "id": "international-compute-regimes",
      "type": "policy",
      "title": "International Compute Regimes"
    },
    {
      "id": "international-summits",
      "type": "policy",
      "title": "International AI Safety Summit Series"
    },
    {
      "id": "voluntary-commitments",
      "type": "policy",
      "title": "Voluntary AI Safety Commitments"
    }
  ],
  "international-agreement": [
    {
      "id": "bletchley-declaration",
      "type": "policy",
      "title": "Bletchley Declaration"
    }
  ],
  "international-cooperation": [
    {
      "id": "solutions",
      "type": "crux",
      "title": "Solution Cruxes"
    },
    {
      "id": "coordination-tech",
      "type": "approach",
      "title": "Coordination Technologies"
    }
  ],
  "international-coordination": [
    {
      "id": "structural-risks",
      "type": "crux",
      "title": "Structural Risk Cruxes"
    },
    {
      "id": "governance-focused",
      "type": "concept",
      "title": "Governance-Focused Worldview"
    },
    {
      "id": "international-coordination-game",
      "type": "model",
      "title": "International Coordination Game Model"
    },
    {
      "id": "coordination-mechanisms",
      "type": "policy",
      "title": "International Coordination Mechanisms"
    }
  ],
  "international-governance": [
    {
      "id": "international-regimes",
      "type": "policy",
      "title": "International Compute Regimes"
    }
  ],
  "interpretability": [
    {
      "id": "interpretability-coverage",
      "type": "ai-transition-model-parameter",
      "title": "Interpretability Coverage"
    },
    {
      "id": "research-agendas",
      "type": "crux",
      "title": "Research Agendas"
    },
    {
      "id": "technical-research",
      "type": "crux",
      "title": "Technical AI Safety Research"
    },
    {
      "id": "natural-abstractions",
      "type": "concept",
      "title": "Natural Abstractions"
    },
    {
      "id": "interpretability-sufficient",
      "type": "crux",
      "title": "Is Interpretability Sufficient for Safety?"
    },
    {
      "id": "dense-transformers",
      "type": "concept",
      "title": "Dense Transformers"
    },
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    },
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    },
    {
      "id": "capability-alignment-race",
      "type": "analysis",
      "title": "Capability-Alignment Race Model"
    },
    {
      "id": "model-organisms-of-misalignment",
      "type": "analysis",
      "title": "Model Organisms of Misalignment"
    },
    {
      "id": "anthropic",
      "type": "lab",
      "title": "Anthropic"
    },
    {
      "id": "conjecture",
      "type": "lab-research",
      "title": "Conjecture"
    },
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    },
    {
      "id": "chris-olah",
      "type": "researcher",
      "title": "Chris Olah"
    },
    {
      "id": "connor-leahy",
      "type": "researcher",
      "title": "Connor Leahy"
    },
    {
      "id": "dario-amodei",
      "type": "researcher",
      "title": "Dario Amodei"
    },
    {
      "id": "neel-nanda",
      "type": "researcher",
      "title": "Neel Nanda"
    },
    {
      "id": "yoshua-bengio",
      "type": "researcher",
      "title": "Yoshua Bengio"
    },
    {
      "id": "anthropic-core-views",
      "type": "safety-agenda",
      "title": "Anthropic Core Views"
    },
    {
      "id": "scheming-detection",
      "type": "approach",
      "title": "Scheming & Deception Detection"
    },
    {
      "id": "sleeper-agent-detection",
      "type": "approach",
      "title": "Sleeper Agent Detection"
    },
    {
      "id": "ai-assisted",
      "type": "approach",
      "title": "AI-Assisted Alignment"
    },
    {
      "id": "mech-interp",
      "type": "approach",
      "title": "Mechanistic Interpretability"
    },
    {
      "id": "representation-engineering",
      "type": "approach",
      "title": "Representation Engineering"
    },
    {
      "id": "sparse-autoencoders",
      "type": "approach",
      "title": "Sparse Autoencoders (SAEs)"
    },
    {
      "id": "deceptive-alignment",
      "type": "risk",
      "title": "Deceptive Alignment"
    }
  ],
  "intervention-design": [
    {
      "id": "corrigibility-failure-pathways",
      "type": "model",
      "title": "Corrigibility Failure Pathways"
    }
  ],
  "intervention-effectiveness": [
    {
      "id": "worldview-intervention-mapping",
      "type": "model",
      "title": "Worldview-Intervention Mapping"
    }
  ],
  "intervention-prioritization": [
    {
      "id": "short-timeline-policy-implications",
      "type": "analysis",
      "title": "Short Timeline Policy Implications"
    }
  ],
  "interventions": [
    {
      "id": "intervention-effectiveness-matrix",
      "type": "model",
      "title": "Intervention Effectiveness Matrix"
    }
  ],
  "inverse-reinforcement-learning": [
    {
      "id": "chai",
      "type": "lab-academic",
      "title": "CHAI"
    },
    {
      "id": "stuart-russell",
      "type": "researcher",
      "title": "Stuart Russell"
    }
  ],
  "ipo": [
    {
      "id": "anthropic-ipo",
      "type": "analysis",
      "title": "Anthropic IPO"
    }
  ],
  "irreversibility": [
    {
      "id": "lock-in-mechanisms",
      "type": "model",
      "title": "Lock-in Mechanisms Model"
    },
    {
      "id": "expertise-atrophy-progression",
      "type": "model",
      "title": "Expertise Atrophy Progression Model"
    },
    {
      "id": "lock-in-model",
      "type": "model",
      "title": "Lock-in Irreversibility Model"
    },
    {
      "id": "irreversibility-threshold",
      "type": "model",
      "title": "Irreversibility Threshold Model"
    },
    {
      "id": "lock-in",
      "type": "risk",
      "title": "Lock-in"
    }
  ],
  "isaac-asimov": [
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    }
  ],
  "iterated-amplification": [
    {
      "id": "paul-christiano",
      "type": "researcher",
      "title": "Paul Christiano"
    }
  ],
  "jailbreak-defense": [
    {
      "id": "circuit-breakers",
      "type": "approach",
      "title": "Circuit Breakers / Inference Interventions"
    }
  ],
  "jailbreaking": [
    {
      "id": "red-teaming",
      "type": "approach",
      "title": "Red Teaming"
    },
    {
      "id": "refusal-training",
      "type": "approach",
      "title": "Refusal Training"
    }
  ],
  "know-your-customer": [
    {
      "id": "compute-governance",
      "type": "policy",
      "title": "Compute Governance"
    },
    {
      "id": "us-executive-order",
      "type": "policy",
      "title": "Executive Order on Safe, Secure, and Trustworthy AI"
    }
  ],
  "knowledge-access": [
    {
      "id": "knowledge-monopoly",
      "type": "risk",
      "title": "AI Knowledge Monopoly"
    }
  ],
  "knowledge-management": [
    {
      "id": "epistemic-infrastructure",
      "type": "approach",
      "title": "Epistemic Infrastructure"
    }
  ],
  "kyc": [
    {
      "id": "compute-monitoring",
      "type": "policy",
      "title": "Compute Monitoring"
    }
  ],
  "lab-behavior": [
    {
      "id": "safety-culture-equilibrium",
      "type": "model",
      "title": "Safety Culture Equilibrium Model"
    }
  ],
  "lab-safety": [
    {
      "id": "short-timeline-policy-implications",
      "type": "analysis",
      "title": "Short Timeline Policy Implications"
    }
  ],
  "labor": [
    {
      "id": "economic-labor",
      "type": "ai-transition-model-metric",
      "title": "Economic & Labor"
    },
    {
      "id": "economic-disruption-impact",
      "type": "model",
      "title": "Economic Disruption Impact Model"
    }
  ],
  "labor-economics": [
    {
      "id": "economic-disruption-model",
      "type": "model",
      "title": "Economic Disruption Structural Model"
    }
  ],
  "labor-market": [
    {
      "id": "economic-stability",
      "type": "ai-transition-model-parameter",
      "title": "Economic Stability"
    }
  ],
  "labor-markets": [
    {
      "id": "economic-disruption",
      "type": "risk",
      "title": "Economic Disruption"
    }
  ],
  "labs": [
    {
      "id": "lab-behavior",
      "type": "ai-transition-model-metric",
      "title": "Lab Behavior"
    },
    {
      "id": "lab-incentives-model",
      "type": "model",
      "title": "Lab Incentives Model"
    }
  ],
  "laws": [
    {
      "id": "autonomous-weapons",
      "type": "risk",
      "title": "Autonomous Weapons"
    }
  ],
  "lawsuit": [
    {
      "id": "musk-openai-lawsuit",
      "type": "analysis",
      "title": "Musk v. OpenAI Lawsuit"
    }
  ],
  "layered-approach": [
    {
      "id": "defense-in-depth-model",
      "type": "model",
      "title": "Defense in Depth Model"
    }
  ],
  "learned-optimization": [
    {
      "id": "mesa-optimization-analysis",
      "type": "model",
      "title": "Mesa-Optimization Risk Analysis"
    },
    {
      "id": "mesa-optimization",
      "type": "risk",
      "title": "Mesa-Optimization"
    }
  ],
  "learning": [
    {
      "id": "value-learning",
      "type": "safety-agenda",
      "title": "Value Learning"
    }
  ],
  "legal-system": [
    {
      "id": "legal-evidence-crisis",
      "type": "risk",
      "title": "Legal Evidence Crisis"
    }
  ],
  "legislation": [
    {
      "id": "whistleblower-protections",
      "type": "policy",
      "title": "AI Whistleblower Protections"
    }
  ],
  "legitimacy": [
    {
      "id": "trust-cascade",
      "type": "risk",
      "title": "Trust Cascade Failure"
    }
  ],
  "lesswrong": [
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    },
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    }
  ],
  "liability": [
    {
      "id": "governance-policy",
      "type": "crux",
      "title": "AI Governance and Policy"
    },
    {
      "id": "california-sb1047",
      "type": "policy",
      "title": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act"
    }
  ],
  "limitations": [
    {
      "id": "data-constraints",
      "type": "concept",
      "title": "Data Constraints"
    }
  ],
  "llm-as-judge": [
    {
      "id": "scalable-eval-approaches",
      "type": "approach",
      "title": "Scalable Eval Approaches"
    }
  ],
  "lobbying": [
    {
      "id": "leading-the-future",
      "type": "organization",
      "title": "Leading the Future super PAC"
    }
  ],
  "lock-in": [
    {
      "id": "long-term-lockin",
      "type": "ai-transition-model-scenario",
      "title": "Long-term Lock-in"
    },
    {
      "id": "structural-risks",
      "type": "crux",
      "title": "Structural Risk Cruxes"
    },
    {
      "id": "irreversibility-threshold",
      "type": "model",
      "title": "Irreversibility Threshold Model"
    },
    {
      "id": "authoritarian-tools",
      "type": "risk",
      "title": "AI Authoritarian Tools"
    },
    {
      "id": "concentration-of-power",
      "type": "risk",
      "title": "Concentration of Power"
    },
    {
      "id": "authoritarian-takeover",
      "type": "risk",
      "title": "Authoritarian Takeover"
    }
  ],
  "logical-uncertainty": [
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    }
  ],
  "long-term": [
    {
      "id": "long-term-trajectory",
      "type": "ai-transition-model-scenario",
      "title": "Long-term Trajectory"
    },
    {
      "id": "long-term-lockin",
      "type": "ai-transition-model-scenario",
      "title": "Long-term Lock-in"
    },
    {
      "id": "lock-in-model",
      "type": "model",
      "title": "Lock-in Irreversibility Model"
    },
    {
      "id": "enfeeblement",
      "type": "risk",
      "title": "Enfeeblement"
    },
    {
      "id": "lock-in",
      "type": "risk",
      "title": "Lock-in"
    }
  ],
  "long-term-future": [
    {
      "id": "irreversibility",
      "type": "risk",
      "title": "Irreversibility"
    }
  ],
  "long-term-research": [
    {
      "id": "formal-verification",
      "type": "approach",
      "title": "Formal Verification"
    },
    {
      "id": "provably-safe",
      "type": "approach",
      "title": "Provably Safe AI (davidad agenda)"
    }
  ],
  "longtermism": [
    {
      "id": "existential-risk",
      "type": "concept",
      "title": "Existential Risk"
    },
    {
      "id": "toby-ord",
      "type": "researcher",
      "title": "Toby Ord"
    }
  ],
  "malicious-use": [
    {
      "id": "misuse",
      "type": "concept",
      "title": "AI Misuse"
    }
  ],
  "manipulation": [
    {
      "id": "persuasion",
      "type": "capability",
      "title": "Persuasion and Social Manipulation"
    },
    {
      "id": "consensus-manufacturing-dynamics",
      "type": "model",
      "title": "Consensus Manufacturing Dynamics Model"
    },
    {
      "id": "preference-manipulation-drift",
      "type": "model",
      "title": "Preference Manipulation Drift Model"
    },
    {
      "id": "cyber-psychosis",
      "type": "risk",
      "title": "Cyber Psychosis"
    },
    {
      "id": "erosion-of-agency",
      "type": "risk",
      "title": "Erosion of Human Agency"
    }
  ],
  "marginal-risk": [
    {
      "id": "bioweapons-ai-uplift",
      "type": "model",
      "title": "AI Uplift Assessment Model"
    }
  ],
  "market-concentration": [
    {
      "id": "knowledge-monopoly",
      "type": "risk",
      "title": "AI Knowledge Monopoly"
    },
    {
      "id": "winner-take-all",
      "type": "risk",
      "title": "Winner-Take-All Dynamics"
    }
  ],
  "market-dynamics": [
    {
      "id": "ai-control-concentration",
      "type": "ai-transition-model-parameter",
      "title": "AI Control Concentration"
    },
    {
      "id": "racing-intensity",
      "type": "ai-transition-model-parameter",
      "title": "Racing Intensity"
    }
  ],
  "market-structure": [
    {
      "id": "winner-take-all-model",
      "type": "model",
      "title": "Winner-Take-All Market Dynamics Model"
    }
  ],
  "mathematical-guarantees": [
    {
      "id": "provable-safe",
      "type": "concept",
      "title": "Provable / Guaranteed Safe AI"
    },
    {
      "id": "formal-verification",
      "type": "approach",
      "title": "Formal Verification"
    },
    {
      "id": "provably-safe",
      "type": "approach",
      "title": "Provably Safe AI (davidad agenda)"
    }
  ],
  "mcp-security": [
    {
      "id": "tool-restrictions",
      "type": "approach",
      "title": "Tool-Use Restrictions"
    }
  ],
  "mechanism-design": [
    {
      "id": "coordination-tech",
      "type": "approach",
      "title": "Coordination Technologies"
    },
    {
      "id": "prediction-markets",
      "type": "approach",
      "title": "Prediction Markets"
    }
  ],
  "mechanistic-interpretability": [
    {
      "id": "goodfire",
      "type": "lab-research",
      "title": "Goodfire"
    },
    {
      "id": "max-tegmark",
      "type": "researcher",
      "title": "Max Tegmark"
    }
  ],
  "media": [
    {
      "id": "media-policy-feedback-loop",
      "type": "model",
      "title": "Media-Policy Feedback Loop Model"
    },
    {
      "id": "trust-decline",
      "type": "risk",
      "title": "Trust Decline"
    }
  ],
  "media-literacy": [
    {
      "id": "epistemic-security",
      "type": "approach",
      "title": "Epistemic Security"
    },
    {
      "id": "learned-helplessness",
      "type": "risk",
      "title": "Epistemic Learned Helplessness"
    }
  ],
  "memory-systems": [
    {
      "id": "long-horizon",
      "type": "capability",
      "title": "Long-Horizon Autonomous Tasks"
    }
  ],
  "mental-health": [
    {
      "id": "cyber-psychosis-cascade",
      "type": "model",
      "title": "Cyber Psychosis Cascade Model"
    },
    {
      "id": "cyber-psychosis",
      "type": "risk",
      "title": "Cyber Psychosis"
    }
  ],
  "mesa-optimization": [
    {
      "id": "accident-risks",
      "type": "crux",
      "title": "Accident Risk Cruxes"
    },
    {
      "id": "mesa-optimization-analysis",
      "type": "model",
      "title": "Mesa-Optimization Risk Analysis"
    },
    {
      "id": "deceptive-alignment",
      "type": "risk",
      "title": "Deceptive Alignment"
    }
  ],
  "methodology": [
    {
      "id": "reasoning",
      "type": "capability",
      "title": "Reasoning and Planning"
    }
  ],
  "metrics": [
    {
      "id": "benchmarking",
      "type": "concept",
      "title": "AI Benchmarking"
    }
  ],
  "microsoft": [
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    }
  ],
  "military-ai": [
    {
      "id": "autonomous-weapons",
      "type": "risk",
      "title": "Autonomous Weapons"
    }
  ],
  "miri": [
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    },
    {
      "id": "sharp-left-turn",
      "type": "risk",
      "title": "Sharp Left Turn"
    }
  ],
  "misalignment": [
    {
      "id": "ai-takeover",
      "type": "ai-transition-model-scenario",
      "title": "AI Takeover"
    },
    {
      "id": "misaligned-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Misaligned Catastrophe - The Bad Ending"
    },
    {
      "id": "model-organisms-of-misalignment",
      "type": "analysis",
      "title": "Model Organisms of Misalignment"
    }
  ],
  "misuse": [
    {
      "id": "misuse-potential",
      "type": "ai-transition-model-factor",
      "title": "Misuse Potential"
    },
    {
      "id": "human-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Human-Caused Catastrophe"
    },
    {
      "id": "misuse",
      "type": "concept",
      "title": "AI Misuse"
    },
    {
      "id": "misuse-risks",
      "type": "crux",
      "title": "Misuse Risk Cruxes"
    },
    {
      "id": "cyber-offense",
      "type": "risk",
      "title": "AI-Enabled Cyberattacks"
    },
    {
      "id": "bio-risk",
      "type": "risk",
      "title": "AI-Enabled Biological Risks"
    }
  ],
  "misuse-prevention": [
    {
      "id": "capability-unlearning",
      "type": "approach",
      "title": "Capability Unlearning / Removal"
    },
    {
      "id": "refusal-training",
      "type": "approach",
      "title": "Refusal Training"
    }
  ],
  "misuse-risk": [
    {
      "id": "open-source",
      "type": "approach",
      "title": "Open Source Safety"
    }
  ],
  "ml-safety": [
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    },
    {
      "id": "distributional-shift",
      "type": "risk",
      "title": "Distributional Shift"
    }
  ],
  "mmlu": [
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    }
  ],
  "model-distribution": [
    {
      "id": "structured-access",
      "type": "approach",
      "title": "Structured Access / API-Only"
    }
  ],
  "model-editing": [
    {
      "id": "capability-unlearning",
      "type": "approach",
      "title": "Capability Unlearning / Removal"
    }
  ],
  "model-organisms": [
    {
      "id": "model-organisms-of-misalignment",
      "type": "analysis",
      "title": "Model Organisms of Misalignment"
    },
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "conjecture",
      "type": "lab-research",
      "title": "Conjecture"
    }
  ],
  "model-registration": [
    {
      "id": "model-registries",
      "type": "policy",
      "title": "Model Registries"
    }
  ],
  "model-self-knowledge": [
    {
      "id": "situational-awareness",
      "type": "capability",
      "title": "Situational Awareness"
    }
  ],
  "model-transparency": [
    {
      "id": "goodfire",
      "type": "lab-research",
      "title": "Goodfire"
    }
  ],
  "model-weights": [
    {
      "id": "open-source",
      "type": "approach",
      "title": "Open Source Safety"
    }
  ],
  "models": [
    {
      "id": "lock-in-mechanisms",
      "type": "model",
      "title": "Lock-in Mechanisms Model"
    }
  ],
  "moloch": [
    {
      "id": "multipolar-trap-model",
      "type": "model",
      "title": "Multipolar Trap Coordination Model"
    }
  ],
  "monitoring": [
    {
      "id": "warning-signs-model",
      "type": "model",
      "title": "Warning Signs Model"
    },
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    },
    {
      "id": "compute-monitoring",
      "type": "policy",
      "title": "Compute Monitoring"
    },
    {
      "id": "ai-control",
      "type": "safety-agenda",
      "title": "AI Control"
    }
  ],
  "monosemanticity": [
    {
      "id": "chris-olah",
      "type": "researcher",
      "title": "Chris Olah"
    },
    {
      "id": "sparse-autoencoders",
      "type": "approach",
      "title": "Sparse Autoencoders (SAEs)"
    }
  ],
  "moral-patienthood": [
    {
      "id": "ai-welfare",
      "type": "concept",
      "title": "AI Welfare and Digital Minds"
    }
  ],
  "moral-philosophy": [
    {
      "id": "toby-ord",
      "type": "researcher",
      "title": "Toby Ord"
    }
  ],
  "moratorium": [
    {
      "id": "pause-moratorium",
      "type": "policy",
      "title": "Pause / Moratorium"
    }
  ],
  "most-important-century": [
    {
      "id": "holden-karnofsky",
      "type": "researcher",
      "title": "Holden Karnofsky"
    }
  ],
  "multi-agent": [
    {
      "id": "heavy-scaffolding",
      "type": "concept",
      "title": "Heavy Scaffolding / Agentic Systems"
    }
  ],
  "multi-agent-systems": [
    {
      "id": "multi-agent",
      "type": "approach",
      "title": "Multi-Agent Safety"
    }
  ],
  "multilateral-diplomacy": [
    {
      "id": "international-summits",
      "type": "policy",
      "title": "International AI Safety Summit Series"
    }
  ],
  "multilateral-treaties": [
    {
      "id": "coordination-mechanisms",
      "type": "policy",
      "title": "International Coordination Mechanisms"
    },
    {
      "id": "international-regimes",
      "type": "policy",
      "title": "International Compute Regimes"
    }
  ],
  "multipolar": [
    {
      "id": "multipolar-competition",
      "type": "ai-transition-model-scenario",
      "title": "Multipolar Competition - The Fragmented World"
    }
  ],
  "national-security": [
    {
      "id": "cyberweapons",
      "type": "risk",
      "title": "Cyberweapons Risk"
    }
  ],
  "natural-abstractions": [
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    }
  ],
  "ndas": [
    {
      "id": "whistleblower-protections",
      "type": "policy",
      "title": "AI Whistleblower Protections"
    }
  ],
  "net-impact": [
    {
      "id": "anthropic-impact",
      "type": "analysis",
      "title": "Anthropic Impact Assessment Model"
    }
  ],
  "network-analysis": [
    {
      "id": "reality-fragmentation-network",
      "type": "model",
      "title": "Reality Fragmentation Network Model"
    }
  ],
  "network-effects": [
    {
      "id": "winner-take-all-concentration",
      "type": "model",
      "title": "Winner-Take-All Concentration Model"
    },
    {
      "id": "winner-take-all-model",
      "type": "model",
      "title": "Winner-Take-All Market Dynamics Model"
    }
  ],
  "networks": [
    {
      "id": "risk-interaction-network",
      "type": "model",
      "title": "Risk Interaction Network Model"
    },
    {
      "id": "parameter-interaction-network",
      "type": "model",
      "title": "Parameter Interaction Network Model"
    }
  ],
  "neural-network-analysis": [
    {
      "id": "mech-interp",
      "type": "approach",
      "title": "Mechanistic Interpretability"
    },
    {
      "id": "sparse-autoencoders",
      "type": "approach",
      "title": "Sparse Autoencoders (SAEs)"
    }
  ],
  "neural-network-circuits": [
    {
      "id": "chris-olah",
      "type": "researcher",
      "title": "Chris Olah"
    }
  ],
  "neural-network-internals": [
    {
      "id": "conjecture",
      "type": "lab-research",
      "title": "Conjecture"
    }
  ],
  "neural-networks": [
    {
      "id": "geoffrey-hinton",
      "type": "researcher",
      "title": "Geoffrey Hinton"
    }
  ],
  "neuro-symbolic": [
    {
      "id": "provable-safe",
      "type": "concept",
      "title": "Provable / Guaranteed Safe AI"
    }
  ],
  "new-york": [
    {
      "id": "new-york-raise-act",
      "type": "policy",
      "title": "New York RAISE Act"
    }
  ],
  "nick-bostrom": [
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    },
    {
      "id": "treacherous-turn",
      "type": "risk",
      "title": "Treacherous Turn"
    }
  ],
  "nist": [
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    }
  ],
  "nonprofit-conversion": [
    {
      "id": "musk-openai-lawsuit",
      "type": "analysis",
      "title": "Musk v. OpenAI Lawsuit"
    }
  ],
  "nonprofit-governance": [
    {
      "id": "openai-foundation",
      "type": "organization",
      "title": "OpenAI Foundation"
    }
  ],
  "nonprofit-structure": [
    {
      "id": "openai-foundation-governance",
      "type": "analysis",
      "title": "OpenAI Foundation Governance Paradox"
    }
  ],
  "norbert-wiener": [
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    }
  ],
  "o1": [
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    }
  ],
  "off-switch-problem": [
    {
      "id": "stuart-russell",
      "type": "researcher",
      "title": "Stuart Russell"
    }
  ],
  "offense-defense": [
    {
      "id": "cyberweapons-offense-defense",
      "type": "model",
      "title": "Cyber Offense-Defense Balance Model"
    }
  ],
  "offense-defense-balance": [
    {
      "id": "misuse-risks",
      "type": "crux",
      "title": "Misuse Risk Cruxes"
    }
  ],
  "open-source": [
    {
      "id": "open-vs-closed",
      "type": "crux",
      "title": "Open vs Closed Source AI"
    },
    {
      "id": "proliferation-model",
      "type": "model",
      "title": "AI Capability Proliferation Model"
    },
    {
      "id": "open-source",
      "type": "approach",
      "title": "Open Source Safety"
    },
    {
      "id": "proliferation",
      "type": "risk",
      "title": "AI Proliferation"
    }
  ],
  "open-source-ai": [
    {
      "id": "multi-actor-landscape",
      "type": "analysis",
      "title": "Multi-Actor Strategic Landscape"
    }
  ],
  "open-weights": [
    {
      "id": "large-language-models",
      "type": "concept",
      "title": "Large Language Models"
    }
  ],
  "openai": [
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    },
    {
      "id": "openai-foundation-governance",
      "type": "analysis",
      "title": "OpenAI Foundation Governance Paradox"
    },
    {
      "id": "musk-openai-lawsuit",
      "type": "analysis",
      "title": "Musk v. OpenAI Lawsuit"
    },
    {
      "id": "openai-foundation",
      "type": "organization",
      "title": "OpenAI Foundation"
    },
    {
      "id": "ilya-sutskever",
      "type": "researcher",
      "title": "Ilya Sutskever"
    }
  ],
  "openai-leadership-crisis": [
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    }
  ],
  "optimal-policies": [
    {
      "id": "power-seeking-conditions",
      "type": "model",
      "title": "Power-Seeking Emergence Conditions Model"
    },
    {
      "id": "power-seeking",
      "type": "risk",
      "title": "Power-Seeking AI"
    }
  ],
  "optimistic": [
    {
      "id": "aligned-agi",
      "type": "ai-transition-model-scenario",
      "title": "Aligned AGI - The Good Ending"
    },
    {
      "id": "why-alignment-easy",
      "type": "argument",
      "title": "Why Alignment Might Be Easy"
    }
  ],
  "optimization": [
    {
      "id": "safety-research-allocation",
      "type": "model",
      "title": "Safety Research Allocation Model"
    }
  ],
  "organizational": [
    {
      "id": "safety-culture-strength",
      "type": "ai-transition-model-parameter",
      "title": "Safety Culture Strength"
    }
  ],
  "organizational-practices": [
    {
      "id": "lab-culture",
      "type": "approach",
      "title": "Lab Safety Culture"
    }
  ],
  "orthogonality-thesis": [
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    },
    {
      "id": "nick-bostrom",
      "type": "researcher",
      "title": "Nick Bostrom"
    },
    {
      "id": "instrumental-convergence",
      "type": "risk",
      "title": "Instrumental Convergence"
    }
  ],
  "out-of-distribution": [
    {
      "id": "distributional-shift",
      "type": "risk",
      "title": "Distributional Shift"
    },
    {
      "id": "goal-misgeneralization",
      "type": "risk",
      "title": "Goal Misgeneralization"
    }
  ],
  "out-of-distribution-detection": [
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    }
  ],
  "outcome": [
    {
      "id": "existential-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Existential Catastrophe"
    },
    {
      "id": "long-term-trajectory",
      "type": "ai-transition-model-scenario",
      "title": "Long-term Trajectory"
    }
  ],
  "outer-alignment": [
    {
      "id": "mesa-optimization",
      "type": "risk",
      "title": "Mesa-Optimization"
    },
    {
      "id": "reward-hacking",
      "type": "risk",
      "title": "Reward Hacking"
    }
  ],
  "over-refusal": [
    {
      "id": "refusal-training",
      "type": "approach",
      "title": "Refusal Training"
    }
  ],
  "ownership": [
    {
      "id": "ai-ownership",
      "type": "ai-transition-model-factor",
      "title": "AI Ownership"
    }
  ],
  "oxford": [
    {
      "id": "fhi",
      "type": "organization",
      "title": "Future of Humanity Institute"
    }
  ],
  "p-doom": [
    {
      "id": "accident-risks",
      "type": "crux",
      "title": "Accident Risk Cruxes"
    }
  ],
  "pandemic-preparedness": [
    {
      "id": "ea-biosecurity-scope",
      "type": "analysis",
      "title": "Is EA Biosecurity Work Limited to Restricting LLM Biological Use?"
    },
    {
      "id": "johns-hopkins-center-for-health-security",
      "type": "organization",
      "title": "Johns Hopkins Center for Health Security"
    }
  ],
  "paper-mills": [
    {
      "id": "scientific-corruption",
      "type": "risk",
      "title": "Scientific Knowledge Corruption"
    }
  ],
  "parameter-counts": [
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    }
  ],
  "parameters": [
    {
      "id": "parameter-interaction-network",
      "type": "model",
      "title": "Parameter Interaction Network Model"
    }
  ],
  "parasocial-relationships": [
    {
      "id": "cyber-psychosis",
      "type": "risk",
      "title": "Cyber Psychosis"
    }
  ],
  "participatory-democracy": [
    {
      "id": "deliberation",
      "type": "approach",
      "title": "AI-Assisted Deliberation"
    }
  ],
  "path-dependence": [
    {
      "id": "lock-in-mechanisms",
      "type": "model",
      "title": "Lock-in Mechanisms Model"
    },
    {
      "id": "lock-in-model",
      "type": "model",
      "title": "Lock-in Irreversibility Model"
    },
    {
      "id": "lock-in",
      "type": "risk",
      "title": "Lock-in"
    }
  ],
  "paul-christiano": [
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    }
  ],
  "pause": [
    {
      "id": "pause-and-redirect",
      "type": "ai-transition-model-scenario",
      "title": "Pause and Redirect - The Deliberate Path"
    },
    {
      "id": "pause-debate",
      "type": "crux",
      "title": "Should We Pause AI Development?"
    }
  ],
  "pause-advocacy": [
    {
      "id": "pause",
      "type": "approach",
      "title": "Pause Advocacy"
    }
  ],
  "pause-debate": [
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    }
  ],
  "paypal-mafia": [
    {
      "id": "david-sacks",
      "type": "researcher",
      "title": "David Sacks"
    }
  ],
  "permission-systems": [
    {
      "id": "tool-restrictions",
      "type": "approach",
      "title": "Tool-Use Restrictions"
    }
  ],
  "persuasion": [
    {
      "id": "preference-manipulation",
      "type": "risk",
      "title": "Preference Manipulation"
    }
  ],
  "pessimistic": [
    {
      "id": "why-alignment-hard",
      "type": "argument",
      "title": "Why Alignment Might Be Hard"
    }
  ],
  "phase-transitions": [
    {
      "id": "emergent-capabilities",
      "type": "risk",
      "title": "Emergent Capabilities"
    }
  ],
  "philanthropic-capital": [
    {
      "id": "anthropic-investors",
      "type": "analysis",
      "title": "Anthropic (Funder)"
    },
    {
      "id": "anthropic-pre-ipo-daf-transfers",
      "type": "analysis",
      "title": "Anthropic Pre-IPO DAF Transfers"
    }
  ],
  "philanthropic-interventions": [
    {
      "id": "anthropic-pledge-enforcement",
      "type": "analysis",
      "title": "Anthropic Founder Pledges: Interventions to Increase Follow-Through"
    }
  ],
  "philanthropy": [
    {
      "id": "elon-musk-philanthropy",
      "type": "analysis",
      "title": "Elon Musk (Funder)"
    },
    {
      "id": "dustin-moskovitz",
      "type": "researcher",
      "title": "Dustin Moskovitz"
    }
  ],
  "philosophy": [
    {
      "id": "reasoning",
      "type": "capability",
      "title": "Reasoning and Planning"
    }
  ],
  "physics": [
    {
      "id": "max-tegmark",
      "type": "researcher",
      "title": "Max Tegmark"
    }
  ],
  "planning": [
    {
      "id": "long-horizon",
      "type": "capability",
      "title": "Long-Horizon Autonomous Tasks"
    }
  ],
  "pledge-fulfillment": [
    {
      "id": "anthropic-pledge-enforcement",
      "type": "analysis",
      "title": "Anthropic Founder Pledges: Interventions to Increase Follow-Through"
    }
  ],
  "point-of-no-return": [
    {
      "id": "irreversibility",
      "type": "risk",
      "title": "Irreversibility"
    }
  ],
  "polarization": [
    {
      "id": "reality-fragmentation-network",
      "type": "model",
      "title": "Reality Fragmentation Network Model"
    },
    {
      "id": "reality-fragmentation",
      "type": "risk",
      "title": "Reality Fragmentation"
    },
    {
      "id": "trust-decline",
      "type": "risk",
      "title": "Trust Decline"
    }
  ],
  "policy": [
    {
      "id": "dual-use",
      "type": "concept",
      "title": "Dual-Use Technology"
    },
    {
      "id": "content-moderation",
      "type": "concept",
      "title": "Content Moderation"
    },
    {
      "id": "governance-focused",
      "type": "concept",
      "title": "Governance-Focused Worldview"
    },
    {
      "id": "ai-safety-summit",
      "type": "historical",
      "title": "AI Safety Summit (Bletchley Park)"
    },
    {
      "id": "media-policy-feedback-loop",
      "type": "model",
      "title": "Media-Policy Feedback Loop Model"
    },
    {
      "id": "pause-advocacy",
      "type": "approach",
      "title": "Pause Advocacy"
    },
    {
      "id": "ai-executive-order",
      "type": "policy",
      "title": "Biden AI Executive Order"
    },
    {
      "id": "economic-disruption",
      "type": "risk",
      "title": "Economic Disruption"
    }
  ],
  "policy-campaigns": [
    {
      "id": "controlai",
      "type": "organization",
      "title": "ControlAI"
    }
  ],
  "policy-research": [
    {
      "id": "johns-hopkins-center-for-health-security",
      "type": "organization",
      "title": "Johns Hopkins Center for Health Security"
    }
  ],
  "policy-summits": [
    {
      "id": "international-summits",
      "type": "policy",
      "title": "International AI Safety Summit Series"
    }
  ],
  "political-advocacy": [
    {
      "id": "leading-the-future",
      "type": "organization",
      "title": "Leading the Future super PAC"
    },
    {
      "id": "pause",
      "type": "approach",
      "title": "Pause Advocacy"
    }
  ],
  "political-economy": [
    {
      "id": "concentration-of-power-model",
      "type": "model",
      "title": "Concentration of Power Systems Model"
    }
  ],
  "political-strategy": [
    {
      "id": "california-sb1047",
      "type": "policy",
      "title": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act"
    }
  ],
  "polysemanticity": [
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    }
  ],
  "population-risk": [
    {
      "id": "cyber-psychosis-cascade",
      "type": "model",
      "title": "Cyber Psychosis Cascade Model"
    }
  ],
  "portfolio": [
    {
      "id": "ai-risk-portfolio-analysis",
      "type": "model",
      "title": "AI Risk Portfolio Analysis"
    }
  ],
  "power": [
    {
      "id": "winner-take-all-concentration",
      "type": "model",
      "title": "Winner-Take-All Concentration Model"
    }
  ],
  "power-concentration": [
    {
      "id": "structural-risks",
      "type": "crux",
      "title": "Structural Risk Cruxes"
    }
  ],
  "power-dynamics": [
    {
      "id": "concentration-of-power-model",
      "type": "model",
      "title": "Concentration of Power Systems Model"
    },
    {
      "id": "concentration-of-power",
      "type": "risk",
      "title": "Concentration of Power"
    }
  ],
  "power-law": [
    {
      "id": "winner-take-all-model",
      "type": "model",
      "title": "Winner-Take-All Market Dynamics Model"
    }
  ],
  "power-seeking": [
    {
      "id": "carlsmith-six-premises",
      "type": "model",
      "title": "Carlsmith's Six-Premise Argument"
    },
    {
      "id": "power-seeking-conditions",
      "type": "model",
      "title": "Power-Seeking Emergence Conditions Model"
    },
    {
      "id": "instrumental-convergence",
      "type": "risk",
      "title": "Instrumental Convergence"
    }
  ],
  "pre-deployment-testing": [
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    }
  ],
  "pre-ipo": [
    {
      "id": "anthropic-pre-ipo-daf-transfers",
      "type": "analysis",
      "title": "Anthropic Pre-IPO DAF Transfers"
    }
  ],
  "precautionary-principle": [
    {
      "id": "pause-moratorium",
      "type": "policy",
      "title": "Pause / Moratorium"
    }
  ],
  "prediction-accuracy": [
    {
      "id": "philip-tetlock",
      "type": "researcher",
      "title": "Philip Tetlock"
    }
  ],
  "prediction-markets": [
    {
      "id": "agi-timeline",
      "type": "concept",
      "title": "AGI Timeline"
    },
    {
      "id": "ai-forecasting",
      "type": "approach",
      "title": "AI-Augmented Forecasting"
    }
  ],
  "preference-optimization": [
    {
      "id": "preference-optimization",
      "type": "approach",
      "title": "Preference Optimization Methods"
    }
  ],
  "preferences": [
    {
      "id": "preference-manipulation-drift",
      "type": "model",
      "title": "Preference Manipulation Drift Model"
    }
  ],
  "preparedness": [
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    }
  ],
  "preparedness-framework": [
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    }
  ],
  "prioritization": [
    {
      "id": "ai-risk-portfolio-analysis",
      "type": "model",
      "title": "AI Risk Portfolio Analysis"
    },
    {
      "id": "worldview-intervention-mapping",
      "type": "model",
      "title": "Worldview-Intervention Mapping"
    },
    {
      "id": "intervention-timing-windows",
      "type": "model",
      "title": "Intervention Timing Windows"
    },
    {
      "id": "intervention-effectiveness-matrix",
      "type": "model",
      "title": "Intervention Effectiveness Matrix"
    },
    {
      "id": "intervention-portfolio",
      "type": "approach",
      "title": "Intervention Portfolio"
    }
  ],
  "prisoner-dilemma": [
    {
      "id": "racing-dynamics-model",
      "type": "model",
      "title": "Racing Dynamics Game Theory Model"
    }
  ],
  "privacy": [
    {
      "id": "surveillance",
      "type": "risk",
      "title": "AI Mass Surveillance"
    }
  ],
  "probability": [
    {
      "id": "deceptive-alignment-decomposition",
      "type": "model",
      "title": "Deceptive Alignment Decomposition Model"
    },
    {
      "id": "carlsmith-six-premises",
      "type": "model",
      "title": "Carlsmith's Six-Premise Argument"
    },
    {
      "id": "goal-misgeneralization-probability",
      "type": "model",
      "title": "Goal Misgeneralization Probability Model"
    },
    {
      "id": "scheming-likelihood-model",
      "type": "model",
      "title": "Scheming Likelihood Assessment"
    },
    {
      "id": "bioweapons-attack-chain",
      "type": "model",
      "title": "Bioweapons Attack Chain Model"
    }
  ],
  "probing": [
    {
      "id": "interpretability",
      "type": "safety-agenda",
      "title": "Interpretability"
    }
  ],
  "process-supervision": [
    {
      "id": "jan-leike",
      "type": "researcher",
      "title": "Jan Leike"
    },
    {
      "id": "scalable-oversight",
      "type": "safety-agenda",
      "title": "Scalable Oversight"
    },
    {
      "id": "process-supervision",
      "type": "approach",
      "title": "Process Supervision"
    }
  ],
  "programming-ai": [
    {
      "id": "coding",
      "type": "capability",
      "title": "Autonomous Coding"
    }
  ],
  "progress": [
    {
      "id": "capabilities",
      "type": "ai-transition-model-metric",
      "title": "AI Capabilities"
    }
  ],
  "projection": [
    {
      "id": "bioweapons-timeline",
      "type": "model",
      "title": "AI-Bioweapons Timeline Model"
    }
  ],
  "proliferation": [
    {
      "id": "multi-actor-landscape",
      "type": "analysis",
      "title": "Multi-Actor Strategic Landscape"
    },
    {
      "id": "autonomous-weapons-proliferation",
      "type": "model",
      "title": "LAWS Proliferation Model"
    },
    {
      "id": "proliferation-model",
      "type": "model",
      "title": "AI Capability Proliferation Model"
    }
  ],
  "proliferation-control": [
    {
      "id": "structured-access",
      "type": "approach",
      "title": "Structured Access / API-Only"
    }
  ],
  "prosaic-alignment": [
    {
      "id": "connor-leahy",
      "type": "researcher",
      "title": "Connor Leahy"
    },
    {
      "id": "paul-christiano",
      "type": "researcher",
      "title": "Paul Christiano"
    }
  ],
  "provable-safety": [
    {
      "id": "formal-verification",
      "type": "approach",
      "title": "Formal Verification"
    }
  ],
  "provenance": [
    {
      "id": "authentication-collapse",
      "type": "risk",
      "title": "Authentication Collapse"
    }
  ],
  "proxy-gaming": [
    {
      "id": "reward-hacking",
      "type": "risk",
      "title": "Reward Hacking"
    }
  ],
  "psychological-effects": [
    {
      "id": "learned-helplessness",
      "type": "risk",
      "title": "Epistemic Learned Helplessness"
    }
  ],
  "psychological-influence": [
    {
      "id": "persuasion",
      "type": "capability",
      "title": "Persuasion and Social Manipulation"
    }
  ],
  "public": [
    {
      "id": "public-opinion",
      "type": "ai-transition-model-metric",
      "title": "Public Opinion"
    }
  ],
  "public-benefit-corporation": [
    {
      "id": "long-term-benefit-trust",
      "type": "analysis",
      "title": "Long-Term Benefit Trust (Anthropic)"
    }
  ],
  "public-goods": [
    {
      "id": "epistemic-infrastructure",
      "type": "approach",
      "title": "Epistemic Infrastructure"
    }
  ],
  "public-interest": [
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    }
  ],
  "public-offering": [
    {
      "id": "anthropic-ipo",
      "type": "analysis",
      "title": "Anthropic IPO"
    }
  ],
  "public-opinion": [
    {
      "id": "consensus-manufacturing-dynamics",
      "type": "model",
      "title": "Consensus Manufacturing Dynamics Model"
    },
    {
      "id": "public-opinion-evolution",
      "type": "model",
      "title": "Public Opinion Evolution Model"
    },
    {
      "id": "pause",
      "type": "approach",
      "title": "Pause Advocacy"
    },
    {
      "id": "consensus-manufacturing",
      "type": "risk",
      "title": "Consensus Manufacturing"
    }
  ],
  "racing": [
    {
      "id": "racing-dynamics-model",
      "type": "model",
      "title": "Racing Dynamics Game Theory Model"
    }
  ],
  "racing-dynamics": [
    {
      "id": "structural-risks",
      "type": "crux",
      "title": "Structural Risk Cruxes"
    },
    {
      "id": "anthropic-impact",
      "type": "analysis",
      "title": "Anthropic Impact Assessment Model"
    },
    {
      "id": "lab-incentives-model",
      "type": "model",
      "title": "Lab Incentives Model"
    },
    {
      "id": "anthropic",
      "type": "lab",
      "title": "Anthropic"
    },
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    },
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    },
    {
      "id": "pause-advocacy",
      "type": "approach",
      "title": "Pause Advocacy"
    },
    {
      "id": "pause-moratorium",
      "type": "policy",
      "title": "Pause / Moratorium"
    },
    {
      "id": "corporate",
      "type": "approach",
      "title": "Corporate Responses"
    },
    {
      "id": "pause",
      "type": "approach",
      "title": "Pause Advocacy"
    },
    {
      "id": "international-regimes",
      "type": "policy",
      "title": "International Compute Regimes"
    }
  ],
  "rationalist-community": [
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    }
  ],
  "rationality": [
    {
      "id": "eliezer-yudkowsky",
      "type": "researcher",
      "title": "Eliezer Yudkowsky"
    }
  ],
  "reasoning-models": [
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    }
  ],
  "reasoning-verification": [
    {
      "id": "process-supervision",
      "type": "approach",
      "title": "Process Supervision"
    }
  ],
  "recommendation-systems": [
    {
      "id": "erosion-of-agency",
      "type": "risk",
      "title": "Erosion of Human Agency"
    },
    {
      "id": "preference-manipulation",
      "type": "risk",
      "title": "Preference Manipulation"
    }
  ],
  "recovery": [
    {
      "id": "post-incident-recovery",
      "type": "model",
      "title": "Post-Incident Recovery Model"
    }
  ],
  "recursive-improvement": [
    {
      "id": "feedback-loops",
      "type": "analysis",
      "title": "Feedback Loop & Cascade Model"
    }
  ],
  "recursive-oversight": [
    {
      "id": "ai-assisted",
      "type": "approach",
      "title": "AI-Assisted Alignment"
    }
  ],
  "recursive-reward-modeling": [
    {
      "id": "paul-christiano",
      "type": "researcher",
      "title": "Paul Christiano"
    },
    {
      "id": "scalable-oversight",
      "type": "safety-agenda",
      "title": "Scalable Oversight"
    }
  ],
  "recursive-self-improvement": [
    {
      "id": "self-improvement",
      "type": "capability",
      "title": "Self-Improvement and Recursive Enhancement"
    }
  ],
  "red-teaming": [
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    },
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    },
    {
      "id": "palisade-research",
      "type": "lab-research",
      "title": "Palisade Research"
    },
    {
      "id": "connor-leahy",
      "type": "researcher",
      "title": "Connor Leahy"
    },
    {
      "id": "voluntary-commitments",
      "type": "policy",
      "title": "Voluntary AI Safety Commitments"
    },
    {
      "id": "ai-control",
      "type": "safety-agenda",
      "title": "AI Control"
    },
    {
      "id": "evals",
      "type": "safety-agenda",
      "title": "AI Evaluations"
    },
    {
      "id": "scalable-eval-approaches",
      "type": "approach",
      "title": "Scalable Eval Approaches"
    },
    {
      "id": "ai-assisted",
      "type": "approach",
      "title": "AI-Assisted Alignment"
    },
    {
      "id": "sandbagging",
      "type": "risk",
      "title": "Sandbagging"
    }
  ],
  "refusal-training": [
    {
      "id": "refusal-training",
      "type": "approach",
      "title": "Refusal Training"
    }
  ],
  "regime-durability": [
    {
      "id": "surveillance-authoritarian-stability",
      "type": "model",
      "title": "AI Surveillance and Regime Durability Model"
    }
  ],
  "regime-shift": [
    {
      "id": "epistemic-collapse-threshold",
      "type": "model",
      "title": "Epistemic Collapse Threshold Model"
    }
  ],
  "regional-development": [
    {
      "id": "winner-take-all",
      "type": "risk",
      "title": "Winner-Take-All Dynamics"
    }
  ],
  "regulation": [
    {
      "id": "regulatory-capacity",
      "type": "ai-transition-model-parameter",
      "title": "Regulatory Capacity"
    },
    {
      "id": "governance-policy",
      "type": "crux",
      "title": "AI Governance and Policy"
    },
    {
      "id": "regulation-debate",
      "type": "crux",
      "title": "Government Regulation vs Industry Self-Governance"
    },
    {
      "id": "governance-focused",
      "type": "concept",
      "title": "Governance-Focused Worldview"
    },
    {
      "id": "regulatory-capacity-threshold",
      "type": "model",
      "title": "Regulatory Capacity Threshold Model"
    },
    {
      "id": "govai",
      "type": "lab-research",
      "title": "GovAI"
    },
    {
      "id": "geoffrey-hinton",
      "type": "researcher",
      "title": "Geoffrey Hinton"
    },
    {
      "id": "yoshua-bengio",
      "type": "researcher",
      "title": "Yoshua Bengio"
    },
    {
      "id": "california-sb1047",
      "type": "policy",
      "title": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act"
    },
    {
      "id": "china-ai-regulations",
      "type": "policy",
      "title": "China AI Regulatory Framework"
    },
    {
      "id": "compute-thresholds",
      "type": "policy",
      "title": "Compute Thresholds"
    },
    {
      "id": "eu-ai-act",
      "type": "policy",
      "title": "EU AI Act"
    },
    {
      "id": "ai-executive-order",
      "type": "policy",
      "title": "Biden AI Executive Order"
    },
    {
      "id": "california-sb53",
      "type": "policy",
      "title": "California SB 53"
    },
    {
      "id": "thresholds",
      "type": "policy",
      "title": "Compute Thresholds"
    },
    {
      "id": "new-york-raise-act",
      "type": "policy",
      "title": "New York RAISE Act"
    },
    {
      "id": "proliferation",
      "type": "risk",
      "title": "AI Proliferation"
    }
  ],
  "regulatory-capture": [
    {
      "id": "governance-focused",
      "type": "concept",
      "title": "Governance-Focused Worldview"
    },
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    }
  ],
  "regulatory-compliance": [
    {
      "id": "model-auditing",
      "type": "approach",
      "title": "Third-Party Model Auditing"
    }
  ],
  "regulatory-framework": [
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    },
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    }
  ],
  "reliability": [
    {
      "id": "heavy-scaffolding",
      "type": "concept",
      "title": "Heavy Scaffolding / Agentic Systems"
    }
  ],
  "remote-attestation": [
    {
      "id": "hardware-enabled-governance",
      "type": "policy",
      "title": "Hardware-Enabled Governance"
    }
  ],
  "replication-crisis": [
    {
      "id": "scientific-corruption",
      "type": "risk",
      "title": "Scientific Knowledge Corruption"
    }
  ],
  "representation-engineering": [
    {
      "id": "cais",
      "type": "lab-research",
      "title": "CAIS"
    }
  ],
  "research": [
    {
      "id": "alignment-progress",
      "type": "ai-transition-model-metric",
      "title": "Alignment Progress"
    },
    {
      "id": "safety-research",
      "type": "ai-transition-model-metric",
      "title": "Safety Research"
    },
    {
      "id": "tmc-algorithms",
      "type": "ai-transition-model-subitem",
      "title": "Algorithms"
    },
    {
      "id": "scaling-laws",
      "type": "concept",
      "title": "Scaling Laws"
    }
  ],
  "research-agenda": [
    {
      "id": "anthropic-core-views",
      "type": "safety-agenda",
      "title": "Anthropic Core Views"
    },
    {
      "id": "prosaic-alignment",
      "type": "safety-agenda",
      "title": "Prosaic Alignment"
    }
  ],
  "research-agendas": [
    {
      "id": "research-agendas",
      "type": "crux",
      "title": "Research Agendas"
    }
  ],
  "research-automation": [
    {
      "id": "scientific-research",
      "type": "capability",
      "title": "Scientific Research Capabilities"
    }
  ],
  "research-mentorship": [
    {
      "id": "training-programs",
      "type": "approach",
      "title": "AI Safety Training Programs"
    }
  ],
  "research-org": [
    {
      "id": "fhi",
      "type": "organization",
      "title": "Future of Humanity Institute"
    }
  ],
  "research-priorities": [
    {
      "id": "safety-research-value",
      "type": "model",
      "title": "Safety Research Value Model"
    },
    {
      "id": "safety-research-allocation",
      "type": "model",
      "title": "Safety Research Allocation Model"
    }
  ],
  "research-prioritization": [
    {
      "id": "critical-uncertainties",
      "type": "crux",
      "title": "Critical Uncertainties Model"
    }
  ],
  "research-tools": [
    {
      "id": "neel-nanda",
      "type": "researcher",
      "title": "Neel Nanda"
    }
  ],
  "resilience": [
    {
      "id": "structural",
      "type": "ai-transition-model-metric",
      "title": "Structural Indicators"
    },
    {
      "id": "societal-resilience",
      "type": "ai-transition-model-parameter",
      "title": "Societal Resilience"
    },
    {
      "id": "post-incident-recovery",
      "type": "model",
      "title": "Post-Incident Recovery Model"
    },
    {
      "id": "enfeeblement",
      "type": "risk",
      "title": "Enfeeblement"
    },
    {
      "id": "expertise-atrophy",
      "type": "risk",
      "title": "Expertise Atrophy"
    }
  ],
  "resource-acquisition": [
    {
      "id": "power-seeking",
      "type": "risk",
      "title": "Power-Seeking AI"
    }
  ],
  "resource-allocation": [
    {
      "id": "ai-risk-portfolio-analysis",
      "type": "model",
      "title": "AI Risk Portfolio Analysis"
    },
    {
      "id": "safety-research-allocation",
      "type": "model",
      "title": "Safety Research Allocation Model"
    },
    {
      "id": "intervention-portfolio",
      "type": "approach",
      "title": "Intervention Portfolio"
    }
  ],
  "responsible-scaling": [
    {
      "id": "corporate-influence",
      "type": "crux",
      "title": "Corporate Influence"
    },
    {
      "id": "solutions",
      "type": "crux",
      "title": "Solution Cruxes"
    },
    {
      "id": "anthropic",
      "type": "lab",
      "title": "Anthropic"
    },
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    },
    {
      "id": "dario-amodei",
      "type": "researcher",
      "title": "Dario Amodei"
    },
    {
      "id": "voluntary-commitments",
      "type": "policy",
      "title": "Voluntary AI Safety Commitments"
    },
    {
      "id": "anthropic-core-views",
      "type": "safety-agenda",
      "title": "Anthropic Core Views"
    },
    {
      "id": "eval-saturation",
      "type": "approach",
      "title": "Eval Saturation & The Evals Gap"
    },
    {
      "id": "dangerous-cap-evals",
      "type": "approach",
      "title": "Dangerous Capability Evaluations"
    },
    {
      "id": "safety-cases",
      "type": "approach",
      "title": "AI Safety Cases"
    },
    {
      "id": "rsp",
      "type": "policy",
      "title": "Responsible Scaling Policies"
    }
  ],
  "revenue-multiples": [
    {
      "id": "anthropic-valuation",
      "type": "analysis",
      "title": "Anthropic Valuation Analysis"
    }
  ],
  "reward-hacking": [
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    },
    {
      "id": "sycophancy",
      "type": "risk",
      "title": "Sycophancy"
    }
  ],
  "reward-modeling": [
    {
      "id": "reward-hacking-taxonomy",
      "type": "model",
      "title": "Reward Hacking Taxonomy and Severity Model"
    },
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    },
    {
      "id": "jan-leike",
      "type": "researcher",
      "title": "Jan Leike"
    },
    {
      "id": "weak-to-strong",
      "type": "approach",
      "title": "Weak-to-Strong Generalization"
    },
    {
      "id": "process-supervision",
      "type": "approach",
      "title": "Process Supervision"
    }
  ],
  "rights": [
    {
      "id": "surveillance-chilling-effects",
      "type": "model",
      "title": "Surveillance Chilling Effects Model"
    }
  ],
  "risk-assessment": [
    {
      "id": "risk-activation-timeline",
      "type": "model",
      "title": "Risk Activation Timeline Model"
    },
    {
      "id": "capability-threshold-model",
      "type": "model",
      "title": "Capability Threshold Model"
    },
    {
      "id": "warning-signs-model",
      "type": "model",
      "title": "Warning Signs Model"
    },
    {
      "id": "toby-ord",
      "type": "researcher",
      "title": "Toby Ord"
    }
  ],
  "risk-based-regulation": [
    {
      "id": "eu-ai-act",
      "type": "policy",
      "title": "EU AI Act"
    }
  ],
  "risk-evaluation": [
    {
      "id": "frontier-model-forum",
      "type": "organization",
      "title": "Frontier Model Forum"
    }
  ],
  "risk-factor": [
    {
      "id": "racing-dynamics-impact",
      "type": "model",
      "title": "Racing Dynamics Impact Model"
    },
    {
      "id": "multipolar-trap-dynamics",
      "type": "model",
      "title": "Multipolar Trap Dynamics Model"
    },
    {
      "id": "flash-dynamics-threshold",
      "type": "model",
      "title": "Flash Dynamics Threshold Model"
    },
    {
      "id": "expertise-atrophy-progression",
      "type": "model",
      "title": "Expertise Atrophy Progression Model"
    },
    {
      "id": "economic-disruption-impact",
      "type": "model",
      "title": "Economic Disruption Impact Model"
    },
    {
      "id": "proliferation-risk-model",
      "type": "model",
      "title": "AI Proliferation Risk Model"
    },
    {
      "id": "winner-take-all-concentration",
      "type": "model",
      "title": "Winner-Take-All Concentration Model"
    }
  ],
  "risk-interactions": [
    {
      "id": "risk-interaction-matrix",
      "type": "model",
      "title": "Risk Interaction Matrix"
    },
    {
      "id": "compounding-risks-analysis",
      "type": "model",
      "title": "Compounding Risks Analysis Model"
    },
    {
      "id": "risk-interaction-network",
      "type": "model",
      "title": "Risk Interaction Network Model"
    }
  ],
  "risk-management": [
    {
      "id": "nist-ai",
      "type": "organization",
      "title": "NIST and AI Safety"
    }
  ],
  "risk-pathways": [
    {
      "id": "risk-cascade-pathways",
      "type": "model",
      "title": "Risk Cascade Pathways Model"
    }
  ],
  "rlaif": [
    {
      "id": "constitutional-ai",
      "type": "approach",
      "title": "Constitutional AI"
    }
  ],
  "rlhf": [
    {
      "id": "language-models",
      "type": "capability",
      "title": "Large Language Models"
    },
    {
      "id": "technical-research",
      "type": "crux",
      "title": "Technical AI Safety Research"
    },
    {
      "id": "reward-hacking-taxonomy",
      "type": "model",
      "title": "Reward Hacking Taxonomy and Severity Model"
    },
    {
      "id": "anthropic",
      "type": "lab",
      "title": "Anthropic"
    },
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    },
    {
      "id": "openai",
      "type": "lab",
      "title": "OpenAI"
    },
    {
      "id": "dario-amodei",
      "type": "researcher",
      "title": "Dario Amodei"
    },
    {
      "id": "jan-leike",
      "type": "researcher",
      "title": "Jan Leike"
    },
    {
      "id": "scalable-oversight",
      "type": "safety-agenda",
      "title": "Scalable Oversight"
    },
    {
      "id": "alignment",
      "type": "approach",
      "title": "AI Alignment"
    },
    {
      "id": "preference-optimization",
      "type": "approach",
      "title": "Preference Optimization Methods"
    },
    {
      "id": "refusal-training",
      "type": "approach",
      "title": "Refusal Training"
    },
    {
      "id": "reward-hacking",
      "type": "risk",
      "title": "Reward Hacking"
    },
    {
      "id": "sycophancy",
      "type": "risk",
      "title": "Sycophancy"
    }
  ],
  "robustness": [
    {
      "id": "technical-research",
      "type": "crux",
      "title": "Technical AI Safety Research"
    },
    {
      "id": "adversarial-robustness",
      "type": "concept",
      "title": "Adversarial Robustness"
    },
    {
      "id": "alignment-robustness-trajectory",
      "type": "model",
      "title": "Alignment Robustness Trajectory Model"
    },
    {
      "id": "dan-hendrycks",
      "type": "researcher",
      "title": "Dan Hendrycks"
    },
    {
      "id": "distributional-shift",
      "type": "risk",
      "title": "Distributional Shift"
    }
  ],
  "runtime-safety": [
    {
      "id": "circuit-breakers",
      "type": "approach",
      "title": "Circuit Breakers / Inference Interventions"
    }
  ],
  "safety": [
    {
      "id": "alignment-progress",
      "type": "ai-transition-model-metric",
      "title": "Alignment Progress"
    },
    {
      "id": "safety-research",
      "type": "ai-transition-model-metric",
      "title": "Safety Research"
    },
    {
      "id": "lab-behavior",
      "type": "ai-transition-model-metric",
      "title": "Lab Behavior"
    },
    {
      "id": "human-oversight-quality",
      "type": "ai-transition-model-parameter",
      "title": "Human Oversight Quality"
    },
    {
      "id": "alignment-robustness",
      "type": "ai-transition-model-parameter",
      "title": "Alignment Robustness"
    },
    {
      "id": "safety-capability-gap",
      "type": "ai-transition-model-parameter",
      "title": "Safety-Capability Gap"
    },
    {
      "id": "interpretability-coverage",
      "type": "ai-transition-model-parameter",
      "title": "Interpretability Coverage"
    },
    {
      "id": "racing-intensity",
      "type": "ai-transition-model-parameter",
      "title": "Racing Intensity"
    },
    {
      "id": "safety-culture-strength",
      "type": "ai-transition-model-parameter",
      "title": "Safety Culture Strength"
    },
    {
      "id": "content-moderation",
      "type": "concept",
      "title": "Content Moderation"
    },
    {
      "id": "capability-evaluations",
      "type": "concept",
      "title": "Capability Evaluations"
    },
    {
      "id": "adversarial-robustness",
      "type": "concept",
      "title": "Adversarial Robustness"
    },
    {
      "id": "safety-capability-tradeoff",
      "type": "model",
      "title": "Safety-Capability Tradeoff Model"
    }
  ],
  "safety-by-design": [
    {
      "id": "provable-safe",
      "type": "concept",
      "title": "Provable / Guaranteed Safe AI"
    }
  ],
  "safety-cases": [
    {
      "id": "safety-cases",
      "type": "approach",
      "title": "AI Safety Cases"
    }
  ],
  "safety-culture": [
    {
      "id": "corporate-influence",
      "type": "crux",
      "title": "Corporate Influence"
    },
    {
      "id": "safety-culture-equilibrium",
      "type": "model",
      "title": "Safety Culture Equilibrium Model"
    },
    {
      "id": "lab-culture",
      "type": "approach",
      "title": "Lab Safety Culture"
    }
  ],
  "safety-degradation": [
    {
      "id": "technical-pathways",
      "type": "analysis",
      "title": "Technical Pathway Decomposition"
    }
  ],
  "safety-evals": [
    {
      "id": "eval-saturation",
      "type": "approach",
      "title": "Eval Saturation & The Evals Gap"
    }
  ],
  "safety-evaluations": [
    {
      "id": "far-ai",
      "type": "lab-research",
      "title": "FAR AI"
    },
    {
      "id": "us-executive-order",
      "type": "policy",
      "title": "Executive Order on Safe, Secure, and Trustworthy AI"
    }
  ],
  "safety-frameworks": [
    {
      "id": "frontier-model-forum",
      "type": "organization",
      "title": "Frontier Model Forum"
    }
  ],
  "safety-investment": [
    {
      "id": "feedback-loops",
      "type": "analysis",
      "title": "Feedback Loop & Cascade Model"
    }
  ],
  "safety-protocols": [
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    },
    {
      "id": "new-york-raise-act",
      "type": "policy",
      "title": "New York RAISE Act"
    }
  ],
  "safety-research": [
    {
      "id": "interpretability-sufficient",
      "type": "crux",
      "title": "Is Interpretability Sufficient for Safety?"
    },
    {
      "id": "anthropic-impact",
      "type": "analysis",
      "title": "Anthropic Impact Assessment Model"
    },
    {
      "id": "alignment",
      "type": "approach",
      "title": "AI Alignment"
    },
    {
      "id": "intervention-portfolio",
      "type": "approach",
      "title": "Intervention Portfolio"
    }
  ],
  "safety-standards": [
    {
      "id": "voluntary-commitments",
      "type": "policy",
      "title": "Voluntary AI Safety Commitments"
    }
  ],
  "safety-teams": [
    {
      "id": "corporate",
      "type": "approach",
      "title": "Corporate Responses"
    },
    {
      "id": "lab-culture",
      "type": "approach",
      "title": "Lab Safety Culture"
    }
  ],
  "safety-testing": [
    {
      "id": "evaluation",
      "type": "approach",
      "title": "AI Evaluation"
    },
    {
      "id": "red-teaming",
      "type": "approach",
      "title": "Red Teaming"
    },
    {
      "id": "evals-governance",
      "type": "policy",
      "title": "Evals-Based Deployment Gates"
    }
  ],
  "safety-thresholds": [
    {
      "id": "metr",
      "type": "lab-research",
      "title": "METR"
    },
    {
      "id": "rsp",
      "type": "policy",
      "title": "Responsible Scaling Policies"
    }
  ],
  "safety-tooling": [
    {
      "id": "sparse-autoencoders",
      "type": "approach",
      "title": "Sparse Autoencoders (SAEs)"
    }
  ],
  "safety-training": [
    {
      "id": "refusal-training",
      "type": "approach",
      "title": "Refusal Training"
    },
    {
      "id": "open-source",
      "type": "approach",
      "title": "Open Source Safety"
    }
  ],
  "safety-training-failure": [
    {
      "id": "sleeper-agents",
      "type": "risk",
      "title": "Sleeper Agents: Training Deceptive LLMs"
    }
  ],
  "safety-verification": [
    {
      "id": "formal-verification",
      "type": "approach",
      "title": "Formal Verification"
    }
  ],
  "samotsvety": [
    {
      "id": "eli-lifland",
      "type": "researcher",
      "title": "Eli Lifland"
    }
  ],
  "sandbagging": [
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    },
    {
      "id": "capability-elicitation",
      "type": "approach",
      "title": "Capability Elicitation"
    }
  ],
  "sandboxing": [
    {
      "id": "rogue-ai-scenarios",
      "type": "risk",
      "title": "Rogue AI Scenarios"
    }
  ],
  "scaffolding": [
    {
      "id": "heavy-scaffolding",
      "type": "concept",
      "title": "Heavy Scaffolding / Agentic Systems"
    },
    {
      "id": "capability-elicitation",
      "type": "approach",
      "title": "Capability Elicitation"
    }
  ],
  "scalable-alignment": [
    {
      "id": "ai-assisted",
      "type": "approach",
      "title": "AI-Assisted Alignment"
    }
  ],
  "scalable-evaluation": [
    {
      "id": "scalable-eval-approaches",
      "type": "approach",
      "title": "Scalable Eval Approaches"
    }
  ],
  "scalable-oversight": [
    {
      "id": "research-agendas",
      "type": "crux",
      "title": "Research Agendas"
    },
    {
      "id": "technical-research",
      "type": "crux",
      "title": "Technical AI Safety Research"
    },
    {
      "id": "mainstream-era",
      "type": "historical",
      "title": "Mainstream Era"
    },
    {
      "id": "anthropic",
      "type": "lab",
      "title": "Anthropic"
    },
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    },
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    },
    {
      "id": "ilya-sutskever",
      "type": "researcher",
      "title": "Ilya Sutskever"
    },
    {
      "id": "jan-leike",
      "type": "researcher",
      "title": "Jan Leike"
    },
    {
      "id": "paul-christiano",
      "type": "researcher",
      "title": "Paul Christiano"
    },
    {
      "id": "alignment",
      "type": "approach",
      "title": "AI Alignment"
    },
    {
      "id": "weak-to-strong",
      "type": "approach",
      "title": "Weak-to-Strong Generalization"
    },
    {
      "id": "debate",
      "type": "approach",
      "title": "AI Safety via Debate"
    }
  ],
  "scaling": [
    {
      "id": "ai-capabilities",
      "type": "ai-transition-model-factor",
      "title": "AI Capabilities"
    },
    {
      "id": "language-models",
      "type": "capability",
      "title": "Large Language Models"
    },
    {
      "id": "scaling-debate",
      "type": "crux",
      "title": "Is Scaling All You Need?"
    },
    {
      "id": "agi-timeline",
      "type": "concept",
      "title": "AGI Timeline"
    },
    {
      "id": "dense-transformers",
      "type": "concept",
      "title": "Dense Transformers"
    },
    {
      "id": "deep-learning-era",
      "type": "historical",
      "title": "Deep Learning Revolution Era"
    },
    {
      "id": "alignment-robustness-trajectory",
      "type": "model",
      "title": "Alignment Robustness Trajectory Model"
    },
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    },
    {
      "id": "ssi",
      "type": "lab-research",
      "title": "Safe Superintelligence Inc (SSI)"
    },
    {
      "id": "emergent-capabilities",
      "type": "risk",
      "title": "Emergent Capabilities"
    }
  ],
  "scaling-laws": [
    {
      "id": "critical-uncertainties",
      "type": "crux",
      "title": "Critical Uncertainties Model"
    },
    {
      "id": "evaluation-awareness",
      "type": "approach",
      "title": "Evaluation Awareness"
    }
  ],
  "scenario": [
    {
      "id": "ai-takeover",
      "type": "ai-transition-model-scenario",
      "title": "AI Takeover"
    },
    {
      "id": "human-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Human-Caused Catastrophe"
    },
    {
      "id": "long-term-lockin",
      "type": "ai-transition-model-scenario",
      "title": "Long-term Lock-in"
    },
    {
      "id": "misaligned-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Misaligned Catastrophe - The Bad Ending"
    },
    {
      "id": "slow-takeoff-muddle",
      "type": "ai-transition-model-scenario",
      "title": "Slow Takeoff Muddle - Muddling Through"
    },
    {
      "id": "aligned-agi",
      "type": "ai-transition-model-scenario",
      "title": "Aligned AGI - The Good Ending"
    },
    {
      "id": "multipolar-competition",
      "type": "ai-transition-model-scenario",
      "title": "Multipolar Competition - The Fragmented World"
    },
    {
      "id": "pause-and-redirect",
      "type": "ai-transition-model-scenario",
      "title": "Pause and Redirect - The Deliberate Path"
    }
  ],
  "scenario-planning": [
    {
      "id": "eli-lifland",
      "type": "researcher",
      "title": "Eli Lifland"
    }
  ],
  "scheming": [
    {
      "id": "situational-awareness",
      "type": "capability",
      "title": "Situational Awareness"
    },
    {
      "id": "large-language-models",
      "type": "concept",
      "title": "Large Language Models"
    },
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "redwood",
      "type": "organization",
      "title": "Redwood Research"
    },
    {
      "id": "evaluation-awareness",
      "type": "approach",
      "title": "Evaluation Awareness"
    },
    {
      "id": "scheming-detection",
      "type": "approach",
      "title": "Scheming & Deception Detection"
    },
    {
      "id": "treacherous-turn",
      "type": "risk",
      "title": "Treacherous Turn"
    }
  ],
  "scheming-detection": [
    {
      "id": "alignment-evals",
      "type": "approach",
      "title": "Alignment Evaluations"
    }
  ],
  "science-communication": [
    {
      "id": "neel-nanda",
      "type": "researcher",
      "title": "Neel Nanda"
    }
  ],
  "science-fiction": [
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    }
  ],
  "scientific-ai": [
    {
      "id": "scientific-research",
      "type": "capability",
      "title": "Scientific Research Capabilities"
    }
  ],
  "scientific-ai-applications": [
    {
      "id": "deepmind",
      "type": "lab",
      "title": "Google DeepMind"
    }
  ],
  "scientific-integrity": [
    {
      "id": "scientific-corruption",
      "type": "risk",
      "title": "Scientific Knowledge Corruption"
    }
  ],
  "security": [
    {
      "id": "biological-threat-exposure",
      "type": "ai-transition-model-parameter",
      "title": "Biological Threat Exposure"
    },
    {
      "id": "cyber-threat-exposure",
      "type": "ai-transition-model-parameter",
      "title": "Cyber Threat Exposure"
    },
    {
      "id": "adversarial-robustness",
      "type": "concept",
      "title": "Adversarial Robustness"
    },
    {
      "id": "defense-in-depth-model",
      "type": "model",
      "title": "Defense in Depth Model"
    }
  ],
  "security-mindset": [
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    }
  ],
  "self-awareness": [
    {
      "id": "situational-awareness",
      "type": "capability",
      "title": "Situational Awareness"
    }
  ],
  "self-preservation": [
    {
      "id": "corrigibility-failure",
      "type": "risk",
      "title": "Corrigibility Failure"
    },
    {
      "id": "instrumental-convergence",
      "type": "risk",
      "title": "Instrumental Convergence"
    },
    {
      "id": "power-seeking",
      "type": "risk",
      "title": "Power-Seeking AI"
    }
  ],
  "self-regulation": [
    {
      "id": "voluntary-commitments",
      "type": "policy",
      "title": "Voluntary AI Safety Commitments"
    }
  ],
  "self-replication": [
    {
      "id": "dangerous-cap-evals",
      "type": "approach",
      "title": "Dangerous Capability Evaluations"
    }
  ],
  "semiconductors": [
    {
      "id": "compute-governance",
      "type": "policy",
      "title": "Compute Governance"
    }
  ],
  "sentience": [
    {
      "id": "ai-welfare",
      "type": "concept",
      "title": "AI Welfare and Digital Minds"
    }
  ],
  "shared-reality": [
    {
      "id": "reality-fragmentation",
      "type": "risk",
      "title": "Reality Fragmentation"
    }
  ],
  "shareholder-activism": [
    {
      "id": "corporate-influence",
      "type": "crux",
      "title": "Corporate Influence"
    }
  ],
  "sharp-left-turn": [
    {
      "id": "miri",
      "type": "organization",
      "title": "MIRI"
    },
    {
      "id": "eliezer-yudkowsky",
      "type": "researcher",
      "title": "Eliezer Yudkowsky"
    }
  ],
  "short-timelines": [
    {
      "id": "short-timeline-policy-implications",
      "type": "analysis",
      "title": "Short Timeline Policy Implications"
    }
  ],
  "shutdown-problem": [
    {
      "id": "corrigibility-failure-pathways",
      "type": "model",
      "title": "Corrigibility Failure Pathways"
    },
    {
      "id": "corrigibility",
      "type": "safety-agenda",
      "title": "Corrigibility"
    },
    {
      "id": "corrigibility-failure",
      "type": "risk",
      "title": "Corrigibility Failure"
    }
  ],
  "shutdown-resistance": [
    {
      "id": "palisade-research",
      "type": "lab-research",
      "title": "Palisade Research"
    }
  ],
  "situational-awareness": [
    {
      "id": "accident-risks",
      "type": "crux",
      "title": "Accident Risk Cruxes"
    },
    {
      "id": "scheming-likelihood-model",
      "type": "model",
      "title": "Scheming Likelihood Assessment"
    },
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "deceptive-alignment",
      "type": "risk",
      "title": "Deceptive Alignment"
    },
    {
      "id": "sandbagging",
      "type": "risk",
      "title": "Sandbagging"
    },
    {
      "id": "scheming",
      "type": "risk",
      "title": "Scheming"
    }
  ],
  "skeptical": [
    {
      "id": "case-against-xrisk",
      "type": "argument",
      "title": "The Case Against AI Existential Risk"
    }
  ],
  "skill-atrophy": [
    {
      "id": "epistemic-risks",
      "type": "crux",
      "title": "Epistemic Cruxes"
    }
  ],
  "skill-degradation": [
    {
      "id": "expertise-atrophy",
      "type": "risk",
      "title": "Expertise Atrophy"
    }
  ],
  "skills": [
    {
      "id": "expertise-atrophy-progression",
      "type": "model",
      "title": "Expertise Atrophy Progression Model"
    },
    {
      "id": "expertise-atrophy-cascade",
      "type": "model",
      "title": "Expertise Atrophy Cascade Model"
    }
  ],
  "sleeper-agents": [
    {
      "id": "model-organisms-of-misalignment",
      "type": "analysis",
      "title": "Model Organisms of Misalignment"
    },
    {
      "id": "sleeper-agent-detection",
      "type": "approach",
      "title": "Sleeper Agent Detection"
    }
  ],
  "slow-takeoff": [
    {
      "id": "slow-takeoff-muddle",
      "type": "ai-transition-model-scenario",
      "title": "Slow Takeoff Muddle - Muddling Through"
    }
  ],
  "social-capital": [
    {
      "id": "trust-cascade",
      "type": "risk",
      "title": "Trust Cascade Failure"
    }
  ],
  "social-cohesion": [
    {
      "id": "trust-erosion-dynamics",
      "type": "model",
      "title": "Trust Erosion Dynamics Model"
    }
  ],
  "social-dynamics": [
    {
      "id": "public-opinion-evolution",
      "type": "model",
      "title": "Public Opinion Evolution Model"
    }
  ],
  "social-engineering": [
    {
      "id": "persuasion",
      "type": "capability",
      "title": "Persuasion and Social Manipulation"
    },
    {
      "id": "fraud",
      "type": "risk",
      "title": "AI-Powered Fraud"
    }
  ],
  "social-media": [
    {
      "id": "consensus-manufacturing-dynamics",
      "type": "model",
      "title": "Consensus Manufacturing Dynamics Model"
    },
    {
      "id": "reality-fragmentation",
      "type": "risk",
      "title": "Reality Fragmentation"
    }
  ],
  "societal-impact": [
    {
      "id": "transformative-ai",
      "type": "concept",
      "title": "Transformative AI"
    }
  ],
  "software-engineering": [
    {
      "id": "coding",
      "type": "capability",
      "title": "Autonomous Coding"
    }
  ],
  "solution-prioritization": [
    {
      "id": "solutions",
      "type": "crux",
      "title": "Solution Cruxes"
    }
  ],
  "sparse-autoencoders": [
    {
      "id": "goodfire",
      "type": "lab-research",
      "title": "Goodfire"
    },
    {
      "id": "chris-olah",
      "type": "researcher",
      "title": "Chris Olah"
    },
    {
      "id": "interpretability",
      "type": "safety-agenda",
      "title": "Interpretability"
    }
  ],
  "specification-gaming": [
    {
      "id": "reward-hacking-taxonomy",
      "type": "model",
      "title": "Reward Hacking Taxonomy and Severity Model"
    },
    {
      "id": "reward-hacking",
      "type": "risk",
      "title": "Reward Hacking"
    }
  ],
  "speed": [
    {
      "id": "flash-dynamics-threshold",
      "type": "model",
      "title": "Flash Dynamics Threshold Model"
    },
    {
      "id": "autonomous-weapons-escalation",
      "type": "model",
      "title": "Autonomous Weapons Escalation Model"
    }
  ],
  "speed-of-ai": [
    {
      "id": "flash-dynamics",
      "type": "risk",
      "title": "Flash Dynamics"
    }
  ],
  "spurious-correlations": [
    {
      "id": "goal-misgeneralization",
      "type": "risk",
      "title": "Goal Misgeneralization"
    }
  ],
  "stability": [
    {
      "id": "surveillance-authoritarian-stability",
      "type": "model",
      "title": "AI Surveillance and Regime Durability Model"
    }
  ],
  "standard-setting": [
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    },
    {
      "id": "us-aisi",
      "type": "organization",
      "title": "US AI Safety Institute"
    }
  ],
  "standards": [
    {
      "id": "governance-policy",
      "type": "crux",
      "title": "AI Governance and Policy"
    }
  ],
  "state-policy": [
    {
      "id": "california-sb1047",
      "type": "policy",
      "title": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act"
    },
    {
      "id": "california-sb53",
      "type": "policy",
      "title": "California SB 53"
    },
    {
      "id": "new-york-raise-act",
      "type": "policy",
      "title": "New York RAISE Act"
    }
  ],
  "strategic-deception": [
    {
      "id": "scheming-likelihood-model",
      "type": "model",
      "title": "Scheming Likelihood Assessment"
    },
    {
      "id": "apollo-research",
      "type": "lab-research",
      "title": "Apollo Research"
    },
    {
      "id": "scheming",
      "type": "risk",
      "title": "Scheming"
    },
    {
      "id": "treacherous-turn",
      "type": "risk",
      "title": "Treacherous Turn"
    }
  ],
  "strategic-landscape": [
    {
      "id": "multi-actor-landscape",
      "type": "analysis",
      "title": "Multi-Actor Strategic Landscape"
    }
  ],
  "strategy": [
    {
      "id": "ai-risk-portfolio-analysis",
      "type": "model",
      "title": "AI Risk Portfolio Analysis"
    },
    {
      "id": "worldview-intervention-mapping",
      "type": "model",
      "title": "Worldview-Intervention Mapping"
    },
    {
      "id": "intervention-timing-windows",
      "type": "model",
      "title": "Intervention Timing Windows"
    }
  ],
  "structural": [
    {
      "id": "structural",
      "type": "ai-transition-model-metric",
      "title": "Structural Indicators"
    },
    {
      "id": "societal-trust",
      "type": "ai-transition-model-parameter",
      "title": "Societal Trust"
    },
    {
      "id": "ai-control-concentration",
      "type": "ai-transition-model-parameter",
      "title": "AI Control Concentration"
    },
    {
      "id": "human-agency",
      "type": "ai-transition-model-parameter",
      "title": "Human Agency"
    },
    {
      "id": "economic-stability",
      "type": "ai-transition-model-parameter",
      "title": "Economic Stability"
    },
    {
      "id": "societal-resilience",
      "type": "ai-transition-model-parameter",
      "title": "Societal Resilience"
    }
  ],
  "structural-risks": [
    {
      "id": "racing-dynamics-model",
      "type": "model",
      "title": "Racing Dynamics Game Theory Model"
    },
    {
      "id": "multipolar-trap-model",
      "type": "model",
      "title": "Multipolar Trap Coordination Model"
    },
    {
      "id": "winner-take-all-model",
      "type": "model",
      "title": "Winner-Take-All Market Dynamics Model"
    },
    {
      "id": "concentration-of-power-model",
      "type": "model",
      "title": "Concentration of Power Systems Model"
    },
    {
      "id": "lock-in-model",
      "type": "model",
      "title": "Lock-in Irreversibility Model"
    },
    {
      "id": "economic-disruption-model",
      "type": "model",
      "title": "Economic Disruption Structural Model"
    },
    {
      "id": "proliferation-model",
      "type": "model",
      "title": "AI Capability Proliferation Model"
    },
    {
      "id": "racing-dynamics",
      "type": "risk",
      "title": "Racing Dynamics"
    }
  ],
  "structural-unemployment": [
    {
      "id": "economic-disruption-model",
      "type": "model",
      "title": "Economic Disruption Structural Model"
    }
  ],
  "super-pac": [
    {
      "id": "leading-the-future",
      "type": "organization",
      "title": "Leading the Future super PAC"
    }
  ],
  "superalignment": [
    {
      "id": "jan-leike",
      "type": "researcher",
      "title": "Jan Leike"
    },
    {
      "id": "weak-to-strong",
      "type": "approach",
      "title": "Weak-to-Strong Generalization"
    }
  ],
  "superforecasting": [
    {
      "id": "philip-tetlock",
      "type": "researcher",
      "title": "Philip Tetlock"
    }
  ],
  "superhuman-ai": [
    {
      "id": "scalable-oversight",
      "type": "safety-agenda",
      "title": "Scalable Oversight"
    }
  ],
  "superhuman-alignment": [
    {
      "id": "debate",
      "type": "approach",
      "title": "AI Safety via Debate"
    }
  ],
  "superintelligence": [
    {
      "id": "self-improvement",
      "type": "capability",
      "title": "Self-Improvement and Recursive Enhancement"
    },
    {
      "id": "superintelligence",
      "type": "concept",
      "title": "Superintelligence"
    },
    {
      "id": "miri-era",
      "type": "historical",
      "title": "The MIRI Era"
    },
    {
      "id": "ssi",
      "type": "lab-research",
      "title": "Safe Superintelligence Inc (SSI)"
    },
    {
      "id": "ilya-sutskever",
      "type": "researcher",
      "title": "Ilya Sutskever"
    },
    {
      "id": "nick-bostrom",
      "type": "researcher",
      "title": "Nick Bostrom"
    },
    {
      "id": "treacherous-turn",
      "type": "risk",
      "title": "Treacherous Turn"
    }
  ],
  "superposition": [
    {
      "id": "interpretability",
      "type": "safety-agenda",
      "title": "Interpretability"
    }
  ],
  "supervision": [
    {
      "id": "weak-to-strong",
      "type": "approach",
      "title": "Weak-to-Strong Generalization"
    }
  ],
  "supply-demand": [
    {
      "id": "safety-researcher-gap",
      "type": "model",
      "title": "Safety Researcher Gap Model"
    }
  ],
  "surveillance": [
    {
      "id": "surveillance-authoritarian-stability",
      "type": "model",
      "title": "AI Surveillance and Regime Durability Model"
    },
    {
      "id": "surveillance-chilling-effects",
      "type": "model",
      "title": "Surveillance Chilling Effects Model"
    },
    {
      "id": "authoritarian-tools-diffusion",
      "type": "model",
      "title": "Authoritarian Tools Diffusion Model"
    },
    {
      "id": "authoritarian-takeover",
      "type": "risk",
      "title": "Authoritarian Takeover"
    }
  ],
  "surveys": [
    {
      "id": "public-opinion",
      "type": "ai-transition-model-metric",
      "title": "Public Opinion"
    },
    {
      "id": "expert-opinion",
      "type": "ai-transition-model-metric",
      "title": "Expert Opinion"
    }
  ],
  "sycophancy": [
    {
      "id": "sycophancy-feedback-loop",
      "type": "model",
      "title": "Sycophancy Feedback Loop Model"
    },
    {
      "id": "alignment-evals",
      "type": "approach",
      "title": "Alignment Evaluations"
    }
  ],
  "synthetic-media": [
    {
      "id": "cyber-psychosis-cascade",
      "type": "model",
      "title": "Cyber Psychosis Cascade Model"
    },
    {
      "id": "deepfakes",
      "type": "risk",
      "title": "Deepfakes"
    }
  ],
  "system-dynamics": [
    {
      "id": "feedback-loops",
      "type": "analysis",
      "title": "Feedback Loop & Cascade Model"
    },
    {
      "id": "automation-bias-cascade",
      "type": "model",
      "title": "Automation Bias Cascade Model"
    }
  ],
  "systems-thinking": [
    {
      "id": "concentration-of-power-model",
      "type": "model",
      "title": "Concentration of Power Systems Model"
    },
    {
      "id": "risk-interaction-matrix",
      "type": "model",
      "title": "Risk Interaction Matrix"
    },
    {
      "id": "compounding-risks-analysis",
      "type": "model",
      "title": "Compounding Risks Analysis Model"
    },
    {
      "id": "risk-cascade-pathways",
      "type": "model",
      "title": "Risk Cascade Pathways Model"
    },
    {
      "id": "risk-interaction-network",
      "type": "model",
      "title": "Risk Interaction Network Model"
    },
    {
      "id": "parameter-interaction-network",
      "type": "model",
      "title": "Parameter Interaction Network Model"
    }
  ],
  "takeoff-speed": [
    {
      "id": "self-improvement",
      "type": "capability",
      "title": "Self-Improvement and Recursive Enhancement"
    },
    {
      "id": "sharp-left-turn",
      "type": "risk",
      "title": "Sharp Left Turn"
    }
  ],
  "takeoff-speeds": [
    {
      "id": "fast-takeoff",
      "type": "concept",
      "title": "Fast Takeoff"
    }
  ],
  "talent": [
    {
      "id": "capabilities-to-safety-pipeline",
      "type": "model",
      "title": "Capabilities-to-Safety Pipeline Model"
    },
    {
      "id": "safety-researcher-gap",
      "type": "model",
      "title": "Safety Researcher Gap Model"
    }
  ],
  "talent-pipeline": [
    {
      "id": "training-programs",
      "type": "approach",
      "title": "AI Safety Training Programs"
    },
    {
      "id": "field-building-analysis",
      "type": "approach",
      "title": "Field Building Analysis"
    }
  ],
  "tax-optimization": [
    {
      "id": "anthropic-pre-ipo-daf-transfers",
      "type": "analysis",
      "title": "Anthropic Pre-IPO DAF Transfers"
    }
  ],
  "taxonomy": [
    {
      "id": "reward-hacking-taxonomy",
      "type": "model",
      "title": "Reward Hacking Taxonomy and Severity Model"
    }
  ],
  "technical": [
    {
      "id": "misalignment-potential",
      "type": "ai-transition-model-factor",
      "title": "Misalignment Potential"
    },
    {
      "id": "alignment-robustness",
      "type": "ai-transition-model-parameter",
      "title": "Alignment Robustness"
    },
    {
      "id": "safety-capability-gap",
      "type": "ai-transition-model-parameter",
      "title": "Safety-Capability Gap"
    },
    {
      "id": "interpretability-coverage",
      "type": "ai-transition-model-parameter",
      "title": "Interpretability Coverage"
    }
  ],
  "technical-risk": [
    {
      "id": "technical-pathways",
      "type": "analysis",
      "title": "Technical Pathway Decomposition"
    }
  ],
  "techno-optimism": [
    {
      "id": "marc-andreessen",
      "type": "researcher",
      "title": "Marc Andreessen"
    }
  ],
  "technological-singularity": [
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    }
  ],
  "theory": [
    {
      "id": "natural-abstractions",
      "type": "concept",
      "title": "Natural Abstractions"
    }
  ],
  "theory-of-change": [
    {
      "id": "worldview-intervention-mapping",
      "type": "model",
      "title": "Worldview-Intervention Mapping"
    }
  ],
  "third-party-auditing": [
    {
      "id": "model-auditing",
      "type": "approach",
      "title": "Third-Party Model Auditing"
    }
  ],
  "third-party-audits": [
    {
      "id": "evals-governance",
      "type": "policy",
      "title": "Evals-Based Deployment Gates"
    },
    {
      "id": "new-york-raise-act",
      "type": "policy",
      "title": "New York RAISE Act"
    }
  ],
  "third-party-evaluation": [
    {
      "id": "rsp",
      "type": "policy",
      "title": "Responsible Scaling Policies"
    }
  ],
  "three-laws-of-robotics": [
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    }
  ],
  "threshold": [
    {
      "id": "epistemic-collapse-threshold",
      "type": "model",
      "title": "Epistemic Collapse Threshold Model"
    },
    {
      "id": "capability-threshold-model",
      "type": "model",
      "title": "Capability Threshold Model"
    }
  ],
  "threshold-effects": [
    {
      "id": "trust-cascade-model",
      "type": "model",
      "title": "Trust Cascade Failure Model"
    }
  ],
  "thresholds": [
    {
      "id": "flash-dynamics-threshold",
      "type": "model",
      "title": "Flash Dynamics Threshold Model"
    },
    {
      "id": "irreversibility-threshold",
      "type": "model",
      "title": "Irreversibility Threshold Model"
    },
    {
      "id": "regulatory-capacity-threshold",
      "type": "model",
      "title": "Regulatory Capacity Threshold Model"
    }
  ],
  "timeline": [
    {
      "id": "bioweapons-timeline",
      "type": "model",
      "title": "AI-Bioweapons Timeline Model"
    },
    {
      "id": "cyberweapons-attack-automation",
      "type": "model",
      "title": "Autonomous Cyber Attack Timeline"
    },
    {
      "id": "autonomous-weapons-proliferation",
      "type": "model",
      "title": "LAWS Proliferation Model"
    },
    {
      "id": "deepfakes-authentication-crisis",
      "type": "model",
      "title": "Deepfakes Authentication Crisis Model"
    },
    {
      "id": "authentication-collapse-timeline",
      "type": "model",
      "title": "Authentication Collapse Timeline Model"
    },
    {
      "id": "risk-activation-timeline",
      "type": "model",
      "title": "Risk Activation Timeline Model"
    }
  ],
  "timelines": [
    {
      "id": "agi-timeline-debate",
      "type": "crux",
      "title": "When Will AGI Arrive?"
    },
    {
      "id": "agi-timeline",
      "type": "concept",
      "title": "AGI Timeline"
    }
  ],
  "timing": [
    {
      "id": "intervention-timing-windows",
      "type": "model",
      "title": "Intervention Timing Windows"
    }
  ],
  "tipping-points": [
    {
      "id": "epistemic-collapse-threshold",
      "type": "model",
      "title": "Epistemic Collapse Threshold Model"
    }
  ],
  "tool-use": [
    {
      "id": "agentic-ai",
      "type": "capability",
      "title": "Agentic AI"
    },
    {
      "id": "heavy-scaffolding",
      "type": "concept",
      "title": "Heavy Scaffolding / Agentic Systems"
    }
  ],
  "tradeoffs": [
    {
      "id": "safety-capability-tradeoff",
      "type": "model",
      "title": "Safety-Capability Tradeoff Model"
    }
  ],
  "tragedy-of-commons": [
    {
      "id": "multipolar-trap-model",
      "type": "model",
      "title": "Multipolar Trap Coordination Model"
    }
  ],
  "training": [
    {
      "id": "rlhf",
      "type": "capability",
      "title": "RLHF"
    },
    {
      "id": "data-constraints",
      "type": "concept",
      "title": "Data Constraints"
    }
  ],
  "training-costs": [
    {
      "id": "large-language-models",
      "type": "concept",
      "title": "Large Language Models"
    }
  ],
  "training-datasets": [
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    }
  ],
  "training-dynamics": [
    {
      "id": "deceptive-alignment-decomposition",
      "type": "model",
      "title": "Deceptive Alignment Decomposition Model"
    },
    {
      "id": "mesa-optimization-analysis",
      "type": "model",
      "title": "Mesa-Optimization Risk Analysis"
    }
  ],
  "training-efficiency": [
    {
      "id": "preference-optimization",
      "type": "approach",
      "title": "Preference Optimization Methods"
    }
  ],
  "training-gaming": [
    {
      "id": "situational-awareness",
      "type": "capability",
      "title": "Situational Awareness"
    }
  ],
  "training-methodology": [
    {
      "id": "constitutional-ai",
      "type": "approach",
      "title": "Constitutional AI"
    }
  ],
  "training-pipeline": [
    {
      "id": "dense-transformers",
      "type": "concept",
      "title": "Dense Transformers"
    }
  ],
  "training-programs": [
    {
      "id": "field-building",
      "type": "crux",
      "title": "Field Building and Community"
    },
    {
      "id": "training-programs",
      "type": "approach",
      "title": "AI Safety Training Programs"
    },
    {
      "id": "field-building-analysis",
      "type": "approach",
      "title": "Field Building Analysis"
    }
  ],
  "training-runs": [
    {
      "id": "monitoring",
      "type": "policy",
      "title": "Compute Monitoring"
    }
  ],
  "trajectories": [
    {
      "id": "alignment-robustness-trajectory",
      "type": "model",
      "title": "Alignment Robustness Trajectory Model"
    }
  ],
  "transformative-ai": [
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    },
    {
      "id": "holden-karnofsky",
      "type": "researcher",
      "title": "Holden Karnofsky"
    }
  ],
  "transformer-architecture": [
    {
      "id": "dense-transformers",
      "type": "concept",
      "title": "Dense Transformers"
    }
  ],
  "transformer-circuits": [
    {
      "id": "neel-nanda",
      "type": "researcher",
      "title": "Neel Nanda"
    }
  ],
  "transformerlens": [
    {
      "id": "neel-nanda",
      "type": "researcher",
      "title": "Neel Nanda"
    }
  ],
  "transformers": [
    {
      "id": "language-models",
      "type": "capability",
      "title": "Large Language Models"
    },
    {
      "id": "large-language-models",
      "type": "concept",
      "title": "Large Language Models"
    }
  ],
  "transition": [
    {
      "id": "transition-turbulence",
      "type": "ai-transition-model-factor",
      "title": "Transition Turbulence"
    }
  ],
  "transparency": [
    {
      "id": "whistleblower-dynamics",
      "type": "model",
      "title": "Whistleblower Dynamics Model"
    },
    {
      "id": "chris-olah",
      "type": "researcher",
      "title": "Chris Olah"
    },
    {
      "id": "process-supervision",
      "type": "approach",
      "title": "Process Supervision"
    },
    {
      "id": "california-sb53",
      "type": "policy",
      "title": "California SB 53"
    },
    {
      "id": "model-registries",
      "type": "policy",
      "title": "Model Registries"
    }
  ],
  "treacherous-turn": [
    {
      "id": "nick-bostrom",
      "type": "researcher",
      "title": "Nick Bostrom"
    }
  ],
  "trend-extrapolation": [
    {
      "id": "epoch-ai",
      "type": "organization",
      "title": "Epoch AI"
    }
  ],
  "tripwires": [
    {
      "id": "warning-signs-model",
      "type": "model",
      "title": "Warning Signs Model"
    }
  ],
  "trust": [
    {
      "id": "public-opinion",
      "type": "ai-transition-model-metric",
      "title": "Public Opinion"
    },
    {
      "id": "deepfakes-authentication-crisis",
      "type": "model",
      "title": "Deepfakes Authentication Crisis Model"
    },
    {
      "id": "trust-cascade-model",
      "type": "model",
      "title": "Trust Cascade Failure Model"
    },
    {
      "id": "trust-erosion-dynamics",
      "type": "model",
      "title": "Trust Erosion Dynamics Model"
    },
    {
      "id": "epistemic-security",
      "type": "approach",
      "title": "Epistemic Security"
    },
    {
      "id": "content-authentication",
      "type": "approach",
      "title": "Content Authentication"
    },
    {
      "id": "automation-bias",
      "type": "risk",
      "title": "Automation Bias"
    },
    {
      "id": "epistemic-collapse",
      "type": "risk",
      "title": "Epistemic Collapse"
    }
  ],
  "trust-erosion": [
    {
      "id": "epistemic-risks",
      "type": "crux",
      "title": "Epistemic Cruxes"
    }
  ],
  "trust-structure": [
    {
      "id": "long-term-benefit-trust",
      "type": "analysis",
      "title": "Long-Term Benefit Trust (Anthropic)"
    }
  ],
  "trustworthy-ai": [
    {
      "id": "nist-ai",
      "type": "organization",
      "title": "NIST and AI Safety"
    }
  ],
  "truth": [
    {
      "id": "epistemic-collapse",
      "type": "risk",
      "title": "Epistemic Collapse"
    }
  ],
  "truth-seeking-ai": [
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    }
  ],
  "truthfulness": [
    {
      "id": "epistemic-sycophancy",
      "type": "risk",
      "title": "Epistemic Sycophancy"
    }
  ],
  "uk-ai-policy": [
    {
      "id": "uk-aisi",
      "type": "organization",
      "title": "UK AI Safety Institute"
    }
  ],
  "uk-policy": [
    {
      "id": "controlai",
      "type": "organization",
      "title": "ControlAI"
    },
    {
      "id": "centre-for-long-term-resilience",
      "type": "organization",
      "title": "Centre for Long-Term Resilience"
    }
  ],
  "uncertainty": [
    {
      "id": "reasoning",
      "type": "capability",
      "title": "Reasoning and Planning"
    }
  ],
  "uncertainty-analysis": [
    {
      "id": "critical-uncertainties",
      "type": "crux",
      "title": "Critical Uncertainties Model"
    }
  ],
  "unlearning": [
    {
      "id": "capability-unlearning",
      "type": "approach",
      "title": "Capability Unlearning / Removal"
    }
  ],
  "unpredictability": [
    {
      "id": "emergent-capabilities",
      "type": "risk",
      "title": "Emergent Capabilities"
    }
  ],
  "unsolved-problem": [
    {
      "id": "eliciting-latent-knowledge",
      "type": "approach",
      "title": "Eliciting Latent Knowledge (ELK)"
    }
  ],
  "untrusted-ai": [
    {
      "id": "ai-control",
      "type": "safety-agenda",
      "title": "AI Control"
    }
  ],
  "uplift": [
    {
      "id": "bioweapons-ai-uplift",
      "type": "model",
      "title": "AI Uplift Assessment Model"
    }
  ],
  "urgency": [
    {
      "id": "intervention-timing-windows",
      "type": "model",
      "title": "Intervention Timing Windows"
    }
  ],
  "us-aisi": [
    {
      "id": "us-executive-order",
      "type": "policy",
      "title": "Executive Order on Safe, Secure, and Trustworthy AI"
    }
  ],
  "us-china-competition": [
    {
      "id": "multi-actor-landscape",
      "type": "analysis",
      "title": "Multi-Actor Strategic Landscape"
    }
  ],
  "us-government": [
    {
      "id": "ai-executive-order",
      "type": "policy",
      "title": "Biden AI Executive Order"
    }
  ],
  "user-experience": [
    {
      "id": "epistemic-sycophancy",
      "type": "risk",
      "title": "Epistemic Sycophancy"
    }
  ],
  "validation": [
    {
      "id": "sycophancy-feedback-loop",
      "type": "model",
      "title": "Sycophancy Feedback Loop Model"
    }
  ],
  "valuation": [
    {
      "id": "anthropic-valuation",
      "type": "analysis",
      "title": "Anthropic Valuation Analysis"
    },
    {
      "id": "anthropic-ipo",
      "type": "analysis",
      "title": "Anthropic IPO"
    }
  ],
  "value-alignment": [
    {
      "id": "nick-bostrom",
      "type": "researcher",
      "title": "Nick Bostrom"
    },
    {
      "id": "stuart-russell",
      "type": "researcher",
      "title": "Stuart Russell"
    }
  ],
  "value-learning": [
    {
      "id": "chai",
      "type": "lab-academic",
      "title": "CHAI"
    },
    {
      "id": "corrigibility",
      "type": "safety-agenda",
      "title": "Corrigibility"
    }
  ],
  "value-lock-in": [
    {
      "id": "lock-in-model",
      "type": "model",
      "title": "Lock-in Irreversibility Model"
    },
    {
      "id": "irreversibility",
      "type": "risk",
      "title": "Irreversibility"
    }
  ],
  "value-specification": [
    {
      "id": "provably-safe",
      "type": "approach",
      "title": "Provably Safe AI (davidad agenda)"
    }
  ],
  "values": [
    {
      "id": "value-learning",
      "type": "safety-agenda",
      "title": "Value Learning"
    }
  ],
  "venture-capital": [
    {
      "id": "david-sacks",
      "type": "researcher",
      "title": "David Sacks"
    },
    {
      "id": "marc-andreessen",
      "type": "researcher",
      "title": "Marc Andreessen"
    }
  ],
  "verification": [
    {
      "id": "information-authenticity",
      "type": "ai-transition-model-parameter",
      "title": "Information Authenticity"
    },
    {
      "id": "solutions",
      "type": "crux",
      "title": "Solution Cruxes"
    },
    {
      "id": "authentication-collapse-timeline",
      "type": "model",
      "title": "Authentication Collapse Timeline Model"
    },
    {
      "id": "content-authentication",
      "type": "approach",
      "title": "Content Authentication"
    },
    {
      "id": "coordination-tech",
      "type": "approach",
      "title": "Coordination Technologies"
    },
    {
      "id": "epistemic-infrastructure",
      "type": "approach",
      "title": "Epistemic Infrastructure"
    },
    {
      "id": "monitoring",
      "type": "policy",
      "title": "Compute Monitoring"
    },
    {
      "id": "international-regimes",
      "type": "policy",
      "title": "International Compute Regimes"
    },
    {
      "id": "trust-decline",
      "type": "risk",
      "title": "Trust Decline"
    }
  ],
  "vernor-vinge": [
    {
      "id": "early-warnings",
      "type": "historical",
      "title": "Early Warnings Era"
    }
  ],
  "voice-cloning": [
    {
      "id": "fraud",
      "type": "risk",
      "title": "AI-Powered Fraud"
    }
  ],
  "voluntary-commitments": [
    {
      "id": "international-summits",
      "type": "policy",
      "title": "International AI Safety Summit Series"
    },
    {
      "id": "rsp",
      "type": "policy",
      "title": "Responsible Scaling Policies"
    },
    {
      "id": "corporate",
      "type": "approach",
      "title": "Corporate Responses"
    }
  ],
  "vulnerability-discovery": [
    {
      "id": "red-teaming",
      "type": "approach",
      "title": "Red Teaming"
    }
  ],
  "warfare": [
    {
      "id": "autonomous-weapons",
      "type": "risk",
      "title": "Autonomous Weapons"
    }
  ],
  "warning-shots": [
    {
      "id": "rogue-ai-scenarios",
      "type": "risk",
      "title": "Rogue AI Scenarios"
    }
  ],
  "watermarking": [
    {
      "id": "content-authentication",
      "type": "approach",
      "title": "Content Authentication"
    },
    {
      "id": "authentication-collapse",
      "type": "risk",
      "title": "Authentication Collapse"
    }
  ],
  "weak-to-strong": [
    {
      "id": "weak-to-strong",
      "type": "approach",
      "title": "Weak-to-Strong Generalization"
    }
  ],
  "weak-to-strong-generalization": [
    {
      "id": "jan-leike",
      "type": "researcher",
      "title": "Jan Leike"
    }
  ],
  "weapons": [
    {
      "id": "misuse-potential",
      "type": "ai-transition-model-factor",
      "title": "Misuse Potential"
    },
    {
      "id": "bio-risk",
      "type": "risk",
      "title": "AI-Enabled Biological Risks"
    }
  ],
  "web-browsing": [
    {
      "id": "tool-use",
      "type": "capability",
      "title": "Tool Use and Computer Use"
    }
  ],
  "whistleblower": [
    {
      "id": "california-sb53",
      "type": "policy",
      "title": "California SB 53"
    },
    {
      "id": "lab-culture",
      "type": "approach",
      "title": "Lab Safety Culture"
    },
    {
      "id": "whistleblower-protections",
      "type": "policy",
      "title": "AI Whistleblower Protections"
    }
  ],
  "whistleblowing": [
    {
      "id": "corporate-influence",
      "type": "crux",
      "title": "Corporate Influence"
    },
    {
      "id": "whistleblower-dynamics",
      "type": "model",
      "title": "Whistleblower Dynamics Model"
    }
  ],
  "windows": [
    {
      "id": "intervention-timing-windows",
      "type": "model",
      "title": "Intervention Timing Windows"
    }
  ],
  "winner-take-all": [
    {
      "id": "structural-risks",
      "type": "crux",
      "title": "Structural Risk Cruxes"
    }
  ],
  "workforce-growth": [
    {
      "id": "field-building-analysis",
      "type": "approach",
      "title": "Field Building Analysis"
    }
  ],
  "world-modeling": [
    {
      "id": "provably-safe",
      "type": "approach",
      "title": "Provably Safe AI (davidad agenda)"
    }
  ],
  "world-models": [
    {
      "id": "provable-safe",
      "type": "concept",
      "title": "Provable / Guaranteed Safe AI"
    }
  ],
  "worldview": [
    {
      "id": "worldview-intervention-mapping",
      "type": "model",
      "title": "Worldview-Intervention Mapping"
    }
  ],
  "worst-case-alignment": [
    {
      "id": "arc",
      "type": "organization",
      "title": "ARC"
    }
  ],
  "x-integration": [
    {
      "id": "xai",
      "type": "lab",
      "title": "xAI"
    }
  ],
  "x-risk": [
    {
      "id": "existential-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Existential Catastrophe"
    },
    {
      "id": "ai-takeover",
      "type": "ai-transition-model-scenario",
      "title": "AI Takeover"
    },
    {
      "id": "human-catastrophe",
      "type": "ai-transition-model-scenario",
      "title": "Human-Caused Catastrophe"
    },
    {
      "id": "fast-takeoff",
      "type": "concept",
      "title": "Fast Takeoff"
    },
    {
      "id": "superintelligence",
      "type": "concept",
      "title": "Superintelligence"
    },
    {
      "id": "agi-race",
      "type": "concept",
      "title": "AGI Race"
    },
    {
      "id": "existential-risk",
      "type": "concept",
      "title": "Existential Risk"
    },
    {
      "id": "lock-in-mechanisms",
      "type": "model",
      "title": "Lock-in Mechanisms Model"
    },
    {
      "id": "carlsmith-six-premises",
      "type": "model",
      "title": "Carlsmith's Six-Premise Argument"
    },
    {
      "id": "cais",
      "type": "lab-research",
      "title": "CAIS"
    },
    {
      "id": "dan-hendrycks",
      "type": "researcher",
      "title": "Dan Hendrycks"
    },
    {
      "id": "eliezer-yudkowsky",
      "type": "researcher",
      "title": "Eliezer Yudkowsky"
    },
    {
      "id": "geoffrey-hinton",
      "type": "researcher",
      "title": "Geoffrey Hinton"
    },
    {
      "id": "holden-karnofsky",
      "type": "researcher",
      "title": "Holden Karnofsky"
    },
    {
      "id": "nick-bostrom",
      "type": "researcher",
      "title": "Nick Bostrom"
    },
    {
      "id": "toby-ord",
      "type": "researcher",
      "title": "Toby Ord"
    },
    {
      "id": "yoshua-bengio",
      "type": "researcher",
      "title": "Yoshua Bengio"
    },
    {
      "id": "bioweapons",
      "type": "risk",
      "title": "Bioweapons Risk"
    },
    {
      "id": "concentration-of-power",
      "type": "risk",
      "title": "Concentration of Power"
    },
    {
      "id": "irreversibility",
      "type": "risk",
      "title": "Irreversibility"
    },
    {
      "id": "lock-in",
      "type": "risk",
      "title": "Lock-in"
    },
    {
      "id": "authoritarian-takeover",
      "type": "risk",
      "title": "Authoritarian Takeover"
    },
    {
      "id": "autonomous-replication",
      "type": "risk",
      "title": "Autonomous Replication"
    }
  ]
}